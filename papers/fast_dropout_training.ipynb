{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Fast dropout training\", Wang, Manning, 2013"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract\n",
    "\n",
    "\"Preventing feature co-adaptation by encouraging independent contributions from different features often improves classification and regression performance. Dropout training (\"Improving Neural Networks by Preventing Co-adaptation of Feature Detectors\") does this by randomly dropping out (zeroing) hidden units and input features during training of neural networks. However, repeatedly sampling a random subset of input features makes training much slower. Based on an examination of the implied objective function of dropout training, we show how to do fast dropout training by sampling from or integrating a Gaussian approximation, instead of doing Monte Carlo optimization of this objective. This approximation, justified by the central limit theorem and empirical evidence, gives an order of magnitude speedup and more stability. We show how to do fast dropout training for classification, regression, and multilayer neural networks. Beyond dropout, our technique is extended to integrate out other types of noise and small image transformation.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "\"Recent work (\"Improving neural networks by preventing co-adaptation of feature detectors\") has shown that preventing feature co-adaptation by dropout training is a promising method for regularization. Applied to neural network training, the idea is to dropout (zero) randomly sampled hidden units and input features during each iteration of optimization. Dropout played an important role in the systems that won recent learning competitions such as ImageNet classification (Krizhevsky et al, 2012) and the Merck molecular activity challenge at www.kaggle.com, and improves performance on various tasks. Dropout can be consider another approach to regularization in addition to the widely used parameter shrinkage methods and model averaging. This process lowers the trust in a feature that is only helpful when other specific features are present, since any particular feature may be dropped out and cannot be depended on. Alternatively, the procedure can be seen as averaging over many neural networks with shared weights.\n",
    "\n",
    "\"Other observations of harmful co-adaptation and ways to address them exist in the literature. Naive Bayes, by completely ignoring co-adaptation, performs better than discriminative methods when there is little data ('On discriminative vs generative classifiers: a comparison of logistic regression and naive bayes', 2002), and continues to perform better on certain relatively large datasets ('Baselines and bigrams: simple, good sentiment and topic classification', 2012). In 'Reducing weight undertraining in structured discriminative learning', Sutton et al 2006, it is observed that training involves trade-offs among weights, where the presence of highly indicative features can cause other useful but weaker features to undertrain. They propose feature bagging: training different models on subsets of features that are combined, an idea further pursued under the name logarithmic opinion pools by 'Logarithmic opinion pools for conditional random fields', Smith et al 2005. Improved performance on Named Entity Recognition and Part-of-Speech Tagging was demonstrated.\n",
    "\n",
    "\"While the effectiveness of these methods in preventing feature co-adaptation has been demonstrated, actually sampling, or training multiple models, make training slower. Moreover, with a dropout rate of $p$, the proportion of the data still not seen after $n$ passes is $p^n$ (eg 5 passes of the data are required to see 95% of it at $p = 0.5$). If the data is not highly redundant, and if we make the relevant data only partially observable at ranom, then the task becomes even harder, and training efficiency may reduce further.\n",
    "\n",
    "\"In this paper, we look at how to achieve the benefit of dropout training without actually sampling, thereby using all the data efficiently. The approach uses a Gaussian approximation that is justified by the central limit theorem and empirical evidence. We show the validity of this approximation and how it can provide an order of magnitude speed-up at training time, while also giving more stability. Fast dropout fits into the general framework of integrating out noise added by the training data ('Noise injection into inputs in back-propagation learning', 1992; 'Training with noise is equivalent to Tikhonov regularization', Bishop, 1995). See 'Learning with marginalized corrupted features', van der Maaten, 2013, for an alternative approach to integrating out noise and a survey of related work from that angle. Their approach is exact for loss functions decomposable by the moment generating function of the independent noise such as the exponential loss and squared error loss. Our approach does not require independence: it can integrate out small transformations that an image classifier should be invariant to. We begin with logistic regression for simplicity, then extend the idea to other loss functions, other noise, and neural networks. Code is provided at the author's website.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Fast approximations to dropout$\\def\\Bernoulli{\\text{Bernoulli}}\n",
    "\\def\\diag{\\text{diag}}$\n",
    "\n",
    "## 2.1 The implied objective function\n",
    "\n",
    "\"We illustrate the idea with logistic regression (LR) given training vector $x$, and label $y \\in \\{0, 1\\}$. To train LR with dropout on data with dimension $m$, first sample $z_i \\sim \\Bernoulli(p_i)$ for $i=1\\dots m$. Here $p_i$ is the probability of not dropping out input $x_i$. After sampling $\\mathbf{z} = \\{z_i\\}_{i=1\\dots m}$ we can compute the stochastic gradient descent (sgd) update as follows:\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{w} = (\\mathbf{y} - \\sigma(\\mathbf{w}^T D_\\mathbf{z} \\mathbf{x}))D_\\mathbf{z} \\mathbf{x}\n",
    "$$\n",
    "\n",
    "... where $D_\\mathbf{z} = \\diag(\\mathbf{z}) \\in \\mathbb{R}^{m \\times n}$, and $\\sigma(x) = 1 / (1 + e^{-x})$ is the logistic function.\n",
    "\n",
    "\"This update rule, applied over the training data for multiple passes, can be seen as a Monte Carlo approximation to the following gradient:\n",
    "\n",
    "$$\n",
    "\\Delta \\bar{\\mathbf{w}} = \\mathbb{E}_{\\mathbf{z};z_i \\sim \\Bernoulli(p_i)} \\left[\n",
    "    (\\mathbf{y} - \\sigma(\\mathbf{w}^T D_\\mathbf{z} \\mathbf{x})) D_\\mathbf{z} \\mathbf{x}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective function with the above gradient is the expected conditional log-likelihood of the label given the data with dropped out dimensions indicated by $\\mathbf{z}$, for $\\mathbf{y} \\sim \\Bernoulli(\\sigma(\\mathbf{w}^T D_\\mathbf{z} \\mathbf{x})))$. This is the implied objective function for dropout training:\n",
    "\n",
    "$$\n",
    "L(\\mathbf{w}) = \\mathbb{E}_\\mathbf{z}\\left[\n",
    "    \\log(p(\\mathbf{y} \\mid D_\\mathbf{z} \\mathbf{x}; \\mathbf{w})\n",
    "\\right] \\\\\n",
    "= \\mathbb{E}_\\mathbf{z} \\left[\n",
    "    \\mathbf{y} \\log(\\sigma(\\mathbf{w}^T D_\\mathbf{z} \\mathbf{x})) +\n",
    "    (1 - \\mathbf{y}) \\log(1 - \\sigma(\\mathbf{w}^T D_\\mathbf{z} \\mathbf{x}))\n",
    "\\right]\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Since we are just taking an expectation, we still have a convex optimization problem provided taht the negative log-likelihood is convex.\n",
    "\n",
    "\"Evaluating the expectation in (1) naively by summing over all possible $\\mathbf{z}$ has complexity $O(2^mm)$. Rather than directly computing the expectation with respect to $\\mathbf{z}$, we propose a variable transformation that allows us to approximately compute the expectation with respect to a simple random variable $Y \\in \\mathbb{R}$, instead of $\\mathbf{z} \\in \\{0,1\\}^m$. In the next subsection, we describe an efficient $O(m)$ approximation that is accurate for machine learning applications where $w_i x_i$ usually come from a unimodal or bounded distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 The Gaussian approximation\n",
    "\n",
    "\"We make the observation that evaluating the objective function $L(w)$ involves taking the expectation with respect to the variable $Y(\\mathbf{z}) = \\mathbf{w}^TD_\\mathbf{z} \\mathbf{x} = \\sum_i^m w_i x_i z_i$, a weighted sum of Bernoulli random variables. For most machine learning problems, $\\{ w_i \\}$ typically forms a unimodal distribution centred at $0$, $\\{x_i\\}$ is either unimodal or in a fixed interval. In this case, $Y$ can be well approximated by a normal distribution even for relatively low dimensional data with $m=10$. More technically, the Lyapunov condition is generally satisfied for a weighted sum of Bernoulli random variables of the form $Y$ that are weighted by real data ('Elements of Large-Sample Theory', Lehmann 1998). Then, Lyapunov's central limit theorem states that $Y(z)$ tends to a normal distribution as $m \\rightarrow \\infty$ (see figure 1). We empirically verify that the approximation is good for typical datasets of moderate dimensions, except when a couple of dimensions dominate all others (see figure 3). Finally, let $S$ be the approximating Gaussian ($Y \\xrightarrow{d} S)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\def\\Var{\\text{Var}}\n",
    "S = \\mathbb{E}_\\mathbf{z} [ Y(\\mathbf{z}) ]\n",
    "+ \\sqrt{\\Var[Y(z)]} \\epsilon\n",
    " = \\mu_S + \\sigma_S \\epsilon\n",
    " \\tag{3}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "- $\\epsilon \\sim \\mathcal{N}(0,1)$,\n",
    "- $\\def\\z{\\mathbf{z}}\\mathbb{E}[Y(\\z)] = \\sum_{i=1}^m p_i w_i x_i$, and\n",
    "- $\\Var[Y(\\z)] = \\sum_{i=1}^m p_i(1-p_i)w_ix_i)^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In the following subsections, based on the Gaussian assumptions above, we present several approximations at different tradeoff points between speed and accuracy. In the end, we present experimental results showing that there is little to no performance loss when using the faster, less accurate approximations.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Gradient computation by sampling from the Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Given good convergence, we note that drawing samples of the approximating Gaussian $S$ of $Y(\\z)$, a constant time operation, is much cheaper than drawing samples of $Y(\\z)$ directly, which takes $O(m)$. This effect is very significant for high dimensional datasets. So without doing much, we can already approximate the objective function (2) $m$ times faster by sampling from $S$ instead of $Y(\\z)$. Empiricallly, this approximation is within the variance of the direct MC approximation of (2) by taking 200 samples of $\\z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Approximating the gradient introduces a complication when using samples from the Gaussian. The gradient (1) involves not only $Y(\\z) \\rightarrow{d} S$, but also $\\def\\x{\\mathbf{x}}D_\\z \\x$ directly:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla L(w) = \\mathbb{E}_\\z\\left[\n",
    "    (y - \\sigma(Y(\\z)))D_\\z \\x\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Let $f(Y(\\z)) = y - \\sigma(Y(\\z))$ and let $g(\\z) = D_\\z \\x$. Naively approximating $\\mathbf{E}[f(Y(\\z))g(\\z)]$ by either $\\mathbb{E}_S[f(S)]\\mathbb{E}_\\z[g(\\z)]$, or worse, by $\\def\\Exp{\\mathbb{E}}f(\\Exp_S[S])\\Exp_\\z[g(\\z)]$ works poorly in terms of both approximation error and final performance. Note that $g(\\z)$ is a linear function and therefore $\\Exp_\\z[g(\\z)] = g(\\Exp_\\z[\\z]) = \\diag(p)\\x$. A good way to approximate (4) is by analytically taking the expectation with respect to $z_i$ and then using a linear approximation to the conditional expectation. More precisely, consider dimension $i$ of the gradient:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L(w)}\n",
    "  {\\partial w_i}\n",
    "  = \\Exp_\\z[f(Y(\\z))x_i z_i] \\\\\n",
    "  = \\sum_{z_i \\in \\{0, 1\\}} p(z_i) z_i x_i \\Exp_{z_{-i}|z_i}[f(Y(\\z))] \\\\\n",
    "  = p(z_i=1)x_i \\Exp_{z_{-i}|z_i=1}[f(Y(\\z))] \\\\\n",
    "  \\approx p_i x_i\\left(\n",
    "    \\Exp_{S \\sim \\mathcal{N}(\\mu_S, \\sigma_S^2)}[f(S)]\n",
    "    + \\\\ \\Delta \\mu_i \\left. \\frac{\\partial \\Exp_{S \\sim \\mathcal{N}(\\mu, \\sigma_S^2)}[f(S)]}{\\partial \\mu} \\right\\vert_{\\mu=\\mu_S}\n",
    "    + \\\\ \\Delta \\sigma_i^2 \\left.\n",
    "       \\frac{\\partial \\Exp_{S \\sim \\mathcal{N}(\\mu_S, \\sigma^2)}[f(S)]}\n",
    "          {\\partial \\sigma^2}\n",
    "    \\right\\vert_{\\sigma^2 = \\sigma_S^2}\n",
    "  \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= p_i x_i \\left(\n",
    "    \\alpha(\\mu_S, \\sigma_S^2) + \n",
    "    \\Delta \\mu_i \\beta(\\mu_S, \\sigma_S^2)\n",
    "    + \\Delta \\sigma_i^2 \\gamma(\\mu_S, \\sigma_S^2)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where:\n",
    "\n",
    "- $z_{-i}$ is the collection of all other $z$s except $z_i$,\n",
    "- $\\mu_S$, $\\sigma_S$ is defined in (3)\n",
    "- $\\Delta \\mu_i = (1 - p_i) x_i w_i$, $\\Delta \\sigma_i^2 = -p_i(1-p_i) x_i^2 w_i^2$ are the changes in $\\mu_S$, $\\sigma_S^2$ due to conditioning on $z_i$.\n",
    "\n",
    "\"Note that the partial derivatives as well as $\\Exp_{S \\sim \\mathcal{N}(\\mu_S, \\sigma_S^2)}[f(S)]$ only need to be computed once per training case, since they are independent of $i$. $\\alpha$, $\\beta$, $\\gamma$ can be computed by drawing $K$ samples from $S$, taking time $O(K)$ (whereas $K$ samples of $Y(\\z)$ take time $O(mK))$. Concretely,\n",
    "\n",
    "$$\n",
    "\\alpha(\\mu, \\sigma^2)\n",
    "= y - \\Exp_{S \\sim \\mathcal{N}(0,1)} \\left[\n",
    "    \\frac{1}{1 + \\exp(-\\mu - \\sigma_S S)}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $\\beta$ and $\\gamma$, we have:\n",
    "\n",
    "$$\n",
    "\\beta(\\mu, \\sigma^2) = \\frac{\\partial \\alpha(\\mu, \\sigma^2)}{\\partial \\mu} \\\\\n",
    "\\gamma(\\mu, \\sigma^2) = \\frac{\\partial \\alpha(\\mu, \\sigma^2)}\n",
    "   {\\partial \\sigma^2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These can be computed by differentiating inside the expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"One can combine (5) and what we do in (7) below to obtain a more accurate yet relatively cheap approximation to the derivative. However, in practice, using only $\\beta$ approximates the derivative to within the variance of successive MC computations of the objective $L$ (see figure 4). Empirically, this is 2-30 times faster compared to MC dropout (see figure 5 and table 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"At a slightly higher loss in accuracy, we can get rid of $\\z$ completely by re-parameterizing the problem in $\\mu_S$ and $\\sigma_S$ and taking derivatives with respect to them instead of approximating the derivative directly. So the objective function (2) becomes\n",
    "\n",
    "$$\n",
    "L(w) \\approx \\Exp_{S \\sim \\mathcal{N}(\\mu_S, \\sigma_S)} \\left[\n",
    "   y \\log(\\sigma(S)) + (1 - y)\\log(1 - \\sigma(S))\n",
    "\\right]\n",
    "\\tag{6}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 A closed-form approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAEACAYAAACj0I2EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ndBERL6GCLiLiJVTQRUS8xP8DcXKKNgFZ6F8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1089a8710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "\n",
    "\n",
    "X = np.arange(-5, 5, 0.1)\n",
    "\n",
    "plt.plot(X, 1 / (1 + np.exp(-X)), label='sigmoid')\n",
    "# # plt.show()\n",
    "# # plt.plot(X, scipy.stats.norm.cdf(X), label='gaussian cdf')\n",
    "plt.plot(X, scipy.stats.norm.cdf(np.sqrt(np.pi / 8) * X), label='gaussian cdf 2')\n",
    "# # plt.plot(X, scipy.stats.norm.cdf(np.pi / 5 * X), label='gaussian cdf 3')\n",
    "\n",
    "# plt.plot(X, 1/(1 + np.exp(-0.07056 * X * X * X - 1.5976 * X)), label='log cubed')\n",
    "# plt.plot(X, scipy.stats.norm.cdf(X), label='gaussian cdf 1')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $\\sigma(x) \\approx \\Phi(\\sqrt{\\frac{\\pi}{8}}x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we have:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^\\infty \\Phi(\\lambda x)\\mathcal{N}(x \\mid \\mu, s^2) \\, dx\n",
    "= \\Phi\\left(\n",
    "    \\frac{\\mu}{\\sqrt{\\lambda^{-2} + s^2}}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want:\n",
    "\n",
    "$$\n",
    "\\alpha(\\mu_S, \\sigma^2_S)\n",
    "= \\Exp_{S \\sim \\mathcal{N}(\\mu_S, \\sigma_S^2)}[f(S)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\Exp_{S \\sim \\mathcal{N}(\\mu_S, \\sigma_S^2)}\n",
    "[\n",
    "    y - \\sigma(S)\n",
    "]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\int_{-\\infty}^\\infty (y - \\sigma(S))\\mathcal{N}(S \\mid \\mu_S, \\sigma_S^2)\\, dS\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= y - \\int_{-\\infty}^\\infty \\sigma(S)\\, \\mathcal{N}(S \\mid \\mu_S, \\sigma_S^2)\\,dS\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\int_{-\\infty}^\\infty \\sigma(S) \\mathcal{N}(S \\mid \\mu_S, \\sigma_S^2)\\,dS \\\\\n",
    "\\approx\n",
    "\\int_{-\\infty}^\\infty\n",
    "\\Phi\\left(\n",
    "  \\sqrt{\\frac{\\pi}{8}}S\n",
    "\\right)\n",
    "\\,\n",
    "\\mathcal{N}(S \\mid \\mu_S, \\sigma_S^2)\\,dS\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\Phi\\left(\n",
    "   \\frac{\\mu_S}\n",
    "      {\\sqrt{\\frac{8}{\\pi} + \\sigma_S^2}}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\approx \\sigma \\left(\n",
    "   \\sqrt{\\frac{8}{\\pi}}\n",
    "   \\frac{\\mu_S}\n",
    "     {\\sqrt{\\frac{8}{\\pi} + \\sigma_S^2}}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\sigma \\left(\n",
    "   \\frac{\\mu_S}\n",
    "      {\\sqrt{1 + \\frac{\\pi}{8} \\sigma_S^2}}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:\n",
    "\n",
    "$$\n",
    "\\alpha(\\mu_S, \\sigma_S^2) \\approx y - \\sigma \\left(\n",
    "  \\frac{\\mu_S}\n",
    "    {\\sqrt{1 + \\frac{\\pi}{8}\\sigma_S^2}}\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"This is an approximation that is used for Bayesian prediction when the posterior is approximated by a Gaussian (MacKay, 1992). As we now have a closed-form approximation of $\\alpha$, one can also obtain expressions for $\\beta$ and $\\gamma$ by differentiating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Furthermore, by substituting $x = \\mu + st$, differentiating with respect to $\\mu$, and (8), we can even approximate the objective function (6) in a closed form:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From https://mathoverflow.net/questions/127086/integral-of-the-product-of-normal-density-and-cdf/127094#127094\n",
    "\n",
    "\"Recall that $\\Phi(x) = P[X \\le x]$ for every $x$, where the random variable $X$ is standard normal, and that, for every suitable function $u$,\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^\\infty u(x)\\,\\phi(x)\\,dx = \\Exp[u(Y)]\n",
    "$$\n",
    "\n",
    "... where the random variable $Y$ is standard normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this for $u = \\Phi$, we have:\n",
    "\n",
    "$$\n",
    "\\int_{-\\infty}^\\infty \\Phi(x)\\,\\phi(x)\\,dx = \\Exp[\\Phi(Y)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# interlude: try to derive the formula for $\\int \\Phi(\\cdot)\\phi(\\cdot)$\n",
    "\n",
    "See ../maths/integrate_cdf_pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
