{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Reinforcement learning\", David Silver\n",
    "\n",
    "- video: https://www.youtube.com/watch?v=2pWv7GOvuf0\n",
    "- slides: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notation__:\n",
    "- history: $H_t = O_1, R_1, A_1, \\dots, A_{t-1}, O_t, R_t$\n",
    "- State, a function of history: $S_t = f(H_t)$\n",
    "- environment state $S_t^e$\n",
    "- agent state $S_t^a$\n",
    "  - can also be any function of history, $S_t^a = f(H_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Information State__\n",
    "\n",
    "A state $S_t$ is Markov is an only if:\n",
    "\n",
    "$$\n",
    "P(S_{t+1} \\mid S_t) = P(S_{t+1} \\mid S_1,\\dots, S_t)\n",
    "$$\n",
    "\n",
    "- the future is independent of the past, given the present:\n",
    "\n",
    "$$\n",
    "H_{1:t} \\rightarrow S_t \\rightarrow H_{t+1:\\infty}\n",
    "$$\n",
    "- once the state is known, the history may be thrown away\n",
    "- the environment state $S_t^e$ is Markov\n",
    "- the history $H_t$ is Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fully Observable Environments__\n",
    "\n",
    "Full observability: agent directly observes agent state:\n",
    "\n",
    "$$\n",
    "O_t = S_t^a = S_t^e\n",
    "$$\n",
    "\n",
    "- agent state = environment state = information state\n",
    "- formally this ia a Markov decision process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Partially Observable Environments__\n",
    "\n",
    "- agent indirectly observes environment\n",
    "- now agent state $\\ne$ environment state\n",
    "- formally this is a partially observable Markov decision process (POMDP)\n",
    "- agent must construct its own state representation $S_t^a$, eg:\n",
    "    - complete history $S_t^a = H_t$\n",
    "    - beliefs of environment state $S_t^a = (P(S_t^e=s^1), \\dots, P(S_t^e=s^n))$\n",
    "    - recurrent neural network: $S_t^a = \\sigma(S_{t-1}^aW_s + O_tW_o)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Major components of an RL agent__\n",
    "\n",
    "- an RL agent may include one or more of these components:\n",
    "  - policy: agent's behavior function\n",
    "  - value function: how good is each state and/or action\n",
    "  - model: agent's representation of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Policy__\n",
    "\n",
    "- a __policy__ is the agent's behavior\n",
    "- it is a map from state to action, eg:\n",
    "- deterministic policy: $a = \\pi(s)$\n",
    "- stochastic policy: $\\pi(a \\mid s) = P(A_t = a \\mid S_t = s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Value Function__\n",
    "\n",
    "- value function is a prediction of __future__ reward\n",
    "- used to evaluate the goodness/badness of states\n",
    "- and therefore to select between actions, eg:\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi(R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model__\n",
    "\n",
    "- a __model__ predicts what the environment will do next\n",
    "- $\\mathcal{P}$ predicts the next state\n",
    "- $\\mathcal{R}$ predicts the next (immediate) reward, eg:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}_{SS'}^a = P(S_{t+1} = s' \\mid S_t = a, A_t = a) \\\\\n",
    "\\mathcal{R}_s^a = \\mathbb{E}[R_{t+a} \\mid S_t = s, A_t = a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Categorizing RL agents (1)__\n",
    "\n",
    "- Value based\n",
    "  - No Policy (implicit)\n",
    "  - Value Function\n",
    "- Policy based\n",
    "  - Policy\n",
    "  - No Value Function\n",
    "- Actor Critic\n",
    "  - Policy\n",
    "  - Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2\n",
    "\n",
    "- MDPs formally describe an environment for reinforcement learning\n",
    "- where the environment is fully observable\n",
    "- the current state completely characterises the process\n",
    "- almost all RL problems can be formalised as MDPs, eg:\n",
    "  - optimal control primarily deals with continuous MDPs\n",
    "  - partially observable problems can be converted into MDPs\n",
    "  - bandits are MDPs with one state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Markov property__\n",
    "\n",
    "- \"the future is independent of the past given the present\"\n",
    "- state captures all relevant information from the history\n",
    "- once the state is known, the history may be thrown away\n",
    "- ie the state is a sufficient statistic of the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__State Transition Matrix__\n",
    "\n",
    "For a Markov state $s$ and successor state $s'$, the __state transition probability__ is defined by:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}_{ss'} = P(S_{t+1} = s' \\mid S_t = s)\n",
    "$$\n",
    "\n",
    "State transition matrix $\\mathcal{P}$ defines transition probabilities from all states $s$ to all successor states $s'$,\n",
    "\n",
    "$$\n",
    "\\mathcal{P} = \\mathcal{P}[\\text{from}][\\text{to}]\n",
    "$$\n",
    "\n",
    "... where each row of the matrix sums to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Markov Process__\n",
    "\n",
    "A __Markov process__ is a memoryless random process, ie a sequence of random states $S_1, S_2, \\dots$ with the Markov property.\n",
    "\n",
    "A __Markov process__ (or __Markov Chain__) is a tuple $<\\mathcal{S}, \\mathcal{P}>$:\n",
    "\n",
    "- $\\mathcal{S}$ is a (finite) set of states\n",
    "- $\\mathcal{P}$ is a state transition probability matrix, $\\mathcal{P}_{ss'} = P(S_{t+1} = s' \\mid S_t = s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Markov Reward Process__\n",
    "\n",
    "A __Markov reward process__ is a Markov chain with values\n",
    "\n",
    "A __Markov Reward Process__ is a tuple $<\\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\gamma>$.\n",
    "\n",
    "- $\\mathcal{S}$ is a finite set of states\n",
    "- $\\mathcal{P}$ is a state transition probability matrix, $\\mathcal{P}_{ss'} = P(S_{t+1} = s' \\mid S_t = s)$\n",
    "- $\\mathcal{R}$ is a reward function, $\\mathcal{R}_s = \\mathbb{E}[R_{t+1} \\mid S_t = s)$\n",
    "- $\\gamma$ is a discount factor, $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Return__\n",
    "\n",
    "The __return__ $G_t$ is the total discounted reward from time-step $t$\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\dots  =\n",
    "\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "- the __discount__ $\\gamma \\in [0,1]$ is the present value of future rewards\n",
    "- the value of receiving reward $R$ after $k+1$ time-steps is $\\gamma^kR$\n",
    "- this values immediate reward above delayed reward\n",
    "  - $\\gamma$ close to $0$ leads to 'myopic' evaluation\n",
    "  - $\\gamma$ close to $1$ leads to 'far-sighted' evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Value Function__\n",
    "\n",
    "The __value function__ $v(s)$ gives the long-term value of state $s$.\n",
    "\n",
    "The __state value function__ $v(s)$ of an MRP is the expected return starting from state $s$:\n",
    "\n",
    "$$\n",
    "v(s) = \\mathbb{E}[G_t \\mid S_t =s ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bellman Equation for MRPs__\n",
    "\n",
    "The value function can be decomposed into two parts:\n",
    "\n",
    "- immediate reward $R_{t+1}$\n",
    "- discounted value of successor state $\\gamma v(S_{t+1})$\n",
    "\n",
    "$$\n",
    "v(s) = \\mathbb{E}[G_t \\mid S_t = s] \\\\\n",
    "= \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s] \\\\\n",
    "= \\mathbb{E}[R_{t+1} + \\gamma(R_{t+2} + \\gamma R_{t+3} + \\dots ) \\mid S_t = s] \\\\\n",
    "= \\mathbb{E}[R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s] \\\\\n",
    "= \\mathcal{R}_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'} v(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bellman Equation in Matrix Form__\n",
    "\n",
    "The Bellman equation can be expressed concisely using matrices:\n",
    "\n",
    "$$\n",
    "v = \\mathcal{R} + \\gamma \\mathcal{P} v\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $v$ is a column vector, with one entry per state\n",
    "- $\\mathcal{R}$ is a column vector with one entry per state\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "v(1) \\\\\n",
    "\\vdots \\\\\n",
    "v(n)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\\mathcal{R}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathcal{R}_n\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\gamma\n",
    "\\begin{bmatrix}\n",
    "\\mathcal{P}_{11} \\dots \\mathcal{P}_{1n} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathcal{P}_{n1} \\dots \\mathcal{P}_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v(1) \\\\\n",
    "\\vdots \\\\\n",
    "v(n)\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solving the Bellman Equation__\n",
    "\n",
    "- the Bellman equation is a linear equation\n",
    "- it can be solved directly:\n",
    "\n",
    "$$\n",
    "v = \\mathcal{R} + \\gamma \\mathcal{P} v \\\\\n",
    "(I - \\gamma \\mathcal{P})v = \\mathcal{R} \\\\\n",
    "v = (I - \\gamma \\mathcal{P})^{-1} \\mathcal{R}\n",
    "$$\n",
    "- computational complexity is $O(n^3)$ for $n$ states\n",
    "- direct solution only possible for small MRPs\n",
    "- many iterative methods for large MRPs, eg:\n",
    "  - dynamic programming\n",
    "  - monte-carlo evaluation\n",
    "  - temporal-difference learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Markov Decision Process__\n",
    "\n",
    "A __Markov decision process (MDP)__ is a Markov reward process with decisions. It is an _environment_ in which all states are Markov.\n",
    "\n",
    "A __Markov Decision Process__ is a tuple $<\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma>$\n",
    "\n",
    "- $\\mathcal{S}$ is a finite set of states\n",
    "- $\\mathcal{A}$ is a finite set of actions\n",
    "- $\\mathcal{P}$ is a state transition probability matrix, $\\mathcal{P}^a_{ss'} = P(S_{t+1} = s' \\mid S_t = s, A_t = a)$\n",
    "- $\\mathcal{R}$ is a reward function, $\\mathcal{R}_s^a = \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t =a ]$\n",
    "- $\\gamma$ is a discount factor $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Policies__\n",
    "\n",
    "A __policy__ $\\pi$ is a distribution over actions given states,\n",
    "\n",
    "$$\n",
    "\\pi(a \\mid s) = P(A_t = a \\mid S_t = s)\n",
    "$$\n",
    "\n",
    "- a policy fully defines the behavior of an agent\n",
    "- MDP policies depend on the current state (not the history)\n",
    "- ie policies are stationary (time-independent), $A_T \\sim \\pi(\\cdot \\mid S_t), \\forall t > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Policies (2)__\n",
    "\n",
    "- given an MDP $\\mathcal{M} = <\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma>$ and a policy $\\pi$\n",
    "- the state sequence $S_1, S_2, \\dots$ is a Markov process $<\\mathcal{S}, \\mathcal{P}^\\pi>$\n",
    "- the state and reward sequence $S_1, R_2, S_2, \\dots$ is a Markov reward process $<\\mathcal{S}, \\mathcal{P}^\\pi, \\mathcal{R}^\\pi, \\gamma>$, where:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}_{s,s'}^\\pi = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s ) \\mathcal{P}_{ss'}^a \\\\\n",
    "\\mathcal{R}_{s}^\\pi = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s ) \\mathcal{R}_{s}^a\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Value function__\n",
    "\n",
    "The __state-value function__ $v_\\pi(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\\pi$\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s]\n",
    "$$\n",
    "\n",
    "The __action-value function__ $q_\\pi(s, a)$ is the expected return starting from state $s$, taking action $a$, and then following policy $\\pi$\n",
    "\n",
    "$$\n",
    "q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s, A_t = a]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bellman Expectation Equation__\n",
    "\n",
    "For value function:\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}[\n",
    "   R_{t+1} + \\gamma v_\\pi(S_{t+1})\n",
    "   \\mid S_t = s\n",
    "]\n",
    "$$\n",
    "\n",
    "action-value function:\n",
    "\n",
    "$$\n",
    "q_\\pi(s, a) = \\mathbb{E}[\n",
    "    R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a\n",
    "]\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a \\in \\mathcal{A}}\n",
    "  \\pi(s, a)\\, q_\\pi(s, a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_\\pi(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q_\\pi(s, a) = \\mathcal{R}^a_s + \\gamma \\sum_{s' \\in \\mathcal{S}}\n",
    "\\mathcal{P}^a_{ss'}\n",
    "\\sum_{a' \\in \\mathcal{A}} \\pi(s', a')\\, q_\\pi(s', a')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(s,a) \\left(\n",
    "   \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{ss'}\n",
    "   v_\\pi(s')\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bellman Expectation Equation (matrix form)__\n",
    "\n",
    "The Bellman expectation equation can be expressed concisely using the induced MRP:\n",
    "\n",
    "$$\n",
    "v_\\pi = \\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi v_\\pi\n",
    "$$\n",
    "\n",
    "with direct solution:\n",
    "\n",
    "$$\n",
    "v_\\pi = (I - \\gamma \\mathcal{P}^\\pi)^{-1} R^\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optimal policy__\n",
    "\n",
    "Define a partial ordering over policies:\n",
    "\n",
    "$$\n",
    "\\pi \\ge \\pi' \\text{ if } v_\\pi(s) \\ge v_{\\pi'}(s), \\forall s\n",
    "$$\n",
    "\n",
    "For any Markov Decision Process:\n",
    "- there exists an optimal policy $\\pi_*$ that is better than or equal to all other policies, $\\pi_* \\ge \\pi, \\forall \\pi$\n",
    "- all optimal policies achieve the optimal value function, $v_{\\pi_*}(s) = v_*(s)$\n",
    "- all optimal policies achieve the optimal action-value function, $q_{\\pi_*}(s, a) = q_*(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bellman Optimality Equation for $v_*$__\n",
    "\n",
    "The optimal value functions are recursively related by the Bellman optimality equations:\n",
    "\n",
    "$$\n",
    "v_*(s) = \\max_a q_*(s, a) \\\\\n",
    "q_*(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_*(s) = \\max_a \\left(\n",
    "   \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s')\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q_*(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}}\n",
    "\\mathcal{P}^a_{ss'} \n",
    "   \\max_{a'} q_*(s', a')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solving the Bellman Optimality Equation__\n",
    "\n",
    "- Bellman Optimality Equation is non-linear\n",
    "- No closed form solution (in general)\n",
    "- Many iterative solution methods:\n",
    "  - value iteration\n",
    "  - policy iteration\n",
    "  - Q-learning\n",
    "  - Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3 Planning by Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is Dynamic Programming__\n",
    "\n",
    "- method for solving complex programs\n",
    "- break down into subproblems\n",
    "  - solve the subproblems\n",
    "  - combine solutions to subproblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Requirements for dynamic programming__\n",
    "\n",
    "- optimal substructure\n",
    "  - optimal solution can be decomposed into subproblems\n",
    "- overlapping subproblems\n",
    "  - subproblems recur many times\n",
    "  - solutions can be cached and reused\n",
    "  \n",
    "- Markov decision processes satisfy both properties\n",
    "  - Bellman equation gives recursive decomposition\n",
    "  - Value function stores and reuses solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Planning by dynamic programming__\n",
    "\n",
    "- DP assumes full knowledge of the MDP\n",
    "- used for _planning_ in an MDP\n",
    "- for prediction:\n",
    "  - input: MDP $<\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma>$ and policy $\\pi$\n",
    "  -    or: MRP $<\\mathcal{S}, \\mathcal{P}^\\pi, \\mathcal{R}^\\pi, \\gamma>$\n",
    "  - output: value function $v_\\pi$\n",
    "- or for control:\n",
    "  - input: MDP $<\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma>$\n",
    "  - output: optimal value function $v_*$\n",
    "  - and optimal policy $\\pi_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Iterative policy evaluation__\n",
    "\n",
    "- problem: evaluate a given policy $\\pi$\n",
    "- solution: iterative application of Bellman expectation backup\n",
    "- $v_1 \\rightarrow v_2 \\rightarrow \\dots \\rightarrow v_\\pi$\n",
    "- using _synchronous_ backups:\n",
    "  - at each iteration $k + 1$\n",
    "  - for all states $s \\in \\mathcal{S}$\n",
    "  - update $v_{k+1}(s)$ from $v_k(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_{k+1}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left(\n",
    "    \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a\n",
    "    v_k(s')\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows 4 cols 4\n",
      "V \n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "R \n",
      " 0 -1 -1 -1\n",
      "-1 -1 -1 -1\n",
      "-1 -1 -1 -1\n",
      "-1 -1 -1  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "V \n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "V_new \n",
      "  0.0000 -13.3536 -18.9126 -20.6242\n",
      "-13.4608 -17.1598 -18.7772 -18.3549\n",
      "-19.2340 -19.0629 -16.6983 -11.6805\n",
      "-21.1956 -19.1763 -13.2877   0.0000\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "gridworld_str = \"\"\"*---\n",
    "----\n",
    "----\n",
    "---*\"\"\"  # * => exit. - => empty square\n",
    "\n",
    "reward_by_symbol = {\n",
    "    '*': 0,\n",
    "    '-': -1\n",
    "}\n",
    "\n",
    "rows = len(gridworld_str.split('\\n'))\n",
    "cols = len(gridworld_str.split('\\n')[0])\n",
    "print('rows %s cols %s' % (rows, cols))\n",
    "V = torch.zeros(rows, cols)\n",
    "print('V', V)\n",
    "\n",
    "gridworld = []\n",
    "gridworld_rows_str = gridworld_str.split('\\n')\n",
    "for r in range(rows):\n",
    "    row_str = gridworld_rows_str[r]\n",
    "    row = []\n",
    "    for c in range(cols):\n",
    "        sym = row_str[c]\n",
    "        row.append(sym)\n",
    "    gridworld.append(row)\n",
    "\n",
    "def get_R(gridworld):\n",
    "    rows = len(gridworld)\n",
    "    cols = len(gridworld[0])\n",
    "    R = torch.zeros(rows, cols)\n",
    "    for r in range(rows):\n",
    "        row = gridworld[r]\n",
    "        for c in range(cols):\n",
    "            char = row[c]\n",
    "            reward = reward_by_symbol[char]\n",
    "            R[r][c] = reward\n",
    "    return R\n",
    "\n",
    "R = get_R(gridworld)\n",
    "print('R', R)\n",
    "\n",
    "def p_a_for_s(s):\n",
    "    action_probs = [\n",
    "        {'a': [0, -1], 'p': 0.25},\n",
    "        {'a': [0, 1], 'p': 0.25},\n",
    "        {'a': [-1, 0], 'p': 0.25},\n",
    "        {'a': [1, 0], 'p': 0.25}\n",
    "    ]\n",
    "    for a_p in action_probs:\n",
    "        a_p['a'] = torch.IntTensor(a_p['a'])\n",
    "    return action_probs\n",
    "\n",
    "def apply_a(s, a):\n",
    "    s_new = s + a\n",
    "    s_new = s_new.clamp(min=0)\n",
    "    s_new[0] = min(s_new[0], rows - 1)\n",
    "    s_new[1] = min(s_new[1], cols - 1)\n",
    "    return s_new\n",
    "\n",
    "def a_to_str(s):\n",
    "    return '\\n'.join(str(a.view(1, -1)).split('\\n')[1:-2])\n",
    "\n",
    "def s_to_str(s):\n",
    "    return '\\n'.join(str(s.view(1, -1)).split('\\n')[1:-2])\n",
    "\n",
    "def is_terminal(s):\n",
    "    return gridworld[s[0]][s[1]] == '*'\n",
    "\n",
    "K = 100\n",
    "print('V', V)\n",
    "for k in range(K):\n",
    "    V_new = torch.zeros(rows, cols)\n",
    "    for s_i in range(rows):\n",
    "        for s_j in range(cols):\n",
    "            s = torch.IntTensor([s_i, s_j])\n",
    "            v_new = 0\n",
    "            if not is_terminal(s):\n",
    "                for a_p in p_a_for_s(s):\n",
    "                    a, p = a_p['a'], a_p['p']\n",
    "                    s_new = apply_a(s, a)\n",
    "                    v_next = V[s_new[0], s_new[1]]\n",
    "                    v_new += p * v_next\n",
    "                r = R[s_new[0], s_new[1]]\n",
    "                v_new += r\n",
    "            V_new[s_i, s_j] = v_new\n",
    "    V = V_new\n",
    "print('V_new', V_new)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
