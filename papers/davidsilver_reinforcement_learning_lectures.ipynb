{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Reinforcement learning\", David Silver\n",
    "\n",
    "- video: https://www.youtube.com/watch?v=2pWv7GOvuf0\n",
    "- slides: http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notation__:\n",
    "- history: $H_t = O_1, R_1, A_1, \\dots, A_{t-1}, O_t, R_t$\n",
    "- State, a function of history: $S_t = f(H_t)$\n",
    "- environment state $S_t^e$\n",
    "- agent state $S_t^a$\n",
    "  - can also be any function of history, $S_t^a = f(H_t)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Information State__\n",
    "\n",
    "A state $S_t$ is Markov is an only if:\n",
    "\n",
    "$$\n",
    "P(S_{t+1} \\mid S_t) = P(S_{t+1} \\mid S_1,\\dots, S_t)\n",
    "$$\n",
    "\n",
    "- the future is independent of the past, given the present:\n",
    "\n",
    "$$\n",
    "H_{1:t} \\rightarrow S_t \\rightarrow H_{t+1:\\infty}\n",
    "$$\n",
    "- once the state is known, the history may be thrown away\n",
    "- the environment state $S_t^e$ is Markov\n",
    "- the history $H_t$ is Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fully Observable Environments__\n",
    "\n",
    "Full observability: agent directly observes agent state:\n",
    "\n",
    "$$\n",
    "O_t = S_t^a = S_t^e\n",
    "$$\n",
    "\n",
    "- agent state = environment state = information state\n",
    "- formally this ia a Markov decision process (MDP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Partially Observable Environments__\n",
    "\n",
    "- agent indirectly observes environment\n",
    "- now agent state $\\ne$ environment state\n",
    "- formally this is a partially observable Markov decision process (POMDP)\n",
    "- agent must construct its own state representation $S_t^a$, eg:\n",
    "    - complete history $S_t^a = H_t$\n",
    "    - beliefs of environment state $S_t^a = (P(S_t^e=s^1), \\dots, P(S_t^e=s^n))$\n",
    "    - recurrent neural network: $S_t^a = \\sigma(S_{t-1}^aW_s + O_tW_o)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Major components of an RL agent__\n",
    "\n",
    "- an RL agent may include one or more of these components:\n",
    "  - policy: agent's behavior function\n",
    "  - value function: how good is each state and/or action\n",
    "  - model: agent's representation of the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Policy__\n",
    "\n",
    "- a __policy__ is the agent's behavior\n",
    "- it is a map from state to action, eg:\n",
    "- deterministic policy: $a = \\pi(s)$\n",
    "- stochastic policy: $\\pi(a \\mid s) = P(A_t = a \\mid S_t = s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Value Function__\n",
    "\n",
    "- value function is a prediction of __future__ reward\n",
    "- used to evaluate the goodness/badness of states\n",
    "- and therefore to select between actions, eg:\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi(R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model__\n",
    "\n",
    "- a __model__ predicts what the environment will do next\n",
    "- $\\mathcal{P}$ predicts the next state\n",
    "- $\\mathcal{R}$ predicts the next (immediate) reward, eg:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}_{SS'}^a = P(S_{t+1} = s' \\mid S_t = a, A_t = a) \\\\\n",
    "\\mathcal{R}_s^a = \\mathbb{E}[R_{t+a} \\mid S_t = s, A_t = a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Categorizing RL agents (1)__\n",
    "\n",
    "- Value based\n",
    "  - No Policy (implicit)\n",
    "  - Value Function\n",
    "- Policy based\n",
    "  - Policy\n",
    "  - No Value Function\n",
    "- Actor Critic\n",
    "  - Policy\n",
    "  - Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 2\n",
    "\n",
    "- MDPs formally describe an environment for reinforcement learning\n",
    "- where the environment is fully observable\n",
    "- the current state completely characterises the process\n",
    "- almost all RL problems can be formalised as MDPs, eg:\n",
    "  - optimal control primarily deals with continuous MDPs\n",
    "  - partially observable problems can be converted into MDPs\n",
    "  - bandits are MDPs with one state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Markov property__\n",
    "\n",
    "- \"the future is independent of the past given the present\"\n",
    "- state captures all relevant information from the history\n",
    "- once the state is known, the history may be thrown away\n",
    "- ie the state is a sufficient statistic of the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__State Transition Matrix__\n",
    "\n",
    "For a Markov state $s$ and successor state $s'$, the __state transition probability__ is defined by:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}_{ss'} = P(S_{t+1} = s' \\mid S_t = s)\n",
    "$$\n",
    "\n",
    "State transition matrix $\\mathcal{P}$ defines transition probabilities from all states $s$ to all successor states $s'$,\n",
    "\n",
    "$$\n",
    "\\mathcal{P} = \\mathcal{P}[\\text{from}][\\text{to}]\n",
    "$$\n",
    "\n",
    "... where each row of the matrix sums to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Markov Process__\n",
    "\n",
    "A __Markov process__ is a memoryless random process, ie a sequence of random states $S_1, S_2, \\dots$ with the Markov property.\n",
    "\n",
    "A __Markov process__ (or __Markov Chain__) is a tuple $<\\mathcal{S}, \\mathcal{P}>$:\n",
    "\n",
    "- $\\mathcal{S}$ is a (finite) set of states\n",
    "- $\\mathcal{P}$ is a state transition probability matrix, $\\mathcal{P}_{ss'} = P(S_{t+1} = s' \\mid S_t = s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Markov Reward Process__\n",
    "\n",
    "A __Markov reward process__ is a Markov chain with values\n",
    "\n",
    "A __Markov Reward Process__ is a tuple $<\\mathcal{S}, \\mathcal{P}, \\mathcal{R}, \\gamma>$.\n",
    "\n",
    "- $\\mathcal{S}$ is a finite set of states\n",
    "- $\\mathcal{P}$ is a state transition probability matrix, $\\mathcal{P}_{ss'} = P(S_{t+1} = s' \\mid S_t = s)$\n",
    "- $\\mathcal{R}$ is a reward function, $\\mathcal{R}_s = \\mathbb{E}[R_{t+1} \\mid S_t = s)$\n",
    "- $\\gamma$ is a discount factor, $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Return__\n",
    "\n",
    "The __return__ $G_t$ is the total discounted reward from time-step $t$\n",
    "\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\dots  =\n",
    "\\sum_{k=0}^\\infty \\gamma^k R_{t+k+1}\n",
    "$$\n",
    "\n",
    "- the __discount__ $\\gamma \\in [0,1]$ is the present value of future rewards\n",
    "- the value of receiving reward $R$ after $k+1$ time-steps is $\\gamma^kR$\n",
    "- this values immediate reward above delayed reward\n",
    "  - $\\gamma$ close to $0$ leads to 'myopic' evaluation\n",
    "  - $\\gamma$ close to $1$ leads to 'far-sighted' evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Value Function__\n",
    "\n",
    "The __value function__ $v(s)$ gives the long-term value of state $s$.\n",
    "\n",
    "The __state value function__ $v(s)$ of an MRP is the expected return starting from state $s$:\n",
    "\n",
    "$$\n",
    "v(s) = \\mathbb{E}[G_t \\mid S_t =s ]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bellman Equation for MRPs__\n",
    "\n",
    "The value function can be decomposed into two parts:\n",
    "\n",
    "- immediate reward $R_{t+1}$\n",
    "- discounted value of successor state $\\gamma v(S_{t+1})$\n",
    "\n",
    "$$\n",
    "v(s) = \\mathbb{E}[G_t \\mid S_t = s] \\\\\n",
    "= \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots \\mid S_t = s] \\\\\n",
    "= \\mathbb{E}[R_{t+1} + \\gamma(R_{t+2} + \\gamma R_{t+3} + \\dots ) \\mid S_t = s] \\\\\n",
    "= \\mathbb{E}[R_{t+1} + \\gamma v(S_{t+1}) \\mid S_t = s] \\\\\n",
    "= \\mathcal{R}_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'} v(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bellman Equation in Matrix Form__\n",
    "\n",
    "The Bellman equation can be expressed concisely using matrices:\n",
    "\n",
    "$$\n",
    "v = \\mathcal{R} + \\gamma \\mathcal{P} v\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "- $v$ is a column vector, with one entry per state\n",
    "- $\\mathcal{R}$ is a column vector with one entry per state\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "v(1) \\\\\n",
    "\\vdots \\\\\n",
    "v(n)\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\\mathcal{R}_1 \\\\\n",
    "\\vdots \\\\\n",
    "\\mathcal{R}_n\n",
    "\\end{bmatrix}\n",
    "+\n",
    "\\gamma\n",
    "\\begin{bmatrix}\n",
    "\\mathcal{P}_{11} \\dots \\mathcal{P}_{1n} \\\\\n",
    "\\vdots \\\\\n",
    "\\mathcal{P}_{n1} \\dots \\mathcal{P}_{nn}\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "v(1) \\\\\n",
    "\\vdots \\\\\n",
    "v(n)\n",
    "\\end{bmatrix}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solving the Bellman Equation__\n",
    "\n",
    "- the Bellman equation is a linear equation\n",
    "- it can be solved directly:\n",
    "\n",
    "$$\n",
    "v = \\mathcal{R} + \\gamma \\mathcal{P} v \\\\\n",
    "(I - \\gamma \\mathcal{P})v = \\mathcal{R} \\\\\n",
    "v = (I - \\gamma \\mathcal{P})^{-1} \\mathcal{R}\n",
    "$$\n",
    "- computational complexity is $O(n^3)$ for $n$ states\n",
    "- direct solution only possible for small MRPs\n",
    "- many iterative methods for large MRPs, eg:\n",
    "  - dynamic programming\n",
    "  - monte-carlo evaluation\n",
    "  - temporal-difference learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Markov Decision Process__\n",
    "\n",
    "A __Markov decision process (MDP)__ is a Markov reward process with decisions. It is an _environment_ in which all states are Markov.\n",
    "\n",
    "A __Markov Decision Process__ is a tuple $<\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma>$\n",
    "\n",
    "- $\\mathcal{S}$ is a finite set of states\n",
    "- $\\mathcal{A}$ is a finite set of actions\n",
    "- $\\mathcal{P}$ is a state transition probability matrix, $\\mathcal{P}^a_{ss'} = P(S_{t+1} = s' \\mid S_t = s, A_t = a)$\n",
    "- $\\mathcal{R}$ is a reward function, $\\mathcal{R}_s^a = \\mathbb{E}[R_{t+1} \\mid S_t = s, A_t =a ]$\n",
    "- $\\gamma$ is a discount factor $\\gamma \\in [0,1]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Policies__\n",
    "\n",
    "A __policy__ $\\pi$ is a distribution over actions given states,\n",
    "\n",
    "$$\n",
    "\\pi(a \\mid s) = P(A_t = a \\mid S_t = s)\n",
    "$$\n",
    "\n",
    "- a policy fully defines the behavior of an agent\n",
    "- MDP policies depend on the current state (not the history)\n",
    "- ie policies are stationary (time-independent), $A_T \\sim \\pi(\\cdot \\mid S_t), \\forall t > 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Policies (2)__\n",
    "\n",
    "- given an MDP $\\mathcal{M} = <\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma>$ and a policy $\\pi$\n",
    "- the state sequence $S_1, S_2, \\dots$ is a Markov process $<\\mathcal{S}, \\mathcal{P}^\\pi>$\n",
    "- the state and reward sequence $S_1, R_2, S_2, \\dots$ is a Markov reward process $<\\mathcal{S}, \\mathcal{P}^\\pi, \\mathcal{R}^\\pi, \\gamma>$, where:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}_{s,s'}^\\pi = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s ) \\mathcal{P}_{ss'}^a \\\\\n",
    "\\mathcal{R}_{s}^\\pi = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s ) \\mathcal{R}_{s}^a\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Value function__\n",
    "\n",
    "The __state-value function__ $v_\\pi(s)$ of an MDP is the expected return starting from state $s$, and then following policy $\\pi$\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t = s]\n",
    "$$\n",
    "\n",
    "The __action-value function__ $q_\\pi(s, a)$ is the expected return starting from state $s$, taking action $a$, and then following policy $\\pi$\n",
    "\n",
    "$$\n",
    "q_\\pi(s, a) = \\mathbb{E}_\\pi[G_t \\mid S_t = s, A_t = a]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bellman Expectation Equation__\n",
    "\n",
    "For value function:\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}[\n",
    "   R_{t+1} + \\gamma v_\\pi(S_{t+1})\n",
    "   \\mid S_t = s\n",
    "]\n",
    "$$\n",
    "\n",
    "action-value function:\n",
    "\n",
    "$$\n",
    "q_\\pi(s, a) = \\mathbb{E}[\n",
    "    R_{t+1} + \\gamma q_\\pi(S_{t+1}, A_{t+1}) \\mid S_t = s, A_t = a\n",
    "]\n",
    "$$\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = \\sum_{a \\in \\mathcal{A}}\n",
    "  \\pi(s, a)\\, q_\\pi(s, a)\n",
    "$$\n",
    "\n",
    "$$\n",
    "q_\\pi(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_\\pi(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q_\\pi(s, a) = \\mathcal{R}^a_s + \\gamma \\sum_{s' \\in \\mathcal{S}}\n",
    "\\mathcal{P}^a_{ss'}\n",
    "\\sum_{a' \\in \\mathcal{A}} \\pi(s', a')\\, q_\\pi(s', a')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(s,a) \\left(\n",
    "   \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{ss'}\n",
    "   v_\\pi(s')\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bellman Expectation Equation (matrix form)__\n",
    "\n",
    "The Bellman expectation equation can be expressed concisely using the induced MRP:\n",
    "\n",
    "$$\n",
    "v_\\pi = \\mathcal{R}^\\pi + \\gamma \\mathcal{P}^\\pi v_\\pi\n",
    "$$\n",
    "\n",
    "with direct solution:\n",
    "\n",
    "$$\n",
    "v_\\pi = (I - \\gamma \\mathcal{P}^\\pi)^{-1} R^\\pi\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Optimal policy__\n",
    "\n",
    "Define a partial ordering over policies:\n",
    "\n",
    "$$\n",
    "\\pi \\ge \\pi' \\text{ if } v_\\pi(s) \\ge v_{\\pi'}(s), \\forall s\n",
    "$$\n",
    "\n",
    "For any Markov Decision Process:\n",
    "- there exists an optimal policy $\\pi_*$ that is better than or equal to all other policies, $\\pi_* \\ge \\pi, \\forall \\pi$\n",
    "- all optimal policies achieve the optimal value function, $v_{\\pi_*}(s) = v_*(s)$\n",
    "- all optimal policies achieve the optimal action-value function, $q_{\\pi_*}(s, a) = q_*(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bellman Optimality Equation for $v_*$__\n",
    "\n",
    "The optimal value functions are recursively related by the Bellman optimality equations:\n",
    "\n",
    "$$\n",
    "v_*(s) = \\max_a q_*(s, a) \\\\\n",
    "q_*(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_*(s) = \\max_a \\left(\n",
    "   \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v_*(s')\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q_*(s, a) = \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}}\n",
    "\\mathcal{P}^a_{ss'} \n",
    "   \\max_{a'} q_*(s', a')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Solving the Bellman Optimality Equation__\n",
    "\n",
    "- Bellman Optimality Equation is non-linear\n",
    "- No closed form solution (in general)\n",
    "- Many iterative solution methods:\n",
    "  - value iteration\n",
    "  - policy iteration\n",
    "  - Q-learning\n",
    "  - Sarsa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture 3 Planning by Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What is Dynamic Programming__\n",
    "\n",
    "- method for solving complex programs\n",
    "- break down into subproblems\n",
    "  - solve the subproblems\n",
    "  - combine solutions to subproblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Requirements for dynamic programming__\n",
    "\n",
    "- optimal substructure\n",
    "  - optimal solution can be decomposed into subproblems\n",
    "- overlapping subproblems\n",
    "  - subproblems recur many times\n",
    "  - solutions can be cached and reused\n",
    "  \n",
    "- Markov decision processes satisfy both properties\n",
    "  - Bellman equation gives recursive decomposition\n",
    "  - Value function stores and reuses solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Planning by dynamic programming__\n",
    "\n",
    "- DP assumes full knowledge of the MDP\n",
    "- used for _planning_ in an MDP\n",
    "- for prediction:\n",
    "  - input: MDP $<\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma>$ and policy $\\pi$\n",
    "  -    or: MRP $<\\mathcal{S}, \\mathcal{P}^\\pi, \\mathcal{R}^\\pi, \\gamma>$\n",
    "  - output: value function $v_\\pi$\n",
    "- or for control:\n",
    "  - input: MDP $<\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma>$\n",
    "  - output: optimal value function $v_*$\n",
    "  - and optimal policy $\\pi_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Iterative policy evaluation__\n",
    "\n",
    "- problem: evaluate a given policy $\\pi$\n",
    "- solution: iterative application of Bellman expectation backup\n",
    "- $v_1 \\rightarrow v_2 \\rightarrow \\dots \\rightarrow v_\\pi$\n",
    "- using _synchronous_ backups:\n",
    "  - at each iteration $k + 1$\n",
    "  - for all states $s \\in \\mathcal{S}$\n",
    "  - update $v_{k+1}(s)$ from $v_k(s')$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_{k+1}(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s) \\left(\n",
    "    \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a\n",
    "    v_k(s')\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows 4 cols 4\n",
      "V \n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      " 0  0  0  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "R \n",
      " 0 -1 -1 -1\n",
      "-1 -1 -1 -1\n",
      "-1 -1 -1 -1\n",
      "-1 -1 -1  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "k 1\n",
      "\n",
      " 0 -1 -1 -1\n",
      "-1 -1 -1 -1\n",
      "-1 -1 -1 -1\n",
      "-1 -1 -1  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "k 2\n",
      "\n",
      " 0.0000 -1.7500 -2.0000 -2.0000\n",
      "-1.7500 -2.0000 -2.0000 -2.0000\n",
      "-2.0000 -2.0000 -2.0000 -1.7500\n",
      "-2.0000 -2.0000 -1.7500  0.0000\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "k 3\n",
      "\n",
      " 0.0000 -2.4375 -2.9375 -3.0000\n",
      "-2.4375 -2.8750 -3.0000 -2.9375\n",
      "-2.9375 -3.0000 -2.8750 -2.4375\n",
      "-3.0000 -2.9375 -2.4375  0.0000\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "V_random \n",
      "  0.0000 -13.9426 -19.9149 -21.9048\n",
      "-13.9426 -17.9251 -19.9155 -19.9149\n",
      "-19.9149 -19.9155 -17.9251 -13.9426\n",
      "-21.9048 -19.9149 -13.9426   0.0000\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "k 1\n",
      "\n",
      " 0 -1 -1 -1\n",
      "-1 -1 -1 -1\n",
      "-1 -1 -1 -1\n",
      "-1 -1 -1  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "k 2\n",
      "\n",
      " 0 -1 -2 -2\n",
      "-1 -2 -2 -2\n",
      "-2 -2 -2 -1\n",
      "-2 -2 -1  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "k 3\n",
      "\n",
      " 0 -1 -2 -3\n",
      "-1 -2 -3 -2\n",
      "-2 -3 -2 -1\n",
      "-3 -2 -1  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "V_optimum \n",
      " 0 -1 -2 -4\n",
      "-1 -2 -3 -2\n",
      "-2 -3 -2 -1\n",
      "-3 -2 -1  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "\n",
    "\n",
    "gridworld_str = \"\"\"*---\n",
    "----\n",
    "----\n",
    "---*\"\"\"  # * => exit. - => empty square\n",
    "\n",
    "reward_by_symbol = {\n",
    "    '*': 0,\n",
    "    '-': -1\n",
    "}\n",
    "\n",
    "rows = len(gridworld_str.split('\\n'))\n",
    "cols = len(gridworld_str.split('\\n')[0])\n",
    "print('rows %s cols %s' % (rows, cols))\n",
    "V = torch.zeros(rows, cols)\n",
    "print('V', V)\n",
    "\n",
    "gridworld = []\n",
    "gridworld_rows_str = gridworld_str.split('\\n')\n",
    "for r in range(rows):\n",
    "    row_str = gridworld_rows_str[r]\n",
    "    row = []\n",
    "    for c in range(cols):\n",
    "        sym = row_str[c]\n",
    "        row.append(sym)\n",
    "    gridworld.append(row)\n",
    "\n",
    "def get_R(gridworld):\n",
    "    rows = len(gridworld)\n",
    "    cols = len(gridworld[0])\n",
    "    R = torch.zeros(rows, cols)\n",
    "    for r in range(rows):\n",
    "        row = gridworld[r]\n",
    "        for c in range(cols):\n",
    "            char = row[c]\n",
    "            reward = reward_by_symbol[char]\n",
    "            R[r][c] = reward\n",
    "    return R\n",
    "\n",
    "R = get_R(gridworld)\n",
    "print('R', R)\n",
    "\n",
    "def random_policy(V, s):\n",
    "    action_probs = [\n",
    "        {'a': [0, -1], 'p': 0.25},\n",
    "        {'a': [0, 1], 'p': 0.25},\n",
    "        {'a': [-1, 0], 'p': 0.25},\n",
    "        {'a': [1, 0], 'p': 0.25}\n",
    "    ]\n",
    "    for a_p in action_probs:\n",
    "        a_p['a'] = torch.IntTensor(a_p['a'])\n",
    "    return action_probs\n",
    "\n",
    "def apply_a(s, a):\n",
    "    s_new = s + a\n",
    "    s_new = s_new.clamp(min=0)\n",
    "    s_new[0] = min(s_new[0], rows - 1)\n",
    "    s_new[1] = min(s_new[1], cols - 1)\n",
    "    return s_new\n",
    "\n",
    "def a_to_str(s):\n",
    "    return '\\n'.join(str(a.view(1, -1)).split('\\n')[1:-2])\n",
    "\n",
    "def s_to_str(s):\n",
    "    return '\\n'.join(str(s.view(1, -1)).split('\\n')[1:-2])\n",
    "\n",
    "def is_terminal(s):\n",
    "    return gridworld[s[0]][s[1]] == '*'\n",
    "\n",
    "K = 100\n",
    "\n",
    "def value_iterate(p_a_for_s):\n",
    "    V = torch.zeros(rows, cols)\n",
    "#     print('V', V)\n",
    "    for k in range(K):\n",
    "        V_new = torch.zeros(rows, cols)\n",
    "        for s_i in range(rows):\n",
    "            for s_j in range(cols):\n",
    "                s = torch.IntTensor([s_i, s_j])\n",
    "                v_new = 0\n",
    "                if not is_terminal(s):\n",
    "                    for a_p in p_a_for_s(V, s):\n",
    "                        a, p = a_p['a'], a_p['p']\n",
    "                        s_new = apply_a(s, a)\n",
    "                        v_next = V[s_new[0], s_new[1]]\n",
    "                        v_new += p * v_next\n",
    "                    r = R[s[0], s[1]]\n",
    "                    v_new += r\n",
    "                V_new[s_i, s_j] = v_new\n",
    "        V = V_new\n",
    "        if k < 3:\n",
    "            print('k', k + 1)\n",
    "            print(V)\n",
    "    return V\n",
    "\n",
    "V_random = value_iterate(random_policy)\n",
    "print('V_random', V_random)\n",
    "\n",
    "def optimum_policy_hardcoded(V, s):\n",
    "    policy = [\n",
    "        ['*', 'l', 'l', 'lr'],\n",
    "        ['u', 'ul', 'lr', 'd'],\n",
    "        ['u', 'ru', 'dr', 'd'],\n",
    "        ['ru', 'r', 'r', '*']\n",
    "    ]\n",
    "    actions = policy[s[0]][s[1]]\n",
    "    p = 1 / len(actions)\n",
    "    action_probs = []\n",
    "    for c in actions:\n",
    "        a = {\n",
    "            'u': [-1, 0],\n",
    "            'd': [1, 0],\n",
    "            'l': [0, -1],\n",
    "            'r': [0, 1]\n",
    "        }[c]\n",
    "        a = torch.IntTensor(a)\n",
    "        action_probs.append({'a': a, 'p': p})\n",
    "#     print(action_probs)\n",
    "    return action_probs\n",
    "\n",
    "V_optimum = value_iterate(optimum_policy_hardcoded)\n",
    "print('V_optimum', V_optimum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__How to improve a policy__\n",
    "\n",
    "- given a policy $\\pi$:\n",
    "  - evaluate the policy $\\pi$: $v_\\pi(s) = \\mathbb{E}[R_{t+1} + \\gamma R_{t+2} + \\dots \\mid S_t = s]$\n",
    "  - improve the policy by acting greedily with respect to $v_\\pi$: $\\pi' = \\text{greedy}(v_\\pi)$\n",
    "- in Small Gridworld improved policy was optimal, $\\pi' = \\pi^*$\n",
    "- in general, need more iterations of improvement/evaluation\n",
    "- but this process of __policy iteration__ always converges to $\\pi_*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Policy improvement__\n",
    "\n",
    "- consider a deterministic policy $a = \\pi(s)$\n",
    "- we can improve the policy by acting greedily:\n",
    "$$\n",
    "\\def\\argmax{\\text{argmax}}\n",
    "\\pi'(s) = \\argmax_{a \\in \\mathcal{A}} q_\\pi(s, a)\n",
    "$$\n",
    "- this improves the value from any state $s$ over one step\n",
    "$$\n",
    "q_\\pi(s, \\pi'(s)) = \\max_{a \\in \\mathcal{A}} q_\\pi(s, a) \\ge q_\\pi(s, \\pi(s)) = v_\\pi(s)\n",
    "$$\n",
    "- it therefore improves the value function, $v_{\\pi'}(s) \\ge v_\\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_\\pi(s) = q_\\pi(s, \\pi(a))\n",
    "= \\mathbb{E}_\\pi[R_{t+1} + \\gamma]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\le q_\\pi(s, \\pi'(a))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\mathbb{E}_\\pi[R_{t+1} + \\gamma v_\\]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Policy improvement (2)_-\n",
    "\n",
    "- if improvements stop:\n",
    "$$\n",
    "v_\\pi(s) = q_\\pi(s, \\pi(s)) = \\max_{a \\in \\mathcal{A}} q_\\pi(s, a)\n",
    "= q_\\pi(s, \\pi'(s))\n",
    "$$\n",
    "\n",
    "ie:\n",
    "$$\n",
    "v_\\pi(s) = \\max_{a \\in \\mathcal{A}}q_\\pi(s, a)\n",
    "$$\n",
    "\n",
    "This is the Bellman optimality equation\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = v_*(s) \\forall s \\in \\mathcal{S}\n",
    "$$\n",
    "\n",
    "Therefore, $\\pi$ is an optimal policy\n",
    "\n",
    "up to 59:15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Notes on v and q__\n",
    "\n",
    "$$\n",
    "v_\\pi(s)  = \\mathbb{E}[R_{t+1} + R_{t+2} + \\dots \\mid S_t = s]\n",
    "$$\n",
    "\n",
    "__For _deterministic_ policy__\n",
    "\n",
    "$$\n",
    "a = \\pi(s) \\\\\n",
    "v_\\pi(s) = q_\\pi(s, \\pi(s))\n",
    "$$\n",
    "\n",
    "If we have the greedy deterministic policy:\n",
    "$$\n",
    "\\pi'(s) = \\argmax_{a \\in \\mathcal{A}} q_\\pi(s, a)\n",
    "$$\n",
    "... then:\n",
    "$$\n",
    "q_{\\pi}(s,\\pi'(s)) = \\max_{a \\in \\mathcal{A}} q_\\pi(s, a)\n",
    "$$\n",
    "And we have:\n",
    "$$\n",
    "\\max_{a \\in \\mathcal{A}} q_\\pi(s, a) \\ge q_\\pi(s, \\pi(s)) = v_\\pi(s)\n",
    "$$\n",
    "and:\n",
    "$$\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_\\pi(s) \\\\\n",
    "= q_\\pi(s, \\pi(s)) \\\\\n",
    "\\le q_\\pi(s, \\pi'(s)) \\\\\n",
    "= \\mathbb{E}_{\\pi'}\\left[R_{t+1} + \\gamma v_\\pi(S_{t+1}) \\mid S_t = s \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\mathbb{E}_{\\pi'}\\left[R_{t+1} + \\gamma q_\\pi(S_{t+1}, \\pi(S_{t+1}) \\mid S_t = s\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\le \\mathbb{E}_{\\pi'} \\left[\n",
    "R_{t+1} + \\gamma q_\\pi(S_{t+1}, \\pi'(S_{t+1})\n",
    "\\mid S_t = s\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\mathbb{E}_{\\pi'} \\left[\n",
    "R_{t+1} + \\gamma R_{t+2} + \\gamma^2 q_\\pi(S_{t+2}, \\pi(S_{t+2})\n",
    "\\mid S_t = s\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\le \\mathbb{E}_{\\pi'} \\left[\n",
    "R_{t+1} + \\gamma R_{t+2} + \\gamma^2 q_\\pi(S_{t+2}, \\pi'(S_{t+2})\n",
    "\\mid S_t = s\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\le \\mathbb{E}_{\\pi'} \\left[\n",
    "R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+3} + \\dots\n",
    "\\mid S_t = s\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=v_{\\pi'}(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k 1\n",
      "\n",
      " 0 -1 -1 -1\n",
      "-1 -1 -1 -1\n",
      "-1 -1 -1 -1\n",
      "-1 -1 -1  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "k 2\n",
      "\n",
      " 0 -1 -2 -2\n",
      "-1 -2 -2 -2\n",
      "-2 -2 -2 -1\n",
      "-2 -2 -1  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "k 3\n",
      "\n",
      " 0 -1 -2 -3\n",
      "-1 -2 -3 -2\n",
      "-2 -3 -2 -1\n",
      "-3 -2 -1  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n",
      "V_optimum \n",
      " 0 -1 -2 -3\n",
      "-1 -2 -3 -2\n",
      "-2 -3 -2 -1\n",
      "-3 -2 -1  0\n",
      "[torch.FloatTensor of size 4x4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def optimum_policy(V, s):\n",
    "    p = 1.0\n",
    "    a_best = None\n",
    "    v_best = None\n",
    "    for a in [\n",
    "        [-1, 0], [1, 0], [0, -1], [0, 1]\n",
    "    ]:\n",
    "        a = torch.IntTensor(a)\n",
    "#         print('s', s)\n",
    "        s_new = apply_a(s, a)\n",
    "        v_new = R[s[0], s[1]] + V[s_new[0], s_new[1]]\n",
    "        if v_best is None or v_new > v_best:\n",
    "            a_best = a\n",
    "            v_best = v_new\n",
    "#     print('a_best', a_best, 'v_best', v_best)\n",
    "    action_probs = [{'a': a_best, 'p': 1.0}]\n",
    "    return action_probs\n",
    "\n",
    "V_optimum = value_iterate(optimum_policy)\n",
    "print('V_optimum', V_optimum)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Principle of Optimality__\n",
    "\n",
    "Any optimal policy can be subdivided into two components:\n",
    "- an optimal first action $A_*$\n",
    "- followed by an optimal policy from successor state $S'$\n",
    "\n",
    "__Theorem (Principle of optimality)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- a policy $\\pi(a \\mid s)$ achieves the optimal value from state $s$, $v_\\pi = v_*(s)$, if and only if:\n",
    "  - for any state $s'$ reachable from $s$\n",
    "  - $\\pi$ achieves the optimal value from state $s'$, $v_\\pi(s') = v_*(s')$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Deterministic value iteration__\n",
    "\n",
    "- if we know the solution to subproblems $v_*(s')$\n",
    "- then solution $v_*(s)$ can be found by one-step lookahead\n",
    "$$\n",
    "v_*(s) \\leftarrow \\max_{a \\in \\mathcal{A}} \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{ss'} v_*(s')\n",
    "$$\n",
    "- the idea of value iteration is to apply these updates iteratively\n",
    "- intuition: start with final rewards and work backwards\n",
    "- still works with loopy, stochastic MDPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Value iteration__\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_{k+1} = \\max_{a \\in \\mathcal{A}} \\mathcal{R}^\\mathbf{a} + \\gamma \\mathcal{P}^\\mathbf{a} \\mathbf{v}_k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Synchronous dynamic programming algorithms__\n",
    "\n",
    "|Problem | Bellman Equation | Algorithm|\n",
    "|----|---|----|\n",
    "| Prediction | Bellman Expectation Equation | Iteractive Policy Evaluation|\n",
    "| Control | Bellman Expectation Equation + Greedy Policy Improvement | Policy Iteration |\n",
    "| Control | Bellman Optimality Equation | Value Iteration |\n",
    "\n",
    "- algorithms are based on state-value function $v_\\pi(s)$ or $v_*(s)$\n",
    "- complexity $O(mn^2)$ per iteration, for $m$ actions and $n$ states\n",
    "- could also apply to action-value function $q_\\pi(s,a)$ or $q_*(s,a)$\n",
    "- complexity $O(m^2n^2)$ per iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Asynchronous dynamic programming__\n",
    "\n",
    "Three simple ideas for asynchronous dynamic programming:\n",
    "- _in-place_ dynamic programming\n",
    "- _prioritized sweeping_\n",
    "- _real-time_ dynamic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In-place dynamic programming__\n",
    "\n",
    "- only store one copy of value function for all $s$ in $\\mathcal{S}$\n",
    "\n",
    "$$\n",
    "v(s) \\leftarrow \\max_{a \\in \\mathcal{A}}\\left(\n",
    "    \\mathcal{R}^a_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v(s')\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prioritized sweeping__\n",
    "\n",
    "- use magnitude of Bellman error to guide state selection, eg:\n",
    "$$\n",
    "\\left|\n",
    "   \\max_{a \\in \\mathcal{A}} \\left(\n",
    "       \\mathcal{R}_s^a + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}_{ss'}^a v(s') - v(s)\n",
    "   \\right)\n",
    "\\right|\n",
    "$$\n",
    "- backup the state with the largest remaining Bellman error\n",
    "- update Bellman error of affected states after each backup\n",
    "- requires knowledge of reverse dynamics (predecessor states)\n",
    "- can be implemented efficiently by maintaining a priority queue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Real-time dynamic programming__\n",
    "\n",
    "- idea: only states that are relevant to agent\n",
    "- use agent's experience to guide the selection of states\n",
    "- after each time-step $S_t$, $A_t$, $R_{t+1}$\n",
    "- backup the state $S_t$\n",
    "$$\n",
    "v(S_t) \\leftarrow \\max_{a \\in \\mathcal{A}} \\left(\n",
    "   \\mathcal{R}^a_{S_t} + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{S_t s'} v(s')\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Full-Width Backups__\n",
    "\n",
    "- DP uses _full-width_ backups\n",
    "- for each backup (sync or async)\n",
    "  - every successor state and action is considered\n",
    "  - using knowledge of the MDP transitions and reward functions\n",
    "- DP is effective for medium-sized problems (millions of states)\n",
    "- for large problems DP suffers Bellman's _curse of dimensionality_\n",
    "  - number of states $n = |\\mathcal{S}|$ grows exponentially with number of state variables\n",
    "- even one backup can be too expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sample backups__\n",
    "\n",
    "- in subsequent lectures we will consider _sample backups_\n",
    "- using sample rewards and sample transitions $<\\mathcal{S}, \\mathcal{A}, \\mathcal{R}, \\mathcal{S'}>$\n",
    "- instead of reward function $\\mathcal{R}$ and transition dynamics $\\mathcal{P}$\n",
    "- advantages:\n",
    "  - model-free: no advance knowledge of MDP required\n",
    "  - breaks the curse of dimensionality through samnpling\n",
    "  - cost of backup is constant, independent of $n = |\\mathcal{S}|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Approximate dynamic programming__\n",
    "\n",
    "- approximate the value function\n",
    "- using a _function approximator_ $\\hat{v}(s, \\mathbf{w})$\n",
    "- apply dynamic programming to $\\hat{v}(\\cdot, \\mathbf{w})$\n",
    "- eg Fitted Value Iteration repeats at each iteration $k$:\n",
    "  - sample states $\\tilde{\\mathcal{S}} \\subseteq \\mathcal{S}$\n",
    "  - for each state $s \\in \\tilde{\\mathcal{S}}$, estimate target value using Bellman optimality equation,\n",
    "$$\n",
    "\\tilde{v}_k = \\max_{a \\in \\mathcal{A}}\\left(\n",
    "  \\mathcal{R}^a_s + \\gamma \\sum_{s' \\in \\mathcal{S}} \\mathcal{P}^a_{ss'}(s', \\mathbf{w}_{\\mathbf{k}})\n",
    "\\right)\n",
    "$$\n",
    "  - train next function $\\hat{v}(\\cdot, \\mathbf{w}_{\\mathbf{k} + \\mathbf{1}})$ using targets $\\{<s, \\tilde{v}_k(s)>\\}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
