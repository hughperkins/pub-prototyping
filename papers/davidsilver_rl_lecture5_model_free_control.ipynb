{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Model Free Control\", David Silver lecture 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model-Free Reinforcement Learning__\n",
    "\n",
    "Last lecture:\n",
    "- model-free prediction\n",
    "- _estimate_ the value function of an unknown MDP\n",
    "\n",
    "This lecture:\n",
    "- model-free control\n",
    "- _optimize_ the value function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Uses of model-free control__\n",
    "\n",
    "Used in cases where either:\n",
    "- MDP model is _unknown_, but experience can be sampled, or\n",
    "- MDP model is _known_, but is too big to use, except by samples\n",
    "\n",
    "Model-free control can solve these problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__On and Off-Policy Learning__\n",
    "\n",
    "On-policy learnin:\n",
    "- \"learn on the job\"\n",
    "- learn about policy $\\pi$ from experience sampled from $\\pi$\n",
    "\n",
    "Off-policy learning:\n",
    "- \"look over someone's shoulder\"\n",
    "- learn about policy $\\pi$ from experience sampled from $\\mu$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generalized policy iteration (refresher)__\n",
    "\n",
    "Policy iteration: estimate $v_\\pi$\n",
    "- eg iterative policy evaluation\n",
    "\n",
    "Policy improvmeent: Generate $\\pi' \\ge \\pi$\n",
    "- eg greedy policy improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generalized policy with monte-carlo evaluation__\n",
    "\n",
    "Policy evaluation: Monte-Carlo policy evalution, $V = v_\\pi$?\n",
    "\n",
    "Policy improvement: greedy policy improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Model-free policy iteration using action-value function__\n",
    "\n",
    "- Greedy policy improvement over $V(s)$ requires model of MDP\n",
    "\n",
    "$$\\def\\argmax{\\text{argmax}}\n",
    "\\pi'(s) = \\argmax_{a \\in \\mathcal{A}} \\mathcal{R}_s^a + \\mathcal{P}_{ss'}^a V(s')\n",
    "$$\n",
    "- greedy policy improvement over $Q(s, a)$ is model-free\n",
    "\n",
    "$$\n",
    "\\pi'(s) = \\argmax_{a \\in \\mathcal{A}} Q(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Generalized policy iteration with action-value function__\n",
    "\n",
    "Policy evaluation: monte-carlo policy evalaution $Q = q_\\pi$\n",
    "\n",
    "Policy improvement: greedy policy improvement?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Greedy exploration__\n",
    "\n",
    "- simplest idea for ensuring continual exploration\n",
    "- all $m$ actions are tried with non-zero probability\n",
    "- with probability $1 - \\epsilon$ choose the greedy action\n",
    "- with probability $\\epsilon$ choose an action at random\n",
    "\n",
    "$$\n",
    "\\pi(a \\mid s) = \\begin{cases}\\epsilon/m + 1 - \\epsilon & \\text{if }a^* = \\argmax_{a \\in \\mathcal{A}} Q(s,a) \\\\\n",
    "\\epsilon / m & \\text{otherwise}\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Greedy policy improvement__\n",
    "\n",
    "__Theorem__ For any $\\epsilon$-greedy policy $\\pi$, the $\\epsilon$-greedy policy $\\pi'$ with respect to $q_\\pi$ is an improvement $v_{\\pi'} \\ge v_\\pi(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_t, \\pi(S_t)) \\mid S_t = s] \\\\\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[R_{t+1} + \\gamma q_\\pi(S_t, \\pi'(S_t)) \\mid S_t = s]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_\\pi(s) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s)\\, q_\\pi(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "v_{\\pi'}(s) = \\sum_{a \\in \\mathcal{A}} \\pi'(a \\mid s) \\, q_{\\pi'}(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q_\\pi(s, \\pi'(s)) = \\sum_{a \\in \\mathcal{A}}\\left(\n",
    "    \\pi'(a \\mid s)\\, q_\\pi(s, a)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\sum_{a \\in \\mathcal{A}} \\left(\n",
    "   \\frac{\\epsilon}{m} q_\\pi(s, a)\n",
    "\\right)\n",
    "+ (1 - \\epsilon)\\, q_\\pi(s, \\argmax_{a} q_\\pi(s, a))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\sum_{a \\in \\mathcal{A}} \\left(\n",
    "   \\frac{\\epsilon}{m} q_\\pi(s, a)\n",
    "\\right)\n",
    "+ (1 - \\epsilon)\\, \\max_{a \\in \\mathcal{A}} q_\\pi(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max over all actions must be at least equal to than any weighted average:\n",
    "\n",
    "$$\n",
    "\\sum_{a \\in \\mathcal{A}} p(a \\mid s)\\, q_\\pi(s, a)\n",
    "$$\n",
    "\n",
    "And we ideally want to form something that combines with the first expression to sum to:\n",
    "\n",
    "$$\n",
    "\\sum_{a \\in \\mathcal{A}} \\pi(s \\mid a) \\, q_\\pi(s,a)\n",
    "$$\n",
    "\n",
    "...which would mean that the second term will be:\n",
    "\n",
    "$$\n",
    "\\pi(s \\mid a) - \\epsilon/m\n",
    "$$\n",
    "\n",
    "... which means that the max term itself, bearing in mind the factor of $(1-\\epsilon)$, should ideally be:\n",
    "\n",
    "$$\n",
    "\\frac{1}{1-\\epsilon}(\\pi(s \\mid a) - \\epsilon/m)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does this sum to 1?  If we sum this over all $m$, the sum will be:\n",
    "\n",
    "$$\n",
    "\\sum_{a \\in \\mathcal{A}} \\frac{\\pi(s \\mid a) - \\epsilon/m}{1 - \\epsilon}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have:\n",
    "\n",
    "$$\n",
    "\\sum_{a \\in \\mathcal{A}} \\pi(s \\mid a) = 1\n",
    "$$\n",
    "\n",
    "... because we are summing over a probability distribution.\n",
    "\n",
    "Similarly, because there are $m$ values for $a$, we have:\n",
    "\n",
    "$$\n",
    "\\sum_{a \\in \\mathcal{A}} \\epsilon/m\n",
    "= m \\epsilon/m\n",
    "= 1\n",
    "$$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\sum_{a \\in \\mathcal{A}} \\frac{\\pi(s,a) - \\epsilon/m}{1 - \\epsilon}\n",
    "= \\frac{1 - \\epsilon}\n",
    "  {1 - \\epsilon}\n",
    "= 1\n",
    "$$\n",
    "\n",
    "... as required"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have:\n",
    "\n",
    "$$\n",
    "v_\\pi(s) = q_\\pi(s, \\pi(s)) = \\sum_{a \\in \\mathcal{A}} \\pi(a \\mid s)\\, q_\\pi(s, a)\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile:\n",
    "\n",
    "$$\n",
    "v_{\\pi'}(s) = q_{\\pi'}(s, \\pi'(s)) = \\sum_{a \\in \\mathcal{A}} \\pi'(a \\mid s)\\, q_{\\pi'}(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use e-greedy just for first action, and then $q_\\pi$ thereafter, ie we want to examine:\n",
    "\n",
    "$$\n",
    "q_\\pi(s, \\pi'(s))\n",
    "$$\n",
    "\n",
    "...where $\\pi'(s)$ is the e-greedy policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "q_\\pi(s, \\pi'(s)) = \\epsilon/m \\sum_{a \\in \\mathcal{A}} q_\\pi(s,a) + (1-\\epsilon) \\max_a q_\\pi(s,a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\ge \\epsilon/m \\sum_{a \\in \\mathcal{A}} q_\\pi(s,a) + (1-\\epsilon) \\sum_{a \\in \\mathcal{A}} \\left(\n",
    "   \\frac{1}{1 - \\epsilon} (\\pi(s \\mid a) - \\epsilon/m) \\, q_\\pi(s, a)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\epsilon/m \\sum_{a \\in \\mathcal{A}} q_\\pi(s,a) + \\sum_{a \\in \\mathcal{A}} (\\pi(s \\mid a) - \\epsilon/m)\\, q_\\pi(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\sum_{a \\in \\mathcal{A}} \\pi(s \\mid a) \\, q_\\pi(s, a)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= v_\\pi(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore:\n",
    "\n",
    "$$\n",
    "q_\\pi(s, \\pi'(s)) \\ge v_\\pi(s)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__GLIE__\n",
    "\n",
    "__definition__ Greedy in the Limit with Infinite Exploration (GLIE)\n",
    "\n",
    "- all state-action pairs are explored infinitely many times,\n",
    "\n",
    "$$\n",
    "\\lim_{k \\rightarrow \\infty} N_k(s, a) = \\infty\n",
    "$$\n",
    "\n",
    "- the policy converges on a greedy policy,\n",
    "\n",
    "$$\n",
    "\\lim_{k \\rightarrow \\infty} \\pi_k(a \\mid s) = \\mathbf{1}(a = \\argmax_{a' \\in \\mathcal{A}} Q_k(s, a'))\n",
    "$$\n",
    "\n",
    "- for example, $\\epsilon$-greedy is GLIE if $\\epsilon$ reduces to zero at $\\epsilon_k = 1/k$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__GLIE Monte-Carlo Control__\n",
    "\n",
    "- sample $k$th episode using $\\pi: \\{S_1, A_1, R_2, \\dots, S_T \\} \\sim \\pi$\n",
    "- for each state $S_t$ and action $A_t$ in the episode,\n",
    "\n",
    "$$\n",
    "N(S_t, A_t) \\leftarrow N(S_t, A_t) + 1 \\\\\n",
    "Q(S_t, A_t) \\leftarrow(S_t, A_t) + \\frac{1}{N(S_t, A_t)} (G_t - Q(S_t, A_t))\n",
    "$$\n",
    "\n",
    "- improve policy based on new action-value function:\n",
    "\n",
    "$$\n",
    "\\epsilon \\leftarrow 1/k \\\\\n",
    "\\pi \\leftarrow \\epsilon\\text{-greedy}(Q)\n",
    "$$\n",
    "\n",
    "__Theorem__ GLIE Monte-Carlo control converges to the optimal action-value function, $Q(s,a) \\rightarrow q_*(s,a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MC vs TD Control__\n",
    "\n",
    "- temporal-difference (TD) learning has several advantages over Monte-Carlo (MC):\n",
    "  - lower variance\n",
    "  - online\n",
    "  - incomplete sequences\n",
    "- natural idea: use TD instead of MC in our control loop\n",
    "  - apply TD to $Q(S,A)$\n",
    "  - use $\\epsilon$-greedy policy improvement\n",
    "  - update every time-step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Update action-value functions with Sarsa__\n",
    "\n",
    "$$\n",
    "Q(S,A) \\leftarrow Q(S,A) + \\alpha\\left(\n",
    "    R + \\gamma Q(S', A') - Q(S,A)\n",
    "\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up to 45:40"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Off-policy learning__\n",
    "\n",
    "- evaluate target policy $\\pi(a \\mid s)$ to compute $v_\\pi(s)$ or $q_\\pi(s,a)$\n",
    "- while following behavior policy $\\mu(a \\mid s)$\n",
    "$$\n",
    "\\{S_1, A_1, R_2, \\dots, S_T \\} \\sim \\mu\n",
    "$$\n",
    "- why is this important?\n",
    "  - learn from observing humans or other agents\n",
    "  - re-use experience generated from old policies $\\pi_1, \\pi_2, \\dots, \\pi_{t-1}$\n",
    "  - learn about optimal policy while following exploratory policy\n",
    "  - learn about multiple policies while following one policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importance Sampling for Off-Policy TD__\n",
    "\n",
    "- use TD targets generated from $\\mu$ to evaluate $\\pi$\n",
    "- weight TD target $R + \\gamma V(S')$ by importance sampling\n",
    "- only need a single importance sampling correction\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) +\n",
    "  \\alpha \\left(\n",
    "      \\frac{\\pi(A_t \\mid S_t)} { \\mu(A_t \\mid S_t)} (R_{t+1} + \\gamma V(S_{t+1})) - V(S_t)\n",
    "  \\right)\n",
    "$$\n",
    "- much lower variance than Monte-Carlo importance sampling\n",
    "- policies only need to be similar over a single step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Q-learning__\n",
    "\n",
    "- we now consider off-policy learning of action-values $Q(s,a)$\n",
    "- no importance sampling is required\n",
    "- next action is chosen using behavior policy $A_{t+1} \\sim \\mu(\\cdot \\mid S_t)$\n",
    "- but we consider alternative successor action $A' \\sim \\pi(\\cdot \\mid S_{t+1})$\n",
    "- and update $Q(S_t, A_t)$ towards value of alternative action\n",
    "\n",
    "$$\n",
    "Q(S_t, A_t) \\leftarrow Q(S_t, A_t) + \\alpha(R_{t+1} + \\gamma Q(S_{t+1}, A') - Q(S_t, A_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Off-Policy control with Q-learning__\n",
    "\n",
    "- we now allow both behavior and target policies to improve\n",
    "- the target policy $\\pi$ is greedy wrt $Q(s,a)$\n",
    "\n",
    "$$\n",
    "\\pi(S_{t+1}) = \\argmax_{a'} Q(S_{t+1}, a')\n",
    "$$\n",
    "\n",
    "- the behavior policy $\\mu$ is eg $\\epsilon$-greedy wrt $Q(s,a)$\n",
    "- the Q-learning target then simplifies:\n",
    "\n",
    "$$\n",
    "R_{t+1} + \\gamma Q(S_{t+1}, A') \\\\\n",
    "= R_{t+1} + \\gamma Q(S_{t+1}, \\argmax_{a'} Q(S_{t+1}, a')) \\\\\n",
    "= R_{t+1} + \\gamma \\max_{a'} Q(S_{t+1}, a')\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Relationship between DP and TD__\n",
    "\n",
    "| Full backup (DP) | Sample backup (TD) |\n",
    "|----|----|\n",
    "|Iterative policy evaluation $V(s) \\leftarrow \\mathbb{E}[R + \\gamma V(S') \\mid s]$ | TD Learning $V(S) \\leftarrow_\\alpha R + \\gamma V(S')$\n",
    "| Q-policy iteration $Q(s,a) \\leftarrow \\mathbb{E}[R + \\gamma Q(S', A') \\mid s,a]$ | Sarsa $Q(S,a) \\leftarrow_\\alpha R + \\gamma Q(S', A')$|\n",
    "| Q-value iteration $Q(s,a) \\leftarrow \\mathbb{E}[R + \\gamma \\max_{a' \\in \\mathcal{A}} Q(S', a') \\mid s,a]$ | Q-learning $Q(S,A) \\leftarrow_\\alpha R + \\gamma \\max_{a' \\in \\mathcal{A}} Q(S', a')$|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
