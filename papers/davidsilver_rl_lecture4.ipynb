{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Model-free prediction\n",
    "\n",
    "- estimate the value function of an _unknown_ MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Monte-Carlo Reinforcement Learning__\n",
    "\n",
    "- MC methods learn directly from episodes of experience\n",
    "- MC is _model-free_: no knowledge of MDP transitions/rewards\n",
    "- MC learns from _complete_ episodes, no bootstrapping\n",
    "- MC uses the simplest possible idea: value = mean return\n",
    "- caveat: can only apply MC to episodic MDPs:\n",
    "  - all episodes must terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Monte-Carlo policy evaluation__\n",
    "\n",
    "- Goal: learn $v_\\pi$ from episodes of experience under policy $\\pi$\n",
    "$$\n",
    "S_1, A_1, R_2, \\dots, S_k \\sim \\pi\n",
    "$$\n",
    "- recall that the _return_ is the total discounted reward\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-1} R_t\n",
    "$$\n",
    "- recall that the value function is the expected return:\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t =s]\n",
    "$$\n",
    "- Monte-Carlo policy evaluation uses _empirical mean_ return instead of _expected_ return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First-visit Monte-Carlo Policy Evaluation__\n",
    "\n",
    "- to evaluate state $s$\n",
    "- the first time-step $t$ that state $s$ is visited in an episode:\n",
    "  - increment counter $N(s) \\leftarrow N(s) + 1$\n",
    "  - increment total return $S(s) \\leftarrow S(s) + G_t$\n",
    "- value is estimated by mean return $V(s) = S(s) / N(s)$\n",
    "- by law of large numbers, $V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Every-visit Monte-Carlo policy evaluation__\n",
    "\n",
    "- to evaluate state $s$:\n",
    "- every time-step $t$ that state $s$ is visited in an episode:\n",
    "  - increment counter $N(s) \\leftarrow N(s) + 1$\n",
    "  - increment total return $S(s) \\leftarrow S(s) + G_t$\n",
    "- value is estimated by mean return $V(s) = S(s) / N(s)$\n",
    "- again, $V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 0\n",
      "G 0\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 0\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G 0\n",
      "G -1\n",
      "G 0\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 0\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = torch.zeros(10, 10, 2)\n",
    "S = torch.zeros(10, 10, 2)\n",
    "\n",
    "def create_cards():\n",
    "    cards = []\n",
    "    for v in range(13):\n",
    "        for i in range(4):\n",
    "            cards.append(v + 1)\n",
    "#     print('cards', cards)\n",
    "    return cards\n",
    "\n",
    "def calc_sum_cards(cards):\n",
    "    sum = 0\n",
    "    num_aces = 0\n",
    "    for card in cards:\n",
    "        if card > 10:\n",
    "            card = 10\n",
    "        if card == 1:\n",
    "            num_aces += 1\n",
    "            card = 11\n",
    "        sum += card\n",
    "    while sum > 21 and num_aces > 0:\n",
    "        sum -= 10\n",
    "        num_aces -= 1\n",
    "    return sum, num_aces\n",
    "\n",
    "def cards_to_state(dealer_card, us):\n",
    "    sum, num_aces = calc_sum_cards(us)\n",
    "    s = torch.IntTensor([sum - 12, dealer_card - 1, 0])\n",
    "    if num_aces > 0:\n",
    "        s[2] = 1\n",
    "    return s, sum > 21\n",
    "\n",
    "def apply_action(cards, s, a):\n",
    "    our_sum = s[0] + 12\n",
    "#     dealer_sum = s[1] + 1\n",
    "    if s[1] == 0:\n",
    "        dealer_cards = [11]\n",
    "        dealer_aces = 1\n",
    "    else:\n",
    "        dealer_cards = [s[1] + 1]\n",
    "        dealer_aces = 0\n",
    "    if a == 0:  # stick\n",
    "        num_aces = 0\n",
    "        while calc_sum_cards(dealer_cards)[0] < 16:\n",
    "            dealer_cards.append(cards[-1])\n",
    "        dealer_sum = calc_sum_cards(dealer_cards)[0]\n",
    "        if dealer_sum > 21:\n",
    "            return cards, _, 1, True\n",
    "        elif dealer_sum == our_sum:\n",
    "            return cards, _, 0, True\n",
    "        else:\n",
    "            return cards, _, -1, True\n",
    "    else:  # twist\n",
    "        card = cards[-1]\n",
    "        cards = cards[:-1]\n",
    "        useable_ace = s[2] == 1\n",
    "        if card > 10:\n",
    "            card = 10\n",
    "        if card == 1:\n",
    "            useable_ace = True\n",
    "            card = 11\n",
    "        our_sum += card\n",
    "        if our_sum > 21 and useable_ace:\n",
    "            our_sum -= 10\n",
    "            useable_ace = False\n",
    "        if our_sum > 21:\n",
    "#             print('bust')\n",
    "            return cards, _, -1, True\n",
    "        else:\n",
    "            s_new = s.clone()\n",
    "            s_new[0] = our_sum - 12\n",
    "            s_new[2] = 1 if useable_ace else 0\n",
    "            return cards, s_new, 0, False\n",
    "#     return cards, s_new, reward\n",
    "\n",
    "episode = 0\n",
    "# while True:\n",
    "while episode < 100:\n",
    "#     print('e %s' % episode)\n",
    "    cards = create_cards()\n",
    "    random.shuffle(cards)\n",
    "#     print('cards', cards)\n",
    "    dealer_card = cards[-1]\n",
    "    if dealer_card > 10:\n",
    "        dealer_card = 10\n",
    "    cards = cards[:-1]\n",
    "    our_cards = []\n",
    "    while calc_sum_cards(our_cards)[0] < 12:\n",
    "        cards = cards[:-1]\n",
    "        our_cards.append(cards[-1])\n",
    "    s, done = cards_to_state(dealer_card, our_cards)\n",
    "    states = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "#         N[s[0], s[1], s[2]] += 1\n",
    "#         S[s[0], s[1], s[2]] += \n",
    "#         print('s', s)\n",
    "        states.append(s.clone())\n",
    "        a = np.random.randint(2)\n",
    "#         print('a', a)\n",
    "        cards, s_new, reward, done = apply_action(cards, s, a)\n",
    "#         print('s_new', s_new)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    G = 0\n",
    "#     print('len(states)', len(states))\n",
    "    for i in range(len(states) - 1, -1, -1):\n",
    "        s = states[i]\n",
    "        r = rewards[i]\n",
    "        G += r\n",
    "#         print('i', i, 's', s, 'r', r, 'G', G)\n",
    "        N[s[0], s[1], s[2]] += 1\n",
    "        S[s[0], s[1], s[2]] += G\n",
    "#     print('G', G)\n",
    "#     print('N', N)\n",
    "#     print('S', S)\n",
    "    episode += 1\n",
    "\n",
    "# plt.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up to 55:10 approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n",
      "it 0\n",
      "V \n",
      "nan\n",
      "nan\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 20\n",
      "V \n",
      "    nan\n",
      " 0.3636\n",
      " 0.5102\n",
      " 0.5882\n",
      " 0.6500\n",
      " 0.7600\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 40\n",
      "V \n",
      "    nan\n",
      " 0.2093\n",
      " 0.4000\n",
      " 0.5368\n",
      " 0.6386\n",
      " 0.7692\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 60\n",
      "V \n",
      "    nan\n",
      " 0.1333\n",
      " 0.2818\n",
      " 0.4655\n",
      " 0.6082\n",
      " 0.7551\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 80\n",
      "V \n",
      "    nan\n",
      " 0.1522\n",
      " 0.3233\n",
      " 0.5067\n",
      " 0.6439\n",
      " 0.8116\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "V \n",
      "    nan\n",
      " 0.1261\n",
      " 0.2542\n",
      " 0.4247\n",
      " 0.6026\n",
      " 0.7901\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Env(object):\n",
    "    def __init__(self):\n",
    "        self.env_size = 7\n",
    "\n",
    "    @property\n",
    "    def num_actions(self):\n",
    "        return 2\n",
    "\n",
    "    @property\n",
    "    def state_shape(self):\n",
    "        return [self.env_size]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.s = torch.IntTensor(1)\n",
    "        self.s[0] = np.random.randint(5) + 1\n",
    "        return self.s\n",
    "        \n",
    "    def act(self, a):\n",
    "        if a == 0:\n",
    "            self.s[0] -= 1\n",
    "        else:\n",
    "            self.s[0] += 1\n",
    "        reward = 1 if self.s[0] == self.env_size - 1 else 0\n",
    "        done = self.s[0] in [0, self.env_size - 1]\n",
    "        return self.s, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        res = ['-'] * self.env_size\n",
    "        res[0] = '*'\n",
    "        res[6] = '*'\n",
    "        res[self.s[0]] = 'X'\n",
    "        print(''.join(res))\n",
    "\n",
    "class RandomPolicy(object):\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def get_action(self, s):\n",
    "        return np.random.randint(self.num_actions)\n",
    "\n",
    "def run_episode(env, policy, render=True):\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    s = env.reset()\n",
    "    while True:\n",
    "        a = policy.get_action(s)\n",
    "        if render:\n",
    "            env.render()\n",
    "        states.append(s.clone())\n",
    "        s, r, done = env.act(a)\n",
    "        if render:\n",
    "            print('a %s r %s' % (a, r))\n",
    "        rewards.append(r)\n",
    "        actions.append(a)\n",
    "        if done:\n",
    "            break\n",
    "#     if render:\n",
    "#         env.render()\n",
    "    return states, actions, rewards\n",
    "\n",
    "env = Env()\n",
    "policy = RandomPolicy(env.num_actions)\n",
    "print(env.state_shape)\n",
    "\n",
    "num_episodes = 5\n",
    "num_episodes = 0\n",
    "for episode in range(num_episodes):\n",
    "    print('')\n",
    "    print('episode %s' % episode)\n",
    "    states, actions, rewards = run_episode(env, policy)\n",
    "\n",
    "def get_linear_index(target, index):\n",
    "    assert len(index.shape) == 1\n",
    "    linear_index = 0\n",
    "    D = index.shape[0]\n",
    "    for d in range(D):\n",
    "        linear_index = linear_index * target.shape[d] + index[d]\n",
    "    return linear_index\n",
    "\n",
    "def tensor_set(target, index, v):\n",
    "    linear_index = get_linear_index(target, index)\n",
    "    target.view(-1)[linear_index] = v\n",
    "\n",
    "def tensor_get(target, index):\n",
    "    linear_index = get_linear_index(target, index)\n",
    "    target = target.view(-1)[linear_index]\n",
    "    return target\n",
    "\n",
    "def tensor_inc(target, index, v):\n",
    "    tensor_set(target, index, tensor_get(target, index) + v)\n",
    "\n",
    "\n",
    "class MonteCarloAllVisits(object):\n",
    "    def __init__(self, env, policy):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.state_shape = env.state_shape\n",
    "        self.N = torch.zeros(self.state_shape)\n",
    "        self.S = torch.zeros(self.state_shape)\n",
    "\n",
    "    def step(self):\n",
    "        states, actions, rewards = run_episode(self.env, self.policy, render=False)\n",
    "        G = 0\n",
    "        for i in range(len(actions) - 1, -1, -1):\n",
    "            s = states[i]\n",
    "            a = actions[i]\n",
    "            r = rewards[i]\n",
    "            G += r\n",
    "            tensor_inc(self.N, s, 1)\n",
    "            tensor_inc(self.S, s, G)\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self.S / self.N\n",
    "\n",
    "runner = MonteCarloAllVisits(env, policy)\n",
    "for it in range(100):\n",
    "    runner.step()\n",
    "    if it % 20 == 0:\n",
    "        print('it', it)\n",
    "        print('V', runner.V)\n",
    "print('V', runner.V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0\n",
      "V \n",
      "nan\n",
      "nan\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 20\n",
      "V \n",
      "    nan\n",
      " 0.0769\n",
      " 0.3333\n",
      " 0.5000\n",
      " 0.5714\n",
      " 0.9000\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 40\n",
      "V \n",
      "    nan\n",
      " 0.0435\n",
      " 0.2917\n",
      " 0.4545\n",
      " 0.6000\n",
      " 0.8261\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 60\n",
      "V \n",
      "    nan\n",
      " 0.1389\n",
      " 0.3514\n",
      " 0.4706\n",
      " 0.6176\n",
      " 0.8108\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 80\n",
      "V \n",
      "    nan\n",
      " 0.1064\n",
      " 0.3191\n",
      " 0.4545\n",
      " 0.6341\n",
      " 0.8478\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "V \n",
      "    nan\n",
      " 0.1053\n",
      " 0.3051\n",
      " 0.4643\n",
      " 0.6471\n",
      " 0.8750\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MonteCarloFirstVisit(object):\n",
    "    def __init__(self, env, policy):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.state_shape = env.state_shape\n",
    "        self.N = torch.zeros(self.state_shape)\n",
    "        self.S = torch.zeros(self.state_shape)\n",
    "#         print('self.N', self.N)\n",
    "\n",
    "    def step(self):\n",
    "        states, actions, rewards = run_episode(self.env, self.policy, render=False)\n",
    "        G = 0\n",
    "#         seen_states = set()\n",
    "        seen_states = torch.zeros(self.state_shape)\n",
    "        seen_states.zero_()\n",
    "        for i in range(len(actions) - 1, -1, -1):\n",
    "            s = states[i]\n",
    "            a = actions[i]\n",
    "            r = rewards[i]\n",
    "            G += r\n",
    "#             print('seen_states', seen_states)\n",
    "#             print('s', s)\n",
    "            if tensor_get(seen_states, s) == 0:\n",
    "                tensor_inc(self.N, s, 1)\n",
    "                tensor_inc(self.S, s, G)\n",
    "                tensor_set(seen_states, s, 1)\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self.S / self.N\n",
    "\n",
    "runner = MonteCarloFirstVisit(env, policy)\n",
    "for it in range(100):\n",
    "    runner.step()\n",
    "    if it % 20 == 0:\n",
    "        print('it', it)\n",
    "        print('V', runner.V)\n",
    "print('V', runner.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0\n",
      "V \n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 10\n",
      "V \n",
      " 0.0000  0.0000  0.0029  0.0195  0.0573  0.4035  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 20\n",
      "V \n",
      " 0.0000  0.0037  0.0301  0.1029  0.2466  0.5270  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 30\n",
      "V \n",
      " 0.0000  0.0301  0.0883  0.1931  0.3675  0.6616  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 40\n",
      "V \n",
      " 0.0000  0.0277  0.0769  0.2083  0.3080  0.6011  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 50\n",
      "V \n",
      " 0.0000  0.0512  0.1254  0.2411  0.3831  0.6242  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 60\n",
      "V \n",
      " 0.0000  0.0809  0.1491  0.3232  0.5130  0.6776  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 70\n",
      "V \n",
      " 0.0000  0.0914  0.2127  0.2844  0.4302  0.6569  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 80\n",
      "V \n",
      " 0.0000  0.0991  0.2589  0.4089  0.5019  0.7534  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 90\n",
      "V \n",
      " 0.0000  0.1230  0.2442  0.3546  0.5294  0.7736  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "V \n",
      " 0.0000  0.1540  0.2136  0.3375  0.5548  0.8322  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TD(object):\n",
    "    def __init__(self, env, policy, alpha=0.1):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.state_shape = env.state_shape\n",
    "        self._V = torch.zeros(self.state_shape)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def step(self, render=False):\n",
    "        states, actions, rewards = run_episode(self.env, self.policy, render=render)\n",
    "        is_terminal = True\n",
    "        s_next = None\n",
    "        for t in range(len(actions) - 1, -1, -1):\n",
    "#             print('t', t)\n",
    "            s = states[t]\n",
    "            a = actions[t]\n",
    "            r = rewards[t]\n",
    "            v_old = tensor_get(self._V, s)\n",
    "            v_next = 0\n",
    "            if s_next is not None:\n",
    "                v_next = tensor_get(self.V, s_next)\n",
    "            v_new = v_old + self.alpha * (r + v_next - v_old)\n",
    "#             if s_next is not None:\n",
    "#                 print('s', s[0], 'a', a, 'r', r, 's_next', s_next[0], 'v_new', v_new)\n",
    "#             else:\n",
    "#                 print('s', s[0], 'a', a, 'r', r, 'v_new', v_new)\n",
    "            tensor_set(self._V, s, v_new)\n",
    "            is_terminal = False\n",
    "            s_next = s\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self._V\n",
    "\n",
    "runner = TD(env, policy)\n",
    "for it in range(100):\n",
    "#     if random.randint(0, 2) == 0:\n",
    "    runner.step()\n",
    "    if it % 10 == 0:\n",
    "        print('it', it)\n",
    "        print('V', runner.V.view(1, -1))\n",
    "print('V', runner.V.view(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MC and TD__\n",
    "\n",
    "- Goal: learn $v_\\pi$ online from experience under policy $\\pi$\n",
    "- incremental every-visit Monte-Carlo:\n",
    "  - update value $V(S_t)$ toward _actual_ return $G_t$\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha(G_t - V(S_t))\n",
    "$$\n",
    "- simplest temporal-difference learning algorithm TD(0):\n",
    "  - update value $V(S_t)$ toward _estimated_ return $R_{t+1} + \\gamma V(S_{t+1})$\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))\n",
    "$$\n",
    "  - $R_{t+1} + \\gamma V(S_{t+1})$ is called the __TD target__\n",
    "  - $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$ is called the __TD error__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCAV V \n",
      "    nan  0.1758  0.3437  0.5091  0.6725  0.8376     nan\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "MCFV V \n",
      "    nan  0.1640  0.3292  0.4962  0.6622  0.8386     nan\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1776  0.2607  0.3970  0.6658  0.7973  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runner = MonteCarloAllVisits(env, policy)\n",
    "for it in range(10000):\n",
    "    runner.step()\n",
    "print('MCAV V', runner.V.view(1, -1))\n",
    "\n",
    "runner = MonteCarloFirstVisit(env, policy)\n",
    "for it in range(10000):\n",
    "    runner.step()\n",
    "print('MCFV V', runner.V.view(1, -1))\n",
    "\n",
    "runner = TD(env, policy)\n",
    "for it in range(10000):\n",
    "    runner.step()\n",
    "print('TD V', runner.V.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD V it 0 \n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 10000 \n",
      " 0.0000  0.1052  0.2769  0.3896  0.5638  0.8184  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 20000 \n",
      " 0.0000  0.1944  0.3405  0.4428  0.5752  0.8185  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 30000 \n",
      " 0.0000  0.1234  0.3096  0.5227  0.6978  0.7968  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 40000 \n",
      " 0.0000  0.1044  0.2875  0.5020  0.7644  0.8913  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 50000 \n",
      " 0.0000  0.1372  0.3139  0.4940  0.6016  0.7570  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 60000 \n",
      " 0.0000  0.1267  0.3496  0.5076  0.6906  0.8221  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 70000 \n",
      " 0.0000  0.1593  0.3139  0.4863  0.7222  0.7864  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 80000 \n",
      " 0.0000  0.1585  0.3963  0.5546  0.7426  0.8313  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 90000 \n",
      " 0.0000  0.0946  0.2947  0.4559  0.6787  0.8369  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1029  0.2977  0.4515  0.6418  0.7740  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "runner = TD(env, policy, alpha=alpha)\n",
    "N = 100000\n",
    "for it in range(N):\n",
    "    if it % (N // 10) == 0:\n",
    "        print('TD V it %s' % it, runner.V.view(1, -1))\n",
    "    runner.step()\n",
    "print('TD V', runner.V.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.1 std \n",
      "1.00000e-02 *\n",
      "  0.0000  5.2341  6.8739  7.6727  7.2182  5.6797  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1962  0.3097  0.4281  0.5776  0.7676  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "alpha 0.05 std \n",
      "1.00000e-02 *\n",
      "  0.0000  3.6696  5.0679  6.0162  6.2872  5.6891  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1691  0.3729  0.4791  0.5851  0.7925  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "alpha 0.02 std \n",
      "1.00000e-02 *\n",
      "  0.0000  3.3570  5.6238  7.2149  7.9160  6.8365  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1301  0.3007  0.4490  0.6213  0.8123  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "alpha 0.01 std \n",
      " 0.0000  0.0403  0.0725  0.0948  0.1024  0.0880  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1461  0.3294  0.5035  0.6783  0.8442  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "alpha 0.001 std \n",
      " 0.0000  0.0416  0.0877  0.1355  0.1743  0.1808  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1259  0.2692  0.4244  0.5897  0.7857  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "alpha 0.0001 std \n",
      "1.00000e-02 *\n",
      "  0.0000  0.0109  0.1104  0.6690  2.8985  9.3392  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.0004  0.0040  0.0228  0.0940  0.3286  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print('V_sum', V_sum)\n",
    "\n",
    "for alpha in [0.1, 0.05, 0.02, 0.01, 0.001, 0.0001]:\n",
    "#     alpha = 0.1\n",
    "    V_sum = torch.zeros(env.state_shape)\n",
    "    Vsquared_sum = torch.zeros(env.state_shape)\n",
    "    count = 0\n",
    "    runner = TD(env, policy, alpha=alpha)\n",
    "    N = 10000\n",
    "    for it in range(N):\n",
    "    #     if it % (N // 10) == 0:\n",
    "    #         print('TD V it %s' % it, runner.V.view(1, -1))\n",
    "        runner.step()\n",
    "        if N > N / 1000:\n",
    "            V_sum += runner.V\n",
    "            Vsquared_sum += runner.V * runner.V\n",
    "            count += 1\n",
    "    # print('V_sum', V_sum)\n",
    "    # print('Vsquared_sum', Vsquared_sum)\n",
    "    # variance is mean of (x - mu)^2\n",
    "    # = E[(x - mu)^2]\n",
    "    # = E[x^2 - 2x mu + mu^2]\n",
    "    # = E[x^2] - 2E[x]^2 + E[x]^2\n",
    "    #  = mean of x^2 - 2x mu + mu^2\n",
    "    # = mean of x^2 - mu^2\n",
    "    variance = Vsquared_sum / count - V_sum * V_sum / count / count\n",
    "    # print('variance', variance.view(1, -1))\n",
    "    std = variance.sqrt()\n",
    "    print('alpha %s' % alpha, 'std', std.view(1, -1))\n",
    "    print('TD V', runner.V.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD V it 500 \n",
      " 0.6664  0.7404\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "TD V \n",
      " 0.6664  0.7404\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "MCAV V it 500 \n",
      " 0.0000  0.7500\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "MCAV V \n",
      " 0.0000  0.7500\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "MCFV V it 500 \n",
      " 0.0000  0.7500\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "MCFV V \n",
      " 0.0000  0.7500\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ABEnv(object):\n",
    "    def __init__(self):\n",
    "        self.episode = 0\n",
    "\n",
    "    @property\n",
    "    def num_actions(self):\n",
    "        return 1\n",
    "\n",
    "    @property\n",
    "    def state_shape(self):\n",
    "        return [2]\n",
    "    \n",
    "    def reset_all(self):\n",
    "        self.episode = -1\n",
    "    \n",
    "    def reset(self):\n",
    "#         print('reset episode=%s' % self.episode)\n",
    "        self.episode += 1\n",
    "        self.s = torch.IntTensor(1)\n",
    "        if self.episode > 0:\n",
    "            self.s[0] = 1\n",
    "        else:\n",
    "            self.s[0] = 0\n",
    "        return self.s\n",
    "        \n",
    "    def act(self, a):\n",
    "        if self.s[0] == 0:\n",
    "            self.s[0] = 1\n",
    "            return self.s, 0, False\n",
    "        else:\n",
    "            if self.episode in [0, 7]:\n",
    "                return None, 0, True\n",
    "            else:\n",
    "                return None, 1, True\n",
    "    \n",
    "    def render(self):\n",
    "        res = ['.', '.']\n",
    "        res[self.s[0]] = 'X'\n",
    "        print(res)\n",
    "    \n",
    "N = 1000\n",
    "ab_env = ABEnv()\n",
    "policy = RandomPolicy(num_actions=ab_env.num_actions)\n",
    "render = False\n",
    "\n",
    "runner = TD(ab_env, policy, alpha=0.1)\n",
    "for it in range(N):\n",
    "    ab_env.reset_all()\n",
    "    for episode in range(8):\n",
    "        if render:\n",
    "            print('it', it)\n",
    "        runner.step(render=render)\n",
    "    if it > 0 and it % (N // 2) == 0:\n",
    "        print('TD V it %s' % it, runner.V.view(1, -1))\n",
    "print('TD V', runner.V.view(1, -1))\n",
    "\n",
    "runner = MonteCarloAllVisits(ab_env, policy)\n",
    "for it in range(N):\n",
    "    ab_env.reset_all()\n",
    "    for episode in range(8):\n",
    "#         print('it', it)\n",
    "        runner.step()\n",
    "    if it > 0 and it % (N // 2) == 0:\n",
    "        print('MCAV V it %s' % it, runner.V.view(1, -1))\n",
    "print('MCAV V', runner.V.view(1, -1))\n",
    "\n",
    "runner = MonteCarloFirstVisit(ab_env, policy)\n",
    "for it in range(N):\n",
    "    ab_env.reset_all()\n",
    "    for episode in range(8):\n",
    "#         print('it', it)\n",
    "        runner.step()\n",
    "    if it > 0 and it % (N // 2) == 0:\n",
    "        print('MCFV V it %s' % it, runner.V.view(1, -1))\n",
    "print('MCFV V', runner.V.view(1, -1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MC Convergence__\n",
    "\n",
    "MC converges to solution with minimum MSE (why?):\n",
    "\n",
    "$$\\def\\argmin{\\text{argmin}}\n",
    "\\argmin_{V} \\mathcal{L} = \\sum_{k=1}^K \\sum_{t=1}^{T_k} (g_t^k - V(s_t^k))^2\n",
    "$$\n",
    "\n",
    "For each example, using $\\mathbf{V} = \\{0, 0.75 \\}$:\n",
    "\n",
    "| $\\tau$ | $\\mathbf{g}$ | MSE |\n",
    "|-------|---------|-------|\n",
    "| A,0,B,0 | 0,0 | 0^2 + 0.75^2 = 0.5625 |\n",
    "| B, 1 | 1 | 0.25 ^ 2 = 0.0625 |\n",
    "| ... | ... | ... |\n",
    "| B, 0 | 0 | 0.75^2 = 0.5625 |\n",
    "\n",
    "- changing value for $A$ will increase the MSE for the first example, and wont change the MSE for any other examples\n",
    "- changing value for $B$ will increase the MSE for some examples, decrease for others, and with the end results that overall MSE increases\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TD Convergence__\n",
    "\n",
    "TD(0) converges to solution of max likelihood Markov model:\n",
    "\n",
    "$$\n",
    "\\mathcal{P}_{ss'}^a = \\frac{1}{N(s,a)} \\sum_{k=1}^K \\sum_{t=1}^{T_k} \\mathbf{1}(s_t^k, a_t^k, s_{t+1}^k = s, a, s') \\\\\n",
    "\\mathcal{R}_s^a = \\frac{1}{N(s,a)} \\sum_{k=1}^K \\sum_{t=1}^{T_k} \\mathbf{1}(s_t^k, a_t^k = s, a)r_t^k\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each example, using $\\mathbf{V} = \\{0.75, 0.75\\}$:\n",
    "\n",
    "| $\\tau$ | (s,r,s') tuples |\n",
    "|-------|--------|\n",
    "| A,0,B,0 | (A,0,B) (B,0,\\*) |\n",
    "| B,1 | (B,1,\\*) |\n",
    "| B,0 | (B,0,\\*) |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__transition probabilities__\n",
    "\n",
    "| s, s' | N(s) | N(s,s') | P |\n",
    "|--------|--------|-----|---|\n",
    "| A, B | 1 | 1 | 1.0 |\n",
    "| B, R0 | 8 | 2 | 0.25 |\n",
    "| B, R1 | 8 | 6 | 0.75 |\n",
    "\n",
    "__rewards__\n",
    "\n",
    "| s | N(s) | sum rewards | R |\n",
    "|---|----|-----|---|\n",
    "| A | 1 | 0 | 0 |\n",
    "| B | 8 | 6 | 0.75 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bootstrapping and sampling__\n",
    "\n",
    "__bootstrapping__: update involves an estimate\n",
    "  - MC does not bootstrap\n",
    "  - TD bootstraps\n",
    "  - DP bootstraps\n",
    "\n",
    "__sampling__: update samples an expectation\n",
    "  - MC samples\n",
    "  - TD samples\n",
    "  - DP doesnt sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__unified view of RL__\n",
    "\n",
    "|  | shallow backups | deep backups |\n",
    "|--|--------|--------|\n",
    "| full backups | DP | Exhaustive search |\n",
    "| sample backups | TD | MC |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up to 1:15:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__n-Step Prediction__\n",
    "\n",
    "- let TD target look $n$ steps into the future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__n-Step Return__\n",
    "\n",
    "- consider the following n-step returns for $n=1,2,\\infty$:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "n = 1 & \\hspace{24px} & (TD) & \\hspace{24px} & G_t^{(1)} = R_{t+1} + \\gamma V(S_{t+1}) \\\\\n",
    "n = 2 & & & & G_t^{(2)} + \\gamma R_{t+1} + R_{t+2} + \\gamma^2 V(S_t + 2) \\\\\n",
    "\\vdots & & & & \\vdots \\\\\n",
    "n = \\infty & & (MC) & & G_t^{(\\infty)} = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-1} R_T\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "- define the n-step return:\n",
    "\n",
    "$$\n",
    "G_t^{n} = R_{t+1} + \\gamma R_{t+2} + \\gamma^2 R_{t+2} + \\dots + \\gamma^{n-1}R_{t+n} + \\gamma^n V(S_{t+n})\n",
    "$$\n",
    "\n",
    "- n-step temporal-difference learning\n",
    "\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha(G_t^{(n)} - V(S_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__$\\lambda$-return__\n",
    "\n",
    "- the $\\lambda$-return $G_t^\\lambda$ combines all $n$-step returns $G_t^{(n)}$\n",
    "- using weight $(1-\\lambda)\\lambda^{n-1}$\n",
    "$$\n",
    "G_t^\\lambda = (1-\\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1} G_t^{(n)}\n",
    "$$\n",
    "- forward-view $TD(\\lambda)$:\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha(G_t^\\lambda - V(S_t))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TD($\\lambda$) Weighting Function__\n",
    "\n",
    "$$\n",
    "G_t^\\lambda = (1 - \\lambda) \\sum_{n=1}^\\infty \\lambda^{n-1} G_t^{(n)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sum_{t=t}^T G_t^\\lambda = 1\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Backward view TD($\\lambda$)__\n",
    "\n",
    "- forward view provides theory\n",
    "- backward view provides mechanism\n",
    "- update online, every step, from incomplete sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Eligibility Traces__\n",
    "\n",
    "$$\n",
    "E_0(s) = 0 \\\\\n",
    "E_t(s) = \\gamma \\lambda E_{t-1}(s) + \\mathbf{1}(S_t = s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Backward view TD($\\lambda$)__\n",
    "\n",
    "- keep an eligibility trace for every state $s$\n",
    "- update value $V(s)$ for every state $s$\n",
    "- in proportion to TD-error $\\delta_t$ and eligibility trace $E_t(s)$:\n",
    "\n",
    "$$\n",
    "\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t) \\\\\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha \\delta_t E_t(s)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0\n",
      "V \n",
      " 0.0000  0.0000  0.0000  0.0000  0.0000  0.1000  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 100\n",
      "V \n",
      " 0.0000  0.1065  0.2425  0.4207  0.6129  0.8431  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 200\n",
      "V \n",
      " 0.0000  0.0757  0.1881  0.4000  0.5574  0.8156  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 300\n",
      "V \n",
      " 0.0000  0.0806  0.3245  0.4481  0.7244  0.8945  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 400\n",
      "V \n",
      " 0.0000  0.1656  0.3054  0.4808  0.6269  0.7876  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "V \n",
      " 0.0000  0.1588  0.3540  0.5242  0.6726  0.8711  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class Runner(object):\n",
    "    def __init__(self, env, policy, learner):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.learner = learner\n",
    "\n",
    "    def run_episode(self, render=True):\n",
    "        s = self.env.reset()\n",
    "        while True:\n",
    "            a = self.policy.get_action(s)\n",
    "            if render:\n",
    "                self.env.render()\n",
    "            s_old = s.clone()\n",
    "            s_new, r, done = env.act(a)\n",
    "            if render:\n",
    "                print('a %s r %s' % (a, r))\n",
    "            learner.step(s_old, a, r, s_new, done, render=render)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "class TD0_Online(object):\n",
    "    def __init__(self, env, alpha=0.1):\n",
    "        self.env = env\n",
    "        self.state_shape = env.state_shape\n",
    "        self._V = torch.zeros(self.state_shape)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    @property\n",
    "    def expects_online(self):\n",
    "        return True\n",
    "\n",
    "    def step(self, s, a, r, s_new, done, render=False):\n",
    "        v_old = tensor_get(self._V, s)\n",
    "        v_next = 0\n",
    "        if s_new is not None:\n",
    "            v_next = tensor_get(self.V, s_new)\n",
    "        v_new = v_old + self.alpha * (r + v_next - v_old)\n",
    "        tensor_set(self._V, s, v_new)\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self._V\n",
    "\n",
    "learner = TD0_Online(env)\n",
    "runner = Runner(env=env, policy=policy, learner=learner)\n",
    "for it in range(500):\n",
    "    render = it % 100 == 0\n",
    "    runner.run_episode(render=False)\n",
    "    if render:\n",
    "        print('it', it)\n",
    "        print('V', learner.V.view(1, -1))\n",
    "print('V', learner.V.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-17-f999d334b47b>, line 16)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-f999d334b47b>\"\u001b[0;36m, line \u001b[0;32m16\u001b[0m\n\u001b[0;31m    self.eligibility *= self.lambda\u001b[0m\n\u001b[0m                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class TDLambdaBackward_Online(object):\n",
    "    def __init__(self, env, gamma=1.0, alpha=0.1, lambda_=0.9):\n",
    "        self.env = env\n",
    "        self.state_shape = env.state_shape\n",
    "        self._V = torch.zeros(self.state_shape)\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.lambda_ = lambda_\n",
    "        self.eligibility = torch.zeros(self.state_shape)\n",
    "\n",
    "    @property\n",
    "    def expects_online(self):\n",
    "        return True\n",
    "\n",
    "    def step(self, s, a, r, s_new, done, render=False):\n",
    "        self.eligibility *= self.lambda_\n",
    "        tensor_inc(self.eligibility, s, 1)\n",
    "\n",
    "        v_old = tensor_get(self._V, s)\n",
    "        v_next = 0\n",
    "        if s_new is not None:\n",
    "            v_next = tensor_get(self.V, s_new)\n",
    "        td_error = r + self.gamma * v_next - v_old\n",
    "        for i in range(self.state_shape[0]):\n",
    "            if self.eligibility[i] > 1e-3:\n",
    "                e_state = torch.IntTensor([i])\n",
    "                v_old = tensor_get(self._V, e_state)\n",
    "                v_new = v_old + self.alpha * td_error * self.eligibility[i]\n",
    "                tensor_set(self._V, e_state, v_new)\n",
    "                \n",
    "#         v_new = v_old + self.alpha * (r + v_next - v_old)\n",
    "#         tensor_set(self._V, s, v_new)\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self._V\n",
    "\n",
    "learner = TDLambdaBackward_Online(env)\n",
    "runner = Runner(env=env, policy=policy, learner=learner)\n",
    "for it in range(50):\n",
    "    render = it % 10 == 0\n",
    "    runner.run_episode(render=False)\n",
    "    if render:\n",
    "        print('it', it)\n",
    "        print('V', learner.V.view(1, -1))\n",
    "print('V', learner.V.view(1, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
