{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Model-free prediction\n",
    "\n",
    "- estimate the value function of an _unknown_ MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Monte-Carlo Reinforcement Learning__\n",
    "\n",
    "- MC methods learn directly from episodes of experience\n",
    "- MC is _model-free_: no knowledge of MDP transitions/rewards\n",
    "- MC learns from _complete_ episodes, no bootstrapping\n",
    "- MC uses the simplest possible idea: value = mean return\n",
    "- caveat: can only apply MC to episodic MDPs:\n",
    "  - all episodes must terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Monte-Carlo policy evaluation__\n",
    "\n",
    "- Goal: learn $v_\\pi$ from episodes of experience under policy $\\pi$\n",
    "$$\n",
    "S_1, A_1, R_2, \\dots, S_k \\sim \\pi\n",
    "$$\n",
    "- recall that the _return_ is the total discounted reward\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-1} R_t\n",
    "$$\n",
    "- recall that the value function is the expected return:\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t =s]\n",
    "$$\n",
    "- Monte-Carlo policy evaluation uses _empirical mean_ return instead of _expected_ return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First-visit Monte-Carlo Policy Evaluation__\n",
    "\n",
    "- to evaluate state $s$\n",
    "- the first time-step $t$ that state $s$ is visited in an episode:\n",
    "  - increment counter $N(s) \\leftarrow N(s) + 1$\n",
    "  - increment total return $S(s) \\leftarrow S(s) + G_t$\n",
    "- value is estimated by mean return $V(s) = S(s) / N(s)$\n",
    "- by law of large numbers, $V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Every-visit Monte-Carlo policy evaluation__\n",
    "\n",
    "- to evaluate state $s$:\n",
    "- every time-step $t$ that state $s$ is visited in an episode:\n",
    "  - increment counter $N(s) \\leftarrow N(s) + 1$\n",
    "  - increment total return $S(s) \\leftarrow S(s) + G_t$\n",
    "- value is estimated by mean return $V(s) = S(s) / N(s)$\n",
    "- again, $V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 0\n",
      "G 0\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 0\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G 0\n",
      "G -1\n",
      "G 0\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 0\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = torch.zeros(10, 10, 2)\n",
    "S = torch.zeros(10, 10, 2)\n",
    "\n",
    "def create_cards():\n",
    "    cards = []\n",
    "    for v in range(13):\n",
    "        for i in range(4):\n",
    "            cards.append(v + 1)\n",
    "#     print('cards', cards)\n",
    "    return cards\n",
    "\n",
    "def calc_sum_cards(cards):\n",
    "    sum = 0\n",
    "    num_aces = 0\n",
    "    for card in cards:\n",
    "        if card > 10:\n",
    "            card = 10\n",
    "        if card == 1:\n",
    "            num_aces += 1\n",
    "            card = 11\n",
    "        sum += card\n",
    "    while sum > 21 and num_aces > 0:\n",
    "        sum -= 10\n",
    "        num_aces -= 1\n",
    "    return sum, num_aces\n",
    "\n",
    "def cards_to_state(dealer_card, us):\n",
    "    sum, num_aces = calc_sum_cards(us)\n",
    "    s = torch.IntTensor([sum - 12, dealer_card - 1, 0])\n",
    "    if num_aces > 0:\n",
    "        s[2] = 1\n",
    "    return s, sum > 21\n",
    "\n",
    "def apply_action(cards, s, a):\n",
    "    our_sum = s[0] + 12\n",
    "#     dealer_sum = s[1] + 1\n",
    "    if s[1] == 0:\n",
    "        dealer_cards = [11]\n",
    "        dealer_aces = 1\n",
    "    else:\n",
    "        dealer_cards = [s[1] + 1]\n",
    "        dealer_aces = 0\n",
    "    if a == 0:  # stick\n",
    "        num_aces = 0\n",
    "        while calc_sum_cards(dealer_cards)[0] < 16:\n",
    "            dealer_cards.append(cards[-1])\n",
    "        dealer_sum = calc_sum_cards(dealer_cards)[0]\n",
    "        if dealer_sum > 21:\n",
    "            return cards, _, 1, True\n",
    "        elif dealer_sum == our_sum:\n",
    "            return cards, _, 0, True\n",
    "        else:\n",
    "            return cards, _, -1, True\n",
    "    else:  # twist\n",
    "        card = cards[-1]\n",
    "        cards = cards[:-1]\n",
    "        useable_ace = s[2] == 1\n",
    "        if card > 10:\n",
    "            card = 10\n",
    "        if card == 1:\n",
    "            useable_ace = True\n",
    "            card = 11\n",
    "        our_sum += card\n",
    "        if our_sum > 21 and useable_ace:\n",
    "            our_sum -= 10\n",
    "            useable_ace = False\n",
    "        if our_sum > 21:\n",
    "#             print('bust')\n",
    "            return cards, _, -1, True\n",
    "        else:\n",
    "            s_new = s.clone()\n",
    "            s_new[0] = our_sum - 12\n",
    "            s_new[2] = 1 if useable_ace else 0\n",
    "            return cards, s_new, 0, False\n",
    "#     return cards, s_new, reward\n",
    "\n",
    "episode = 0\n",
    "# while True:\n",
    "while episode < 100:\n",
    "#     print('e %s' % episode)\n",
    "    cards = create_cards()\n",
    "    random.shuffle(cards)\n",
    "#     print('cards', cards)\n",
    "    dealer_card = cards[-1]\n",
    "    if dealer_card > 10:\n",
    "        dealer_card = 10\n",
    "    cards = cards[:-1]\n",
    "    our_cards = []\n",
    "    while calc_sum_cards(our_cards)[0] < 12:\n",
    "        cards = cards[:-1]\n",
    "        our_cards.append(cards[-1])\n",
    "    s, done = cards_to_state(dealer_card, our_cards)\n",
    "    states = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "#         N[s[0], s[1], s[2]] += 1\n",
    "#         S[s[0], s[1], s[2]] += \n",
    "#         print('s', s)\n",
    "        states.append(s.clone())\n",
    "        a = np.random.randint(2)\n",
    "#         print('a', a)\n",
    "        cards, s_new, reward, done = apply_action(cards, s, a)\n",
    "#         print('s_new', s_new)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    G = 0\n",
    "#     print('len(states)', len(states))\n",
    "    for i in range(len(states) - 1, -1, -1):\n",
    "        s = states[i]\n",
    "        r = rewards[i]\n",
    "        G += r\n",
    "#         print('i', i, 's', s, 'r', r, 'G', G)\n",
    "        N[s[0], s[1], s[2]] += 1\n",
    "        S[s[0], s[1], s[2]] += G\n",
    "#     print('G', G)\n",
    "#     print('N', N)\n",
    "#     print('S', S)\n",
    "    episode += 1\n",
    "\n",
    "# plt.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up to 55:10 approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n",
      "it 0\n",
      "V \n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "  1\n",
      "  1\n",
      "nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 20\n",
      "V \n",
      "    nan\n",
      " 0.2333\n",
      " 0.4375\n",
      " 0.5775\n",
      " 0.6327\n",
      " 0.7600\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 40\n",
      "V \n",
      "    nan\n",
      " 0.1875\n",
      " 0.3889\n",
      " 0.5800\n",
      " 0.7308\n",
      " 0.8696\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 60\n",
      "V \n",
      "    nan\n",
      " 0.1884\n",
      " 0.3740\n",
      " 0.5588\n",
      " 0.7411\n",
      " 0.8824\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 80\n",
      "V \n",
      "    nan\n",
      " 0.2737\n",
      " 0.4364\n",
      " 0.5778\n",
      " 0.7305\n",
      " 0.8929\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "V \n",
      "    nan\n",
      " 0.2547\n",
      " 0.4294\n",
      " 0.5700\n",
      " 0.7126\n",
      " 0.8667\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Env(object):\n",
    "    def __init__(self):\n",
    "        self.env_size = 7\n",
    "\n",
    "    @property\n",
    "    def num_actions(self):\n",
    "        return 2\n",
    "\n",
    "    @property\n",
    "    def state_shape(self):\n",
    "        return [self.env_size]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.s = torch.IntTensor(1)\n",
    "        self.s[0] = np.random.randint(5) + 1\n",
    "        return self.s\n",
    "        \n",
    "    def act(self, a):\n",
    "        if a == 0:\n",
    "            self.s[0] -= 1\n",
    "        else:\n",
    "            self.s[0] += 1\n",
    "        reward = 1 if self.s[0] == self.env_size - 1 else 0\n",
    "        done = self.s[0] in [0, self.env_size - 1]\n",
    "        return self.s, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        res = ['-'] * self.env_size\n",
    "        res[0] = '*'\n",
    "        res[6] = '*'\n",
    "        res[self.s[0]] = 'X'\n",
    "        print(''.join(res))\n",
    "\n",
    "class RandomPolicy(object):\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def get_action(self, s):\n",
    "        return np.random.randint(self.num_actions)\n",
    "\n",
    "def run_episode(env, policy, render=True):\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    s = env.reset()\n",
    "    while True:\n",
    "        a = policy.get_action(s)\n",
    "        if render:\n",
    "            env.render()\n",
    "        states.append(s.clone())\n",
    "        s, r, done = env.act(a)\n",
    "        rewards.append(r)\n",
    "        actions.append(a)\n",
    "        if done:\n",
    "            break\n",
    "    if render:\n",
    "        env.render()\n",
    "    return states, actions, rewards\n",
    "\n",
    "env = Env()\n",
    "policy = RandomPolicy(env.num_actions)\n",
    "print(env.state_shape)\n",
    "\n",
    "num_episodes = 5\n",
    "num_episodes = 0\n",
    "for episode in range(num_episodes):\n",
    "    print('')\n",
    "    print('episode %s' % episode)\n",
    "    states, actions, rewards = run_episode(env, policy)\n",
    "\n",
    "def get_linear_index(target, index):\n",
    "    assert len(index.shape) == 1\n",
    "    linear_index = 0\n",
    "    D = index.shape[0]\n",
    "    for d in range(D):\n",
    "        linear_index = linear_index * target.shape[d] + index[d]\n",
    "    return linear_index\n",
    "\n",
    "def tensor_set(target, index, v):\n",
    "    linear_index = get_linear_index(target, index)\n",
    "    target.view(-1)[linear_index] = v\n",
    "\n",
    "def tensor_get(target, index):\n",
    "    linear_index = get_linear_index(target, index)\n",
    "    target = target.view(-1)[linear_index]\n",
    "    return target\n",
    "\n",
    "def tensor_inc(target, index, v):\n",
    "    tensor_set(target, index, tensor_get(target, index) + v)\n",
    "\n",
    "\n",
    "class MonteCarloAllVisits(object):\n",
    "    def __init__(self, env, policy):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.state_shape = env.state_shape\n",
    "        self.N = torch.zeros(self.state_shape)\n",
    "        self.S = torch.zeros(self.state_shape)\n",
    "\n",
    "    def step(self):\n",
    "        states, actions, rewards = run_episode(self.env, self.policy, render=False)\n",
    "        G = 0\n",
    "        for i in range(len(actions) - 1, -1, -1):\n",
    "            s = states[i]\n",
    "            a = actions[i]\n",
    "            r = rewards[i]\n",
    "            G += r\n",
    "            tensor_inc(self.N, s, 1)\n",
    "            tensor_inc(self.S, s, G)\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self.S / self.N\n",
    "\n",
    "runner = MonteCarloAllVisits(env, policy)\n",
    "for it in range(100):\n",
    "    runner.step()\n",
    "    if it % 20 == 0:\n",
    "        print('it', it)\n",
    "        print('V', runner.V)\n",
    "print('V', runner.V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0\n",
      "V \n",
      "nan\n",
      "nan\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 20\n",
      "V \n",
      "    nan\n",
      " 0.0769\n",
      " 0.3333\n",
      " 0.5000\n",
      " 0.5714\n",
      " 0.9000\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 40\n",
      "V \n",
      "    nan\n",
      " 0.0435\n",
      " 0.2917\n",
      " 0.4545\n",
      " 0.6000\n",
      " 0.8261\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 60\n",
      "V \n",
      "    nan\n",
      " 0.1389\n",
      " 0.3514\n",
      " 0.4706\n",
      " 0.6176\n",
      " 0.8108\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 80\n",
      "V \n",
      "    nan\n",
      " 0.1064\n",
      " 0.3191\n",
      " 0.4545\n",
      " 0.6341\n",
      " 0.8478\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "V \n",
      "    nan\n",
      " 0.1053\n",
      " 0.3051\n",
      " 0.4643\n",
      " 0.6471\n",
      " 0.8750\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MonteCarloFirstVisit(object):\n",
    "    def __init__(self, env, policy):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.state_shape = env.state_shape\n",
    "        self.N = torch.zeros(self.state_shape)\n",
    "        self.S = torch.zeros(self.state_shape)\n",
    "#         print('self.N', self.N)\n",
    "\n",
    "    def step(self):\n",
    "        states, actions, rewards = run_episode(self.env, self.policy, render=False)\n",
    "        G = 0\n",
    "#         seen_states = set()\n",
    "        seen_states = torch.zeros(self.state_shape)\n",
    "        seen_states.zero_()\n",
    "        for i in range(len(actions) - 1, -1, -1):\n",
    "            s = states[i]\n",
    "            a = actions[i]\n",
    "            r = rewards[i]\n",
    "            G += r\n",
    "#             print('seen_states', seen_states)\n",
    "#             print('s', s)\n",
    "            if tensor_get(seen_states, s) == 0:\n",
    "                tensor_inc(self.N, s, 1)\n",
    "                tensor_inc(self.S, s, G)\n",
    "                tensor_set(seen_states, s, 1)\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self.S / self.N\n",
    "\n",
    "runner = MonteCarloFirstVisit(env, policy)\n",
    "for it in range(100):\n",
    "    runner.step()\n",
    "    if it % 20 == 0:\n",
    "        print('it', it)\n",
    "        print('V', runner.V)\n",
    "print('V', runner.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0\n",
      "V \n",
      "1.00000e-02 *\n",
      "  0.0000  0.0000  0.0000  0.0000  1.0000  9.1000  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 10\n",
      "V \n",
      " 0.0000  0.0004  0.0019  0.0080  0.1113  0.5169  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 20\n",
      "V \n",
      " 0.0000  0.0004  0.0094  0.0976  0.2611  0.6264  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 30\n",
      "V \n",
      " 0.0000  0.0078  0.0608  0.1788  0.3369  0.7119  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 40\n",
      "V \n",
      " 0.0000  0.0266  0.0729  0.1870  0.3586  0.7280  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 50\n",
      "V \n",
      " 0.0000  0.0324  0.0932  0.2360  0.5290  0.7264  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 60\n",
      "V \n",
      " 0.0000  0.0605  0.1522  0.3186  0.4720  0.7512  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 70\n",
      "V \n",
      " 0.0000  0.0620  0.1878  0.3706  0.5095  0.7253  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 80\n",
      "V \n",
      " 0.0000  0.1460  0.2115  0.3761  0.5274  0.6731  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 90\n",
      "V \n",
      " 0.0000  0.1405  0.2069  0.3296  0.5687  0.8048  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "V \n",
      " 0.0000  0.1192  0.2243  0.3354  0.5385  0.7701  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TD(object):\n",
    "    def __init__(self, env, policy, alpha=0.1):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.state_shape = env.state_shape\n",
    "        self._V = torch.zeros(self.state_shape)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def step(self):\n",
    "        states, actions, rewards = run_episode(self.env, self.policy, render=False)\n",
    "        is_terminal = True\n",
    "        s_next = None\n",
    "        for t in range(len(actions) - 1, -1, -1):\n",
    "#             print('t', t)\n",
    "            s = states[t]\n",
    "            a = actions[t]\n",
    "            r = rewards[t]\n",
    "            v_old = tensor_get(self._V, s)\n",
    "            v_next = 0\n",
    "            if s_next is not None:\n",
    "                v_next = tensor_get(self.V, s_next)\n",
    "            v_new = v_old + self.alpha * (r + v_next - v_old)\n",
    "#             if s_next is not None:\n",
    "#                 print('s', s[0], 'a', a, 'r', r, 's_next', s_next[0], 'v_new', v_new)\n",
    "#             else:\n",
    "#                 print('s', s[0], 'a', a, 'r', r, 'v_new', v_new)\n",
    "            tensor_set(self._V, s, v_new)\n",
    "            is_terminal = False\n",
    "            s_next = s\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self._V\n",
    "\n",
    "runner = TD(env, policy)\n",
    "for it in range(100):\n",
    "#     if random.randint(0, 2) == 0:\n",
    "    runner.step()\n",
    "    if it % 10 == 0:\n",
    "        print('it', it)\n",
    "        print('V', runner.V.view(1, -1))\n",
    "print('V', runner.V.view(1, -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MC and TD__\n",
    "\n",
    "- Goal: learn $v_\\pi$ online from experience under policy $\\pi$\n",
    "- incremental every-visit Monte-Carlo:\n",
    "  - update value $V(S_t)$ toward _actual_ return $G_t$\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha(G_t - V(S_t))\n",
    "$$\n",
    "- simplest temporal-difference learning algorithm TD(0):\n",
    "  - update value $V(S_t)$ toward _estimated_ return $R_{t+1} + \\gamma V(S_{t+1})$\n",
    "$$\n",
    "V(S_t) \\leftarrow V(S_t) + \\alpha(R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))\n",
    "$$\n",
    "  - $R_{t+1} + \\gamma V(S_{t+1})$ is called the __TD target__\n",
    "  - $\\delta_t = R_{t+1} + \\gamma V(S_{t+1}) - V(S_t)$ is called the __TD error__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCAV V \n",
      "    nan  0.1758  0.3437  0.5091  0.6725  0.8376     nan\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "MCFV V \n",
      "    nan  0.1640  0.3292  0.4962  0.6622  0.8386     nan\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1776  0.2607  0.3970  0.6658  0.7973  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "runner = MonteCarloAllVisits(env, policy)\n",
    "for it in range(10000):\n",
    "    runner.step()\n",
    "print('MCAV V', runner.V.view(1, -1))\n",
    "\n",
    "runner = MonteCarloFirstVisit(env, policy)\n",
    "for it in range(10000):\n",
    "    runner.step()\n",
    "print('MCFV V', runner.V.view(1, -1))\n",
    "\n",
    "runner = TD(env, policy)\n",
    "for it in range(10000):\n",
    "    runner.step()\n",
    "print('TD V', runner.V.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TD V it 0 \n",
      "    0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 10000 \n",
      " 0.0000  0.1052  0.2769  0.3896  0.5638  0.8184  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 20000 \n",
      " 0.0000  0.1944  0.3405  0.4428  0.5752  0.8185  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 30000 \n",
      " 0.0000  0.1234  0.3096  0.5227  0.6978  0.7968  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 40000 \n",
      " 0.0000  0.1044  0.2875  0.5020  0.7644  0.8913  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 50000 \n",
      " 0.0000  0.1372  0.3139  0.4940  0.6016  0.7570  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 60000 \n",
      " 0.0000  0.1267  0.3496  0.5076  0.6906  0.8221  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 70000 \n",
      " 0.0000  0.1593  0.3139  0.4863  0.7222  0.7864  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 80000 \n",
      " 0.0000  0.1585  0.3963  0.5546  0.7426  0.8313  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V it 90000 \n",
      " 0.0000  0.0946  0.2947  0.4559  0.6787  0.8369  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1029  0.2977  0.4515  0.6418  0.7740  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "runner = TD(env, policy, alpha=alpha)\n",
    "N = 100000\n",
    "for it in range(N):\n",
    "    if it % (N // 10) == 0:\n",
    "        print('TD V it %s' % it, runner.V.view(1, -1))\n",
    "    runner.step()\n",
    "print('TD V', runner.V.view(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0.1 std \n",
      "1.00000e-02 *\n",
      "  0.0000  5.2341  6.8739  7.6727  7.2182  5.6797  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1962  0.3097  0.4281  0.5776  0.7676  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "alpha 0.05 std \n",
      "1.00000e-02 *\n",
      "  0.0000  3.6696  5.0679  6.0162  6.2872  5.6891  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1691  0.3729  0.4791  0.5851  0.7925  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "alpha 0.02 std \n",
      "1.00000e-02 *\n",
      "  0.0000  3.3570  5.6238  7.2149  7.9160  6.8365  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1301  0.3007  0.4490  0.6213  0.8123  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "alpha 0.01 std \n",
      " 0.0000  0.0403  0.0725  0.0948  0.1024  0.0880  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1461  0.3294  0.5035  0.6783  0.8442  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "alpha 0.001 std \n",
      " 0.0000  0.0416  0.0877  0.1355  0.1743  0.1808  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.1259  0.2692  0.4244  0.5897  0.7857  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "alpha 0.0001 std \n",
      "1.00000e-02 *\n",
      "  0.0000  0.0109  0.1104  0.6690  2.8985  9.3392  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "TD V \n",
      " 0.0000  0.0004  0.0040  0.0228  0.0940  0.3286  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print('V_sum', V_sum)\n",
    "\n",
    "for alpha in [0.1, 0.05, 0.02, 0.01, 0.001, 0.0001]:\n",
    "#     alpha = 0.1\n",
    "    V_sum = torch.zeros(env.state_shape)\n",
    "    Vsquared_sum = torch.zeros(env.state_shape)\n",
    "    count = 0\n",
    "    runner = TD(env, policy, alpha=alpha)\n",
    "    N = 10000\n",
    "    for it in range(N):\n",
    "    #     if it % (N // 10) == 0:\n",
    "    #         print('TD V it %s' % it, runner.V.view(1, -1))\n",
    "        runner.step()\n",
    "        if N > N / 1000:\n",
    "            V_sum += runner.V\n",
    "            Vsquared_sum += runner.V * runner.V\n",
    "            count += 1\n",
    "    # print('V_sum', V_sum)\n",
    "    # print('Vsquared_sum', Vsquared_sum)\n",
    "    # variance is mean of (x - mu)^2\n",
    "    # = E[(x - mu)^2]\n",
    "    # = E[x^2 - 2x mu + mu^2]\n",
    "    # = E[x^2] - 2E[x]^2 + E[x]^2\n",
    "    #  = mean of x^2 - 2x mu + mu^2\n",
    "    # = mean of x^2 - mu^2\n",
    "    variance = Vsquared_sum / count - V_sum * V_sum / count / count\n",
    "    # print('variance', variance.view(1, -1))\n",
    "    std = variance.sqrt()\n",
    "    print('alpha %s' % alpha, 'std', std.view(1, -1))\n",
    "    print('TD V', runner.V.view(1, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
