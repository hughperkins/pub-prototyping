{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 4: Model-free prediction\n",
    "\n",
    "- estimate the value function of an _unknown_ MDP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Monte-Carlo Reinforcement Learning__\n",
    "\n",
    "- MC methods learn directly from episodes of experience\n",
    "- MC is _model-free_: no knowledge of MDP transitions/rewards\n",
    "- MC learns from _complete_ episodes, no bootstrapping\n",
    "- MC uses the simplest possible idea: value = mean return\n",
    "- caveat: can only apply MC to episodic MDPs:\n",
    "  - all episodes must terminate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Monte-Carlo policy evaluation__\n",
    "\n",
    "- Goal: learn $v_\\pi$ from episodes of experience under policy $\\pi$\n",
    "$$\n",
    "S_1, A_1, R_2, \\dots, S_k \\sim \\pi\n",
    "$$\n",
    "- recall that the _return_ is the total discounted reward\n",
    "$$\n",
    "G_t = R_{t+1} + \\gamma R_{t+2} + \\dots + \\gamma^{T-1} R_t\n",
    "$$\n",
    "- recall that the value function is the expected return:\n",
    "$$\n",
    "v_\\pi(s) = \\mathbb{E}_\\pi[G_t \\mid S_t =s]\n",
    "$$\n",
    "- Monte-Carlo policy evaluation uses _empirical mean_ return instead of _expected_ return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__First-visit Monte-Carlo Policy Evaluation__\n",
    "\n",
    "- to evaluate state $s$\n",
    "- the first time-step $t$ that state $s$ is visited in an episode:\n",
    "  - increment counter $N(s) \\leftarrow N(s) + 1$\n",
    "  - increment total return $S(s) \\leftarrow S(s) + G_t$\n",
    "- value is estimated by mean return $V(s) = S(s) / N(s)$\n",
    "- by law of large numbers, $V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Every-visit Monte-Carlo policy evaluation__\n",
    "\n",
    "- to evaluate state $s$:\n",
    "- every time-step $t$ that state $s$ is visited in an episode:\n",
    "  - increment counter $N(s) \\leftarrow N(s) + 1$\n",
    "  - increment total return $S(s) \\leftarrow S(s) + G_t$\n",
    "- value is estimated by mean return $V(s) = S(s) / N(s)$\n",
    "- again, $V(s) \\rightarrow v_\\pi(s)$ as $N(s) \\rightarrow \\infty$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 0\n",
      "G 0\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 0\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G 0\n",
      "G -1\n",
      "G 0\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 0\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G -1\n",
      "G 1\n",
      "G -1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "N = torch.zeros(10, 10, 2)\n",
    "S = torch.zeros(10, 10, 2)\n",
    "\n",
    "def create_cards():\n",
    "    cards = []\n",
    "    for v in range(13):\n",
    "        for i in range(4):\n",
    "            cards.append(v + 1)\n",
    "#     print('cards', cards)\n",
    "    return cards\n",
    "\n",
    "def calc_sum_cards(cards):\n",
    "    sum = 0\n",
    "    num_aces = 0\n",
    "    for card in cards:\n",
    "        if card > 10:\n",
    "            card = 10\n",
    "        if card == 1:\n",
    "            num_aces += 1\n",
    "            card = 11\n",
    "        sum += card\n",
    "    while sum > 21 and num_aces > 0:\n",
    "        sum -= 10\n",
    "        num_aces -= 1\n",
    "    return sum, num_aces\n",
    "\n",
    "def cards_to_state(dealer_card, us):\n",
    "    sum, num_aces = calc_sum_cards(us)\n",
    "    s = torch.IntTensor([sum - 12, dealer_card - 1, 0])\n",
    "    if num_aces > 0:\n",
    "        s[2] = 1\n",
    "    return s, sum > 21\n",
    "\n",
    "def apply_action(cards, s, a):\n",
    "    our_sum = s[0] + 12\n",
    "#     dealer_sum = s[1] + 1\n",
    "    if s[1] == 0:\n",
    "        dealer_cards = [11]\n",
    "        dealer_aces = 1\n",
    "    else:\n",
    "        dealer_cards = [s[1] + 1]\n",
    "        dealer_aces = 0\n",
    "    if a == 0:  # stick\n",
    "        num_aces = 0\n",
    "        while calc_sum_cards(dealer_cards)[0] < 16:\n",
    "            dealer_cards.append(cards[-1])\n",
    "        dealer_sum = calc_sum_cards(dealer_cards)[0]\n",
    "        if dealer_sum > 21:\n",
    "            return cards, _, 1, True\n",
    "        elif dealer_sum == our_sum:\n",
    "            return cards, _, 0, True\n",
    "        else:\n",
    "            return cards, _, -1, True\n",
    "    else:  # twist\n",
    "        card = cards[-1]\n",
    "        cards = cards[:-1]\n",
    "        useable_ace = s[2] == 1\n",
    "        if card > 10:\n",
    "            card = 10\n",
    "        if card == 1:\n",
    "            useable_ace = True\n",
    "            card = 11\n",
    "        our_sum += card\n",
    "        if our_sum > 21 and useable_ace:\n",
    "            our_sum -= 10\n",
    "            useable_ace = False\n",
    "        if our_sum > 21:\n",
    "#             print('bust')\n",
    "            return cards, _, -1, True\n",
    "        else:\n",
    "            s_new = s.clone()\n",
    "            s_new[0] = our_sum - 12\n",
    "            s_new[2] = 1 if useable_ace else 0\n",
    "            return cards, s_new, 0, False\n",
    "#     return cards, s_new, reward\n",
    "\n",
    "episode = 0\n",
    "# while True:\n",
    "while episode < 100:\n",
    "#     print('e %s' % episode)\n",
    "    cards = create_cards()\n",
    "    random.shuffle(cards)\n",
    "#     print('cards', cards)\n",
    "    dealer_card = cards[-1]\n",
    "    if dealer_card > 10:\n",
    "        dealer_card = 10\n",
    "    cards = cards[:-1]\n",
    "    our_cards = []\n",
    "    while calc_sum_cards(our_cards)[0] < 12:\n",
    "        cards = cards[:-1]\n",
    "        our_cards.append(cards[-1])\n",
    "    s, done = cards_to_state(dealer_card, our_cards)\n",
    "    states = []\n",
    "    rewards = []\n",
    "    while True:\n",
    "#         N[s[0], s[1], s[2]] += 1\n",
    "#         S[s[0], s[1], s[2]] += \n",
    "#         print('s', s)\n",
    "        states.append(s.clone())\n",
    "        a = np.random.randint(2)\n",
    "#         print('a', a)\n",
    "        cards, s_new, reward, done = apply_action(cards, s, a)\n",
    "#         print('s_new', s_new)\n",
    "        rewards.append(reward)\n",
    "        if done:\n",
    "            break\n",
    "    G = 0\n",
    "#     print('len(states)', len(states))\n",
    "    for i in range(len(states) - 1, -1, -1):\n",
    "        s = states[i]\n",
    "        r = rewards[i]\n",
    "        G += r\n",
    "#         print('i', i, 's', s, 'r', r, 'G', G)\n",
    "        N[s[0], s[1], s[2]] += 1\n",
    "        S[s[0], s[1], s[2]] += G\n",
    "#     print('G', G)\n",
    "#     print('N', N)\n",
    "#     print('S', S)\n",
    "    episode += 1\n",
    "\n",
    "# plt.imshow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "up to 55:10 approx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7]\n",
      "it 0\n",
      "V \n",
      "nan\n",
      "nan\n",
      "nan\n",
      "nan\n",
      "  1\n",
      "  1\n",
      "nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 20\n",
      "V \n",
      "    nan\n",
      " 0.2333\n",
      " 0.4375\n",
      " 0.5775\n",
      " 0.6327\n",
      " 0.7600\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 40\n",
      "V \n",
      "    nan\n",
      " 0.1875\n",
      " 0.3889\n",
      " 0.5800\n",
      " 0.7308\n",
      " 0.8696\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 60\n",
      "V \n",
      "    nan\n",
      " 0.1884\n",
      " 0.3740\n",
      " 0.5588\n",
      " 0.7411\n",
      " 0.8824\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 80\n",
      "V \n",
      "    nan\n",
      " 0.2737\n",
      " 0.4364\n",
      " 0.5778\n",
      " 0.7305\n",
      " 0.8929\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "V \n",
      "    nan\n",
      " 0.2547\n",
      " 0.4294\n",
      " 0.5700\n",
      " 0.7126\n",
      " 0.8667\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Env(object):\n",
    "    def __init__(self):\n",
    "        self.env_size = 7\n",
    "\n",
    "    @property\n",
    "    def num_actions(self):\n",
    "        return 2\n",
    "\n",
    "    @property\n",
    "    def state_shape(self):\n",
    "        return [self.env_size]\n",
    "    \n",
    "    def reset(self):\n",
    "        self.s = torch.IntTensor(1)\n",
    "        self.s[0] = np.random.randint(5) + 1\n",
    "        return self.s\n",
    "        \n",
    "    def act(self, a):\n",
    "        if a == 0:\n",
    "            self.s[0] -= 1\n",
    "        else:\n",
    "            self.s[0] += 1\n",
    "        reward = 1 if self.s[0] == self.env_size - 1 else 0\n",
    "        done = self.s[0] in [0, self.env_size - 1]\n",
    "        return self.s, reward, done\n",
    "    \n",
    "    def render(self):\n",
    "        res = ['-'] * self.env_size\n",
    "        res[0] = '*'\n",
    "        res[6] = '*'\n",
    "        res[self.s[0]] = 'X'\n",
    "        print(''.join(res))\n",
    "\n",
    "class RandomPolicy(object):\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def get_action(self, s):\n",
    "        return np.random.randint(self.num_actions)\n",
    "\n",
    "def run_episode(env, policy, render=True):\n",
    "    actions = []\n",
    "    rewards = []\n",
    "    states = []\n",
    "    s = env.reset()\n",
    "    while True:\n",
    "        a = policy.get_action(s)\n",
    "        if render:\n",
    "            env.render()\n",
    "        states.append(s.clone())\n",
    "        s, r, done = env.act(a)\n",
    "        rewards.append(r)\n",
    "        actions.append(a)\n",
    "        if done:\n",
    "            break\n",
    "    if render:\n",
    "        env.render()\n",
    "    return states, actions, rewards\n",
    "\n",
    "env = Env()\n",
    "policy = RandomPolicy(env.num_actions)\n",
    "print(env.state_shape)\n",
    "\n",
    "num_episodes = 5\n",
    "num_episodes = 0\n",
    "for episode in range(num_episodes):\n",
    "    print('')\n",
    "    print('episode %s' % episode)\n",
    "    states, actions, rewards = run_episode(env, policy)\n",
    "\n",
    "def get_linear_index(target, index):\n",
    "    assert len(index.shape) == 1\n",
    "    linear_index = 0\n",
    "    D = index.shape[0]\n",
    "    for d in range(D):\n",
    "        linear_index = linear_index * target.shape[d] + index[d]\n",
    "    return linear_index\n",
    "\n",
    "def tensor_set(target, index, v):\n",
    "    linear_index = get_linear_index(target, index)\n",
    "    target.view(-1)[linear_index] = v\n",
    "\n",
    "def tensor_get(target, index):\n",
    "    linear_index = get_linear_index(target, index)\n",
    "    target = target.view(-1)[linear_index]\n",
    "    return target\n",
    "\n",
    "def tensor_inc(target, index, v):\n",
    "    tensor_set(target, index, tensor_get(target, index) + v)\n",
    "\n",
    "\n",
    "class MonteCarloAllVisits(object):\n",
    "    def __init__(self, env, policy):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.state_shape = env.state_shape\n",
    "        self.N = torch.zeros(self.state_shape)\n",
    "        self.S = torch.zeros(self.state_shape)\n",
    "\n",
    "    def step(self):\n",
    "        states, actions, rewards = run_episode(self.env, self.policy, render=False)\n",
    "        G = 0\n",
    "        for i in range(len(actions) - 1, -1, -1):\n",
    "            s = states[i]\n",
    "            a = actions[i]\n",
    "            r = rewards[i]\n",
    "            G += r\n",
    "            tensor_inc(self.N, s, 1)\n",
    "            tensor_inc(self.S, s, G)\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self.S / self.N\n",
    "\n",
    "runner = MonteCarloAllVisits(env, policy)\n",
    "for it in range(100):\n",
    "    runner.step()\n",
    "    if it % 20 == 0:\n",
    "        print('it', it)\n",
    "        print('V', runner.V)\n",
    "print('V', runner.V)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0\n",
      "V \n",
      "nan\n",
      "nan\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "  1\n",
      "nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 20\n",
      "V \n",
      "    nan\n",
      " 0.0769\n",
      " 0.3333\n",
      " 0.5000\n",
      " 0.5714\n",
      " 0.9000\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 40\n",
      "V \n",
      "    nan\n",
      " 0.0435\n",
      " 0.2917\n",
      " 0.4545\n",
      " 0.6000\n",
      " 0.8261\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 60\n",
      "V \n",
      "    nan\n",
      " 0.1389\n",
      " 0.3514\n",
      " 0.4706\n",
      " 0.6176\n",
      " 0.8108\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "it 80\n",
      "V \n",
      "    nan\n",
      " 0.1064\n",
      " 0.3191\n",
      " 0.4545\n",
      " 0.6341\n",
      " 0.8478\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n",
      "V \n",
      "    nan\n",
      " 0.1053\n",
      " 0.3051\n",
      " 0.4643\n",
      " 0.6471\n",
      " 0.8750\n",
      "    nan\n",
      "[torch.FloatTensor of size 7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class MonteCarloFirstVisit(object):\n",
    "    def __init__(self, env, policy):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.state_shape = env.state_shape\n",
    "        self.N = torch.zeros(self.state_shape)\n",
    "        self.S = torch.zeros(self.state_shape)\n",
    "#         print('self.N', self.N)\n",
    "\n",
    "    def step(self):\n",
    "        states, actions, rewards = run_episode(self.env, self.policy, render=False)\n",
    "        G = 0\n",
    "#         seen_states = set()\n",
    "        seen_states = torch.zeros(self.state_shape)\n",
    "        seen_states.zero_()\n",
    "        for i in range(len(actions) - 1, -1, -1):\n",
    "            s = states[i]\n",
    "            a = actions[i]\n",
    "            r = rewards[i]\n",
    "            G += r\n",
    "#             print('seen_states', seen_states)\n",
    "#             print('s', s)\n",
    "            if tensor_get(seen_states, s) == 0:\n",
    "                tensor_inc(self.N, s, 1)\n",
    "                tensor_inc(self.S, s, G)\n",
    "                tensor_set(seen_states, s, 1)\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self.S / self.N\n",
    "\n",
    "runner = MonteCarloFirstVisit(env, policy)\n",
    "for it in range(100):\n",
    "    runner.step()\n",
    "    if it % 20 == 0:\n",
    "        print('it', it)\n",
    "        print('V', runner.V)\n",
    "print('V', runner.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "it 0\n",
      "V \n",
      " 0.0000  0.0001  0.0016  0.0064  0.0307  0.1030  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 10\n",
      "V \n",
      " 0.0000  0.0040  0.0265  0.1264  0.2425  0.4410  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 20\n",
      "V \n",
      " 0.0000  0.0123  0.5195  0.9359  1.4297  1.5510  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 30\n",
      "V \n",
      " 0.0000  0.7024  3.0198  6.5217  8.6699  5.8119  0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 40\n",
      "V \n",
      "  0.0000   1.9465  11.4581  19.2049  18.1844  10.5330   0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 50\n",
      "V \n",
      "  0.0000  64.5294  98.2167  90.3260  65.9835  25.4893   0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 60\n",
      "V \n",
      "   0.0000  165.3389  331.6011  351.0712  169.0275   65.9930    0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 70\n",
      "V \n",
      "    0.0000   893.8348  3545.5962  3703.3303  1736.8694   314.3897     0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 80\n",
      "V \n",
      "\n",
      "Columns 0 to 5 \n",
      "     0.0000   5013.9551  14255.0811  18314.0117  10492.3408   3617.6785\n",
      "\n",
      "Columns 6 to 6 \n",
      "     0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "it 90\n",
      "V \n",
      "\n",
      "Columns 0 to 5 \n",
      "     0.0000  12881.3281  27154.0469  33118.4922  26502.4453  10813.1836\n",
      "\n",
      "Columns 6 to 6 \n",
      "     0.0000\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n",
      "V \n",
      "\n",
      "Columns 0 to 5 \n",
      " 0.0000e+00  4.0865e+04  1.5363e+05  2.0759e+05  1.5752e+05  8.3279e+04\n",
      "\n",
      "Columns 6 to 6 \n",
      " 0.0000e+00\n",
      "[torch.FloatTensor of size 1x7]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class TD(object):\n",
    "    def __init__(self, env, policy, alpha=0.1):\n",
    "        self.env = env\n",
    "        self.policy = policy\n",
    "        self.state_shape = env.state_shape\n",
    "        self._V = torch.zeros(self.state_shape)\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def step(self):\n",
    "        states, actions, rewards = run_episode(self.env, self.policy, render=False)\n",
    "        is_terminal = True\n",
    "        s_next = None\n",
    "        for t in range(len(actions) - 1, -1, -1):\n",
    "#             print('t', t)\n",
    "            s = states[t]\n",
    "            a = actions[t]\n",
    "            r = rewards[t]\n",
    "            v_old = tensor_get(self._V, s)\n",
    "            v_next = 0\n",
    "            if s_next is not None:\n",
    "                v_next = tensor_get(self.V, s_next)\n",
    "            v_new = v_old + self.alpha * (r + v_next - v_old)\n",
    "#             if s_next is not None:\n",
    "#                 print('s', s[0], 'a', a, 'r', r, 's_next', s_next[0], 'v_new', v_new)\n",
    "#             else:\n",
    "#                 print('s', s[0], 'a', a, 'r', r, 'v_new', v_new)\n",
    "            tensor_set(self._V, s, v_new)\n",
    "            is_terminal = False\n",
    "            s_next = s\n",
    "\n",
    "    @property\n",
    "    def V(self):\n",
    "        return self._V\n",
    "\n",
    "runner = TD(env, policy)\n",
    "for it in range(100):\n",
    "#     if random.randint(0, 2) == 0:\n",
    "    runner.step()\n",
    "    if it % 10 == 0:\n",
    "        print('it', it)\n",
    "        print('V', runner.V.view(1, -1))\n",
    "print('V', runner.V.view(1, -1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
