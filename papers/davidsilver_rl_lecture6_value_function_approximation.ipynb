{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Value Function Approximation\", David Silver lecture 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Value Function Approximation__\n",
    "\n",
    "- Estimate value function with _function approximation_\n",
    "\n",
    "$$\n",
    "\\hat{v}(s, \\mathbf{w}) \\approx v_\\pi(s) \\text{, or}\\\\\n",
    "\\hat{q}(s, a, \\mathbf{w}) \\approx q_\\pi(s, a)\n",
    "$$\n",
    "- _generalize_ from seen states to unseen states\n",
    "- _update_ parameter $\\mathbf{w}$ using MC or TD learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Types of Value Function Approximation__\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "s & \\mapsto \\hat{v}(s, \\mathbf{w}) \\\\\n",
    "s, a & \\mapsto \\hat{q}(s, a, \\mathbf{w}) \\\\\n",
    "s & \\mapsto \\hat{q}(s, a_1, \\mathbf{w}) \\dots \\hat{q}(s, a_m, \\mathbf{w})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Which function approximator?__\n",
    "\n",
    "There are many function approximators, eg:\n",
    "\n",
    "- linear combinations of features\n",
    "- neural network\n",
    "- decision tree\n",
    "- nearest neighbor\n",
    "- fourier / wavelet bases\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Which function approximator? 2__\n",
    "\n",
    "We consider __differentiable__ function approximators, ie:\n",
    "\n",
    "- linear combinations of features\n",
    "- neural network\n",
    "\n",
    "Furthermore, we require a training method that is suitable for __non-stationary, non-iid__ data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Incremental Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Gradient Descent__\n",
    "\n",
    "- Let $J(\\mathbf{w})$ be a differentiable function of parameter vector $\\mathbf{w}$\n",
    "- define the _gradient_ of $J(\\mathbf{w})$ to be:\n",
    "\n",
    "$$\n",
    "\\nabla_\\mathbf{w}J(\\mathbf{w}) = \\begin{pmatrix}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_1} \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}_n}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "- to find a local minimum of $J(\\mathbf{w})$, adjust $\\mathbf{w}$ in direction of -ve gradient:\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{w} = - \\frac{1}{2} \\alpha \\nabla_\\mathbf{w} J(\\mathbf{w})\n",
    "$$\n",
    "... where $\\alpha$ is a step-size parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Value function approximation by stochastic gradient descent__\n",
    "\n",
    "- goal: find parameter vector $\\mathbf{w}$ minimizing mean-squared error between approximate value function $\\hat{v}(s, \\mathbf{w})$ and true value function $v_\\pi(s)$:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\mathbb{E}_\\pi \\left[\n",
    "   (v_\\pi(S) - \\hat{V}(S, \\mathbf{w}))^2\n",
    "\\right]\n",
    "$$\n",
    "- gradient descent finds a local minimum:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\mathbf{w} & = - \\frac{1}{2} \\alpha \\nabla_\\mathbf{w} J(\\mathbf{w}) \\\\ & = \\alpha \\mathbb{E}_\\pi\\left[\n",
    "   (v_\\pi(S) - \\hat{v}(S,\\mathbf{w}))\\nabla_\\mathbf{w} \\hat{v}(S, \\mathbf{w})\n",
    "\\right]\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "- stochastic gradient descent _samples_ the gradient\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{w} = \\alpha(v_\\pi(S) - \\hat{v}(S,\\mathbf{w}))\\nabla_\\mathbf{w} \\hat{v}(S, \\mathbf{w})\n",
    "$$\n",
    "- expected update is equal to full gradient update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21:15 to here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Feature vectors__\n",
    "\n",
    "- represent state by a _feature vector_\n",
    "\n",
    "$$\n",
    "\\mathbf{x}(S) = \\begin{pmatrix}\n",
    "x_1(S) \\\\\n",
    "\\vdots \\\\\n",
    "x_n(S)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "- for example:\n",
    "  - distance of robot from landmarks\n",
    "  - trends in the stock market\n",
    "  - piece and pawn configurations in chess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear value function approximation__\n",
    "\n",
    "- represent value function by a linear combination of features\n",
    "\n",
    "$$\n",
    "\\hat{v}(S, \\mathbf{w}) = \\mathbf{x}(S)^T \\mathbf{w} = \\sum_{j=1}^n x_j(S)\\,\\mathbf{w}_j\n",
    "$$\n",
    "- objective function is quadratic in parameters $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\mathbb{E}_\\pi\\left[\n",
    "    (v_\\pi(S) - \\mathbf{x}(S)^T \\mathbf{w})^2\n",
    "\\right]\n",
    "$$\n",
    "- stochastic gradient descent converges on __global__ optimum\n",
    "- update rule is particularly simple:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_\\mathbf{w} \\hat{v}(S, \\mathbf{w}) & = \\mathbf{x}(S) \\\\\n",
    "\\Delta \\mathbf{w} & = \\alpha(v_\\pi(S) - \\hat{v}(S, \\mathbf{w}))\\, \\mathbf{x}(S)\n",
    "\\end{align}\n",
    "$$\n",
    "- \"Update = step-size x prediction error x feature value\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Table lookup features__\n",
    "\n",
    "- table lookup is a special case of linear value function approximation\n",
    "- using _table lookup features_\n",
    "\n",
    "$$\n",
    "\\mathbf{x}^\\text{table}(S) = \\begin{pmatrix}\n",
    "1(S=s_1) \\\\\n",
    "\\vdots \\\\\n",
    "1(S=s_n)\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "- parameter vector $\\mathbf{w}$ gives values of each individual state\n",
    "\n",
    "$$\n",
    "\\hat{v}(S, \\mathbf{w}) = \\begin{pmatrix}\n",
    "1(S=s_1) \\\\\n",
    "\\vdots \\\\\n",
    "1(S=s_n)\n",
    "\\end{pmatrix}\n",
    "\\cdot\n",
    "\\begin{pmatrix}\n",
    "w_1 \\\\\n",
    "\\vdots \\\\\n",
    "w_n\n",
    "\\end{pmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30:53 to here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Incremental prediction algorithms__\n",
    "\n",
    "- have assumed true value function $v_\\pi(s)$ given by supervisor/oracle\n",
    "- but in RL there is no supervisor, only rewrds\n",
    "- in practice, we substitute a _target_ for $v_\\pi(s)$:\n",
    "  - for M, the target is the return $G_t$\n",
    "  \n",
    "  $$\n",
    "  \\Delta \\mathbf{w} = \\alpha(G_t - \\hat{v}(S_t, \\mathbf{w})\\, \\nabla_\\mathbf{w} \\hat{v}(S_t, \\mathbf{w}))\n",
    "  $$\n",
    "  - for TD(0), the target is the TD target $R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})$:\n",
    "  \n",
    "  $$\n",
    "  \\Delta \\mathbf{w} = \\alpha(R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w})) \\, \\nabla_\\mathbf{w}(S_t, \\mathbf{w})\n",
    "  $$\n",
    "  - for TD($\\lambda$), the target is $\\lambda$-return $G_t^\\lambda$:\n",
    "  \n",
    "  $$\n",
    "  \\Delta \\mathbf{w} = \\alpha(G_t^\\lambda - \\hat{v}(S_t, \\mathbf{w}) \\, \\nabla_\\mathbf{w}(S_t, \\mathbf{w}))\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Monte-Carlo with value function approximation__\n",
    "\n",
    "- return $G_t$ is an unbiased, noisy sample of true value $v_\\pi(S_t)$\n",
    "- can therefore apply supervised learning to 'training data':\n",
    "\n",
    "$$\n",
    "\\langle S_1, G_1 \\rangle, \\langle S_2, G_2\\rangle,\n",
    "\\dots,\n",
    "\\langle S_T, G_T \\rangle\n",
    "$$\n",
    "- for example, using _linear Monte-Carlo policy evaluation_:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\mathbf{w} & = \\alpha(G_t - \\hat{v}(S_t, \\mathbf{w})) \\, \\nabla_\\mathbf{w} \\hat{v}(S_t, \\mathbf{w}) \\\\\n",
    "& = \\alpha(G_t - \\hat{v})(S_t, \\mathbf{w})) \\, \\mathbf{x}(S_t)\n",
    "\\end{align}\n",
    "$$\n",
    "- Monte-Carlo evaluation converges to a local optimum\n",
    "- even when using non-linear value function approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TD Learning with Value Function Approximation__\n",
    "\n",
    "- the TD-target $R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w})$ is a _biased_ sample of true value $v_\\pi(S_t)$\n",
    "- can still apply supervised learning to 'training data':\n",
    "\n",
    "$$\n",
    "\\langle S_1, R_2 + \\gamma \\hat{v}(S_2, \\mathbf{w}) \\rangle, \\langle S_2, R_3 + \\gamma \\hat{v}(S_3, \\mathbf{w}) \\rangle, \\dots, \\langle S_{T-1}, R_T \\rangle\n",
    "$$\n",
    "- for example, using _linear TD(0)_:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\mathbf{w} & = \\alpha(R + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}))\\, \\nabla_\\mathbf{w} \\hat{v}(S_t, \\mathbf{w}) \\\\\n",
    "& = \\alpha \\delta \\mathbf{x}(S)\n",
    "\\end{align}\n",
    "$$\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\delta = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}))\n",
    "$$\n",
    "- linear TD(0) converges (close) to global optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__TD($\\lambda$) with Value Function Approximation__\n",
    "\n",
    "- the $\\lambda$-return $G_t^\\lambda$ is also a biased sample of true value $v_\\pi(s)$\n",
    "- can again apply supervised learning to 'training data':\n",
    "\n",
    "$$\n",
    "\\langle S_1, G_1^\\lambda \\rangle, \\langle S_2, G_2^\\lambda \\rangle, \\dots, \\langle S_{T-1}, G_{T-1}^\\lambda \\rangle\n",
    "$$\n",
    "- forward view linear TD($\\lambda$)\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\mathbf{w} & = \\alpha(G_t^\\lambda - \\hat{v}(S_t, \\mathbf{w}) \\, \\nabla_\\mathbf{w} \\hat{v}(S_t, \\mathbf{w})) \\\\\n",
    "& = \\alpha(G_t^\\lambda - \\hat{v}(S_t, \\mathbf{w})) \\, \\mathbf{x}(S_t)\n",
    "\\end{align}\n",
    "$$\n",
    "- backward view linear TD($\\lambda$):\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{w} = \\alpha \\delta_t E_t\n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\delta_t & = R_{t+1} + \\gamma \\hat{v}(S_{t+1}, \\mathbf{w}) - \\hat{v}(S_t, \\mathbf{w}) \\\\\n",
    "E_t & = \\gamma \\lambda E_{t-1} + \\delta_t \\mathbf{x}(S_t)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Forward view and backward view TD($\\lambda$) are equivalent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "49:00 to here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
