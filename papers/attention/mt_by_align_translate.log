simple rnn seq-to-seq, no attention at all. using rnn; seq2seq_noattention.py
N = 4
hidden_size = 64

<start>this<end>this<end>this<end>this
epoch 1800
<start>far foo thst<end>
<start>far foo this<end>this<end>t
<start>this<end>this<end>this<end>this
epoch 1850
<start>far foo thst<end>
<start>far foo this<end>this<end>t
<start>this<end>this<end>this<end>this
^CTraceback (most recent call last):

trying anki, with 4 pairs :-P
even with 128 hidden neurons, it's kind of junky
- looks like it doesnt really learn to pay attnetion to internal state,
  just goes off last few letters
- maybe turn off teacher forcing, even during training?
  - or maybe attenuate the loss with distance from the enc-dec boundary?
     - (or both?)


junky seqt oseq iwthout attention
- maybe because encoder doestn learn language model?
- added teacher forcing to encoder
- also dropped N down to 8, and increased hidden size to 256
=> encoder on its own (no decoder) learned ok ish:
```
epoch 95 encoder:
    [I'm glad w<end>] => [H'm glad w<end>]
    [When did y<end>] => [Hhen did y<end>]
    [Somebody a<end>] => [Hhmebody a<end>]
    [He was lea<end>] => [He was lea<end>]
    [She always<end>] => [Hhe always<end>]
```

add back in decoder, fairly junky:
```
epoch 730 encoder:
    [I'm glad w<end>] => [H'm wlad w<end>]
    [When did y<end>] => [Hhe  w d w<end>]
    [Somebody a<end>] => [Hhme ody w<end>]
    [He was lea<end>] => [He wa<end> wea<end>]
    [She always<end>] => [Hhe w<end>way<end><end>]

epoch 730 decoder:
    [I'm glad w] => [Je suis ra] [Qe s<end><end><end><end><end><end><end>]
    [When did y] => [Quand êtes] [Qe s s s s ]

epoch 735 encoder:
    [I'm glad w<end>] => [H'm wlad w<end>]
    [When did y<end>] => [Hhe  w d w<end>]
    [Somebody a<end>] => [Hhme ody w<end>]
    [He was lea<end>] => [He wa<end> wea<end>]
    [She always<end>] => [Hhe w<end>way<end><end>]

epoch 735 decoder:
    [I'm glad w] => [Je suis ra] [Qe s<end>e<end> se ]
    [When did y] => [Quand êtes] [Qe s<end><end><end><end><end><end><end>]
    ```

Since this has caused encoder things to get owrse, lets bump up hidden size to 512


after fixing some bugs, like remembering to train the decoder ;-). still junky
```

epoch 756 encoder:
    [I'm glad w] => [S'm alad a]
    [When did y] => [She  a d w]
    [Somebody a] => [Shm  od  a]
    [He was lea] => [Se aa<end> ae ]
    [She always] => [She alwa<end> ]

epoch 756 decoder:
    [I'm glad w] => [Je suis ra] [Qu <end><end><end><end><end><end><end><end>]
    [When did y] => [Quand êtes] [Qu <end><end><end><end><end><end><end><end>]
```

try hidden size 1024...
```
epoch 158 encoder:
    [I'm glad w] => [S'm glad w]
    [When did y] => [Shen did y]
    [Somebody a] => [Shmebody a]
    [He was lea] => [Se was lea]
    [She always] => [She always]

epoch 158 decoder:
    [I'm glad w] => [Je suis ra] [Je <end><end><end><end><end><end><end><end>]
    [When did y] => [Quand êtes] [Qu <end><end><end><end><end><end><end><end>]
```

after a while, encoder becomes pretty junky:
```
epoch 228 encoder:
    [I'm glad w] => [S'm alad a]
    [When did y] => [She  aid a]
    [Somebody a] => [Shme od  a]
    [He was lea] => [Se aas aea]
    [She always] => [She always]

epoch 228 decoder:
    [I'm glad w] => [Je suis ra] [Qu <end><end><end><end><end><end><end><end>]
    [When did y] => [Quand êtes] [Qu <end><end><end><end><end><end><end><end>]
```
Of course, decoder cant do much with sentences that all end in space-a...

Need gradient clipping?


with gradient clipping to 4.0, and dropping hidden down to 128:
```
epoch 1186 encoder:
    [I'm glad w] => [H'm glad w]
    [When did y] => [Hhen d d <end>]
    [Somebody a] => [Homewod  a]
    [He was lea] => [He was lea]
    [She always] => [Hoe always]

epoch 1186 decoder:
    [I'm glad w] => [Je suis ra] [Qe sui<end> <end><end><end>]
    [When did y] => [Quand êtes] [Qu <end><end><end><end><end><end><end><end>]

epoch 1188 encoder:
    [I'm glad w] => [H'm glad w]
    [When did y] => [Hhen d d y]
    [Somebody a] => [Homewod  a]
    [He was lea] => [He was lea]
    [She always] => [Hoe always]

epoch 1188 decoder:
    [I'm glad w] => [Je suis ra] [Qe sui<end> <end><end><end>]
    [When did y] => [Quand êtes] [Qu <end><end><end><end><end><end><end><end>]
```

put back the '<end>' token in encoder:
```
epoch 634 encoder:
    [I'm glad w<end>] => [H'm glad w<end>]
    [When did y<end>] => [Hhe  did <end><end>]
    [Somebody a<end>] => [Home ody a<end>]
    [He was lea<end>] => [He was lea<end>]
    [She always<end>] => [Hoe always<end>]

epoch 634 decoder:
    [I'm glad w] => [Je suis ra] [Je su <end> <end><end><end>]
    [When did y] => [Quand êtes] [Quel<end><end><end><end><end><end><end>]
```
working more or less ok :-)


with N = 16, kind of similar:
```
epoch 340 encoder:
    [I'm glad w<end>] => [I m <end>la<end> <end><end>]
    [When did y<end>] => [Iha  <end><end><end> <end><end>]
    [Somebody a<end>] => [Iome o<end><end> a<end>]
    [He was lea<end>] => [Ie wa<end> <end>ea<end>]
    [She always<end>] => [Ioe a<end>aa<end><end><end>]

epoch 340 decoder:
    [I'm glad w] => [Je suis ra] [Je <end><end><end><end><end><end><end><end>]
    [When did y] => [Quand êtes] [Que<end> <end><end><end><end><end><end>]
```

abandon on first error:
```
epoch 92 encoder:
    [I'm glad w<end>] => [I m glad <end><end>]
    [When did y<end>] => [Ihe  did y<end>]
    [Somebody a<end>] => [Iome od  a<end>]
    [He was lea<end>] => [Ie was lea<end>]
    [She always<end>] => [Ioe alaa<end>s<end>]

epoch 92 decoder:
    [I'm glad w] => [Je suis ra] [Je suis <end>]
    [When did y] => [Quand êtes] [Que]
```
kind of ok

seq2seq_noattention_trainbyparts.py v3 (ie with batching across n and t):
```
epoch 304 encoder:
    [<start>I'm glad w<end>] => [I m glad w<end>e]
    [<start>When did y<end>] => [Ihen did y<end>p]
    [<start>Somebody a<end>] => [Ihmebody a<end> ]
    [<start>He was lea<end>] => [Ie was lea<end><end>]

epoch 304 decoder:
    [I'm glad w] => [Je suis ra] [Je suis <end><end><end>]
    [When did y] => [Quand êtes] [Quel est  <end>]
    [Somebody a] => [Quelqu'un ] [Quel est  <end>]
    [He was lea] => [Il s'appuy] [Il s'a re <end>]
```
with: N=16, hidden_size=256, max_sentence_len=10


with commit 3d024b4 of seqtoseq_attention.py, working ok :-)
- using N=4
- hidden_size=256
- max_sentence_len = 4

try N = 16 => harder, eg:
```
epoch 1008 encoder:
    forward [<start>I'm glad w<end>] => [I m glad w<end>]
    forward [<start>When did y<end>] => [Ihe  did y<end>]
    forward [<start>Somebody a<end>] => [Ihme ody a<end>]
    forward [<start>He was lea<end>] => [Ie was lea<end>]
    back [<start>I'm glad w<end>] => [<start>I'm glad  ]
    back [<start>When did y<end>] => [<start>Whea did  ]
    back [<start>Somebody a<end>] => [<start>Somebody  ]
    back [<start>He was lea<end>] => [<start>He w's l  ]

epoch 1008 decoder:
    [I'm glad w] => [Je suis ra] [Je suis   <end>]
    [When did y] => [Quand êtes] [Quesd êtest]
    [Somebody a] => [Quelqu'un ] [Quelqu'un <end>]
    [He was lea] => [Il s'appuy] [Il s'appuy<end>]
```

drop hidden_size to 128:
```

epoch 1440 encoder:
    forward [<start>I'm glad w<end>] => [S m glad w<end>]
    forward [<start>When did y<end>] => [Sha  did y<end>]
    forward [<start>Somebody a<end>] => [Shmebody a<end>]
    forward [<start>He was lea<end>] => [Se was lea<end>]
    back [<start>I'm glad w<end>] => [<start>I'm glad  ]
    back [<start>When did y<end>] => [<start>When did  ]
    back [<start>Somebody a<end>] => [<start>Somebo e  ]
    back [<start>He was lea<end>] => [<start>He w's l  ]

epoch 1440 decoder:
    [I'm glad w] => [Je suis ra] [Je suis ra<end>]
    [When did y] => [Quand êtes] [Quend êtes<end>]
    [Somebody a] => [Quelqu'un ] [Queluu'ui <end>]
    [He was lea] => [Il s'appuy] [Il s'appuy<end>]
```

drop hidden size to 64, struggles:
```

epoch 1328 encoder:
    forward [<start>I'm glad w<end>] => [I   w<end>a  w ]
    forward [<start>When did y<end>] => [Ihe       <end>]
    forward [<start>Somebody a<end>] => [Ihue o   a<end>]
    forward [<start>He was lea<end>] => [Ie wa   oa<end>]
    back [<start>I'm glad w<end>] => [<start>I'  glad  ]
    back [<start>When did y<end>] => [<start>Whe  did  ]
    back [<start>Somebody a<end>] => [<start>Somebod   ]
    back [<start>He was lea<end>] => [<start>He wa  l  ]

epoch 1328 decoder:
    [I'm glad w] => [Je suis ra] [Je         ]
    [When did y] => [Quand êtes] [Qu         ]
    [Somebody a] => [Quelqu'un ] [Que  uu  uu]
    [He was lea] => [Il s'appuy] [Il         ]
```

N=32, hidden_size=256, getting slow, results fairly junky after 1000 epochs:
```

epoch 1232 encoder:
    forward [<start>I'm glad w<end>] => [H s wlad w<end>]
    forward [<start>When did y<end>] => [Hhe  d s wo]
    forward [<start>Somebody a<end>] => [Hhme  u  <end><end>]
    forward [<start>He was lea<end>] => [He w<end><end> de <end>]
    back [<start>I'm glad w<end>] => [<start>Ioe  hed t]
    back [<start>When did y<end>] => [<start> hee ded t]
    back [<start>Somebody a<end>] => [<start>loheblee t]
    back [<start>He was lea<end>] => [<start>He h's   t]

epoch 1232 decoder:
    [I'm glad w] => [Je suis ra] [Je s  <end> <end><end><end>]
    [When did y] => [Quand êtes] [Ques  s es<end>]
    [Somebody a] => [Quelqu'un ] [Que  u'uu <end>]
    [He was lea] => [Il s'appuy] [Il s'ap<end>u <end>]
```
=> maybe need faster machine?

add logging of epoch time. weirdly, tis twice as slow on a c4.4xlarge than on the Mac:

Mac:
```
epoch 64 encoder:
    forward [<start>I'm glad w<end>] => [I s wl   w<end>]
    forward [<start>When did y<end>] => [Ihe  w s w ]
    forward [<start>Somebody a<end>] => [Ihu   u  w<end>]
    forward [<start>He was lea<end>] => [Ie w<end><end> tl  ]
    back [<start>I'm glad w<end>] => [<start>toe  hed  ]
    back [<start>When did y<end>] => [<start> hee hed  ]
    back [<start>Somebody a<end>] => [<start>  h dlee  ]
    back [<start>He was lea<end>] => [<start>He h'e    ]

epoch 64 decoder:
    [I'm glad w] => [Je suis ra] [Je         ]
    [When did y] => [Quand êtes] [Je         ]
    [Somebody a] => [Quelqu'un ] [Je         ]
    [He was lea] => [Il s'appuy] [Il         ]

time per epoch 0.13007281720638275
```

c4.4xlarge:
```
epoch 96 encoder:
    forward [<start>I'm glad w<end>] => [I s wla  w<end>]
    forward [<start>When did y<end>] => [Ihe  w s wo]
    forward [<start>Somebody a<end>] => [Ihm   u  w<end>]
    forward [<start>He was lea<end>] => [Ie w<end><end> tl  ]
    back [<start>I'm glad w<end>] => [<start>toe  hed  ]
    back [<start>When did y<end>] => [<start> hee hed  ]
    back [<start>Somebody a<end>] => [<start>l h dlee  ]
    back [<start>He was lea<end>] => [<start>He h'e    ]

epoch 96 decoder:
    [I'm glad w] => [Je suis ra] [Je         ]
    [When did y] => [Quand êtes] [Il         ]
    [Somebody a] => [Quelqu'un ] [Ile        ]
    [He was lea] => [Il s'appuy] [Il         ]

time per epoch 0.3063374161720276
```

on an aws g2.xlarge, with cuda enabled:
```
epoch 96 decoder:
    [I'm glad w] => [Je suis ra] [Je         ]
    [When did y] => [Quand êtes] [Il         ]
    [Somebody a] => [Quelqu'un ] [Ile        ]
    [He was lea] => [Il s'appuy] [Il         ]

time per epoch 0.061708614230155945
```
(twice as fast as mac laptop, but only twice as fast...)

on an aws p2.xlarge:
```
epoch 128 encoder:
    forward [<start>I'm glad w<end>] => [I s wla  w<end>]
    forward [<start>When did y<end>] => [Ihe  w s wo]
    forward [<start>Somebody a<end>] => [Ihm   u  <end><end>]
    forward [<start>He was lea<end>] => [Ie w<end><end> tl  ]
    back [<start>I'm glad w<end>] => [<start>toe  hed  ]
    back [<start>When did y<end>] => [<start> hee hed  ]
    back [<start>Somebody a<end>] => [<start>l h dlee  ]
    back [<start>He was lea<end>] => [<start>He h'e    ]

epoch 128 decoder:
    [I'm glad w] => [Je suis ra] [Je         ]
    [When did y] => [Quand êtes] [Jee    ee  ]
    [Somebody a] => [Quelqu'un ] [Jle   ee   ]
    [He was lea] => [Il s'appuy] [Il         ]

time per epoch 0.04161100089550018
```
50% fatser than g2 :-)

try increasing neurons, N=32, H=512:
```
epoch 1216 encoder:
    forward [<start>I'm glad w<end>] => [I s glad w<end>]
    forward [<start>When did y<end>] => [Ihe  did <end><end>]
    forward [<start>Somebody a<end>] => [Ihme  d  a<end>]
    forward [<start>He was lea<end>] => [Ie was lea<end>]
    back [<start>I'm glad w<end>] => [<start>I'm  lad  ]
    back [<start>When did y<end>] => [<start>Thee ded  ]
    back [<start>Somebody a<end>] => [<start>Somebode  ]
    back [<start>He was lea<end>] => [<start>He was    ]

epoch 1216 decoder:
    [I'm glad w] => [Je suis ra] [Je suis ra<end>]
    [When did y] => [Quand êtes] [Quan  êtes<end>]
    [Somebody a] => [Quelqu'un ] [Quelqueun <end>]
    [He was lea] => [Il s'appuy] [Il s a pu<end><end>]

time per epoch 0.
...
epoch 2064 encoder:
    forward [<start>I'm glad w<end>] => [W s glad w<end>]
    forward [<start>When did y<end>] => [Whe  did <end><end>]
    forward [<start>Somebody a<end>] => [Whme  d  a<end>]
    forward [<start>He was lea<end>] => [We was lea<end>]
    back [<start>I'm glad w<end>] => [<start>I'm  lad  ]
    back [<start>When did y<end>] => [<start>When ded  ]
    back [<start>Somebody a<end>] => [<start>Somebode  ]
    back [<start>He was lea<end>] => [<start>He was    ]

epoch 2064 decoder:
    [I'm glad w] => [Je suis ra] [Je suis ra<end>]
    [When did y] => [Quand êtes] [Quand êtes<end>]
    [Somebody a] => [Quelqu'un ] [Quelqueun <end>]
    [He was lea] => [Il s'appuy] [Il s appuy<end>]

time per epoch 0.05154143273830414
```
(running on p2)


try N=64,H=512, fairly junky:
```
epoch 1680 encoder:
    forward [<start>I'm glad w<end>] => [I   <end>oad <end>a]
    forward [<start>When did y<end>] => [Iha  <end>os <end><end>]
    forward [<start>Somebody a<end>] => [Ihm      <end><end>]
    forward [<start>He was lea<end>] => [Ie <end>an <end><end> <end>]
    back [<start>I'm glad w<end>] => [<start>Ioe  hee  ]
    back [<start>When did y<end>] => [<start>Whee dee  ]
    back [<start>Somebody a<end>] => [<start>To  deee  ]
    back [<start>He was lea<end>] => [<start>He hie    ]

epoch 1680 decoder:
    [I'm glad w] => [Je suis ra] [Je su s <end><end><end>]
    [When did y] => [Quand êtes] [Que e  te <end>]
    [Somebody a] => [Quelqu'un ] [Quel ueuu <end>]
    [He was lea] => [Il s'appuy] [Il se  pu<end><end>]

time per epoch 0.0611894428730011
```

incrase H to 1024, N=64,H=512, decoder output ok. weirdly, the encoder output is junky:
```
epoch 1008 encoder:
    forward [<start>I'm glad w<end>] => [I m goa<end> w<end>]
    forward [<start>When did y<end>] => [Iha  dos <end><end>]
    forward [<start>Somebody a<end>] => [Ihm  od  <end><end>]
    forward [<start>He was lea<end>] => [Ie wan <end>e<end><end>]
    back [<start>I'm glad w<end>] => [<start>Iom  hed  ]
    back [<start>When did y<end>] => [<start>Whee hed  ]
    back [<start>Somebody a<end>] => [<start> o  bode  ]
    back [<start>He was lea<end>] => [<start>He has    ]

epoch 1008 decoder:
    [I'm glad w] => [Je suis ra] [Je suis ra<end>]
    [When did y] => [Quand êtes] [Quand êtes<end>]
    [Somebody a] => [Quelqu'un ] [Quelqu'un <end>]
    [He was lea] => [Il s'appuy] [Il s'appuy<end>]

time per epoch 0.13847991824150085
```

try max sentence length 20, go back to N=16, H=256, junky:
```
epoch 2112 encoder:
    forward [<start>I'm glad we agree on<end>] => [W m ela  ee e re  en<end>]
    forward [<start>When did you become <end>] => [Whe  e n eou ee oue e]
    forward [<start>He was leaning again<end>] => [We ee  ee   n<end> a r n<end>]
    forward [<start>She always got up ea<end>] => [Whe e ee   eou e  e  ]
    back [<start>I'm glad we agree on<end>] => [<start>I'e glee ee a ree on]
    back [<start>When did you become <end>] => [<start>Whee dee  oe re oeen]
    back [<start>He was leaning again<end>] => [<start>ee  ae l enine a  on]
    back [<start>She always got up ea<end>] => [<start>Wee     ae  ee ue  n]

epoch 2112 decoder:
    [I'm glad we agree on] => [Je suis ravi que nou] [Je  ui  e            ]
    [When did you become ] => [Quand êtes-vous deve] [Quen  eeeeeeeeeeeeeee]
    [He was leaning again] => [Il s'appuyait contre] [Il n e  e            ]
    [She always got up ea] => [Elle se levait toujo] [Elle ee eeeeeeeeeeeee]

time per epoch 0.07332244515419006
```

try S=20,N=64,H=512, junktastic:
```
epoch 2176 encoder:
    forward [<start>I'm glad we agree on<end>] => [I m  oee te te e  te ]
    forward [<start>When did you become <end>] => [Iee  t e tou te e   t]
    forward [<start>He was leaning again<end>] => [Ie  e  te   n  te en ]
    forward [<start>She always got up ea<end>] => [Ihe  ee    toe te ee ]
    back [<start>I'm glad we agree on<end>] => [<start>Ioe   ee ee    ee ee]
    back [<start>When did you become <end>] => [<start> eee eee  oe     eee]
    back [<start>He was leaning again<end>] => [<start>ee  ee   e ene    ee]
    back [<start>She always got up ea<end>] => [<start> ee     ee   e ee  e]

epoch 2176 decoder:
    [I'm glad we agree on] => [Je suis ravi que nou] [Je  ue  eeeeeeeeeeeee]
    [When did you become ] => [Quand êtes-vous deve] [Qu eeeeeeeeeeeeeeeeee]
    [He was leaning again] => [Il s'appuyait contre] [Il s'appuyait eeeeeee]
    [She always got up ea] => [Elle se levait toujo] [Elle  e eeeeeeeeeeeee]

time per epoch 0.1375909149646759
```
