{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Reinforcement Learning\", CG Nicholls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each timestep $t$:\n",
    "- receive observation $x_t$, from the environment\n",
    "- choose action $a_t$\n",
    "- receive reward $r_{t+1}$\n",
    "- see observation $x_{t+1}$\n",
    "\n",
    "state is Markov, ie:\n",
    "$$\n",
    "P(x_{t+1} \\mid x_t) = P(x_{t+1} \\mid x_1, x_2, \\dots, x_t)\n",
    "$$\n",
    "\n",
    "Therefore we can model as a Markov decision process (MDP).\n",
    "\n",
    "Means the state captures all you need to know about the system\n",
    "- in contrast, if the state did not include the cart's velocity, then it wouldn't be Markov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dont want to choose deterministically, since we want to explore different ways of playing\n",
    "- therefore, choose our actions using a conditional probability distribution $P(a \\mid x)$\n",
    "\n",
    "Must satisfy $P(a_L \\mid x) + P(a_R \\mid x) = 1$ for all observations, ie agent must choose exactly one of $a_L$ or $a_R$.\n",
    "\n",
    "__Definitions:__\n",
    "\n",
    "- $\\pi(a \\mid x; \\theta) = P(a \\mid x; \\theta)$ is the _policy_\n",
    "- $\\tau$ is the _trajectory_: $(x_0, a_0, r_1, x_1, a_1, r_2, \\dots, x_{T-1}, a_{T-1}, r_T, x_T)$\n",
    "- $R_\\tau$ is the _total reward_ of the trajectory: $r_1 + r_2 + \\dots + r_T$\n",
    "- the _expected reward_ for a given policy $\\pi$ is $\\mathbb{E}_\\tau[R_\\tau \\mid \\pi]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy method\n",
    "\n",
    "Draw $\\theta$ from a Gaussian distribution, with mean $\\mathbf{\\mu} = \\{ \\mu_1, \\mu_2, \\dots, \\mu_K\\}$, and axis-aligned variance, $\\mathbf{\\sigma} = \\{\\sigma_1, \\sigma_2, \\dots, \\sigma_K\\}$.\n",
    "\n",
    "- draw $N$ samples of $\\theta$\n",
    "- sample reward for each sample\n",
    "- update Gaussian distribution:\n",
    "   - new $\\mu$ is the sample mean of the retained $\\theta$ samples\n",
    "   - new $\\sigma$ is the sample variance of the retained $\\theta$ samples"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
