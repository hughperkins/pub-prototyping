{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Challenges in Data-to-Document Generation\", Wiseman, Shieber, Rush, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "\n",
    "\"Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, proposse a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "\"Over the past several years, neural text generation systems have shown impressive performance on tasks such as machine translation and summarization. As neural systems begin to move toward generating longer outputs in response to longer and more complicated inputs, however, the generated texts begin to display reference errors, inter-sentence incoherence, and a lack of fidelity to the source material. The goal of this paper is to suggest a particular, long-form generation task in which these challenges may be fruitfully explored, to provide a publicly available dataset for this task, to suggest some automatic evaluation metrics, and finally to establish how current, neural text generation methods perform on this task.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Aside__ Ideas from me on what might be important in a metric:\n",
    "- grammatical/linguistic coherence => standard linguistic models, eg lstm, rnn?\n",
    "- contains the data that it's supposed to contain => the extractive bit?\n",
    "- concise (vs contains a bunch of trivial/useless bits/irrelevant bits/noise) => simply use minus the length as the score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"A classic problem in natural-language generation (NLG) (Kukich, 1983; McKeown, 1992; Reiter and Dale, 1997) involves taking structured data, such as a table, as input, and producing text that adequately and fluently describes this data as output. Unlike machine translation, which aims for a complete transduction of the sentence to be translated, this form of NLG is typically taken to require addressing (at least) two separate challenges: _what to say_, the selection of an appropriate subset of the input data to discuss, and _how to say it_, the surface realization of a generation (Reiter and Dale, 1997; Jurafsky and Martin, 2014). Traditionally, these two challenges have been modularized and handled separately by generation systems. However, neural generation systems, which are typically trained end-to-end as conditional language models (Mikolov et al, 2010; Sutskever et al, 2011, 2014), blur this distinction.\n",
    "\n",
    "\"In this context, we believe the problem of generating multi-sentence summaries of tables or database records to be a reasonable next-problem for neural techniques to tackle as they begin to consider more difficult NLG tasks. In particular, we would like this generation task to have the following two properties: (1) it is relatively easy to obtain fairly clean summaries and their corresponding databases for dataset construction, and (2) the summaries should be primarily focused on conveying the information in the database. This latter property ensures that the task is somewhat congenial to a standard encoder-decoder approach, and more importantly, that it is reasonable to _evaluate_ generations in terms of their fidelity to the database.\n",
    "\n",
    "\"One task that meets these criteria is that of generating summaries of sports games from associated box-score data, and there is indeed a long history of NLG work that generates sports games summaries (Robin, 1994; Tanaka-Ishii et al., 1998, Barzilay and Lapata, 2005). To this end, we make the following contributions:\n",
    "\n",
    "- We introduce a new large-scale corpus consisting of textual descriptions of basketball games paired with extensive statistical tables. This dataset is sufficiently large that fully data-driven approaches might be sufficient.\n",
    "- We introduce a series of extractive evalution models to automatically evaluate output generation performance, exploiting the fact that post-hoc information extraction is significantly easier than generation itself.\n",
    "- We apply a series of state-of-the-art neural methods, as well as a simple templated generation system, to our data-to-document generation task in order to establish baseliens and study their generations.\n",
    "\n",
    "\"Our experiments indicate that neural systems are quite good at producing fluent outputs and generally score well on standard word-match metrics, but perform quite poorly at content selection and at capturing long-term structure. While the use of copy-based models and additional reconstruction terms in the training can lead to improvements in BLEU and in our proposed extractive evalutions, current models are still quite far from producing human-level output, and are significantly worse than templated systems in terms of content selection and realization. Overall, we believe this problem of data-to-document generation highlights important remaining challenges in neural generation systems, and the use of extractive evaluation reveals significant issues hidden by standard automatic metrics.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data-to-Text Datasets\n",
    "\n",
    "\"We consider the problem of generating descriptive text from database records. Following the notation in Liang et al. (2009), let $\\mathbf{s} = \\{r_j\\}_{j=1}^J$ be a set of records, where for each $r \\in \\mathbf{s}$ we define $r.t \\in \\mathcal{T}$ to be the _type_ of $r$, and we assume each $r$ to be a binarized relation, where $r.e$ and $r.m$ are a record's entity and value, respectively. For example, a database recording statistics for a basketball game might have a record $r$ such that $r.t = \\mathtt{Points}$, $r.e = \\mathtt{Russell\\,Westbrook}$, and $r.m = 50$. In this case, $r.e$ gives the player in question, and $r.m$ gives the number of points the player scored. From these records, we are interested in generating descriptive text, $\\hat{y}_{1:T}=\\hat{y}_1,\\dots,\\hat{y}_T$ of $T$ words such that $\\hat{y}_{1:T}$ is an adequate and fluent summary of $\\mathbf{s}$. A dataset for training data-to-document systems typically consists of $(\\mathbf{s}, y_{1:T})$ pairs, where $y_{1:T}$ is a document consisting of a gold (ie, human generated) summary for database $\\mathbf{s}$.\n",
    "\n",
    "\"Several benchmark datasets have been used in recent years for the text generation task, the most popular of these being $\\mathtt{WeatherGov}$ (Liang et al, 2009) and $\\mathtt{Robocup}$ (Chen and Mooney, 2008). Recently, neural generation systems have shown strong results on these datasets, with the system of Mei et al. (2016) achieving BLEU scores in the 60s and 70s on $\\mathtt{WeatherGov}$, and BLEU scores of almost 30 even on the smaller $\\mathtt{Robocup}$ dataset. These results are quite promising, and suggest that neural models are a good fit for text generation. However, the statistics of these datasets, shown in Table 1, indicate that these datasets use relatively simple language and record structure. Furthermore, there is reason to believe that $\\mathtt{WeatherGov}$ is at least partially machine-generated (Reiter, 2017). More recently, Lebret et al. (2016) introduced the $\\mathtt{WikiBio}$ dataset, which is at least an order of magnitude larger in terms of number of tokens and record types. However, as shown in Table 1, this dataset too only contains short (single-sentence) generations, and relatively few records per generation. As such, we believe that early success on these datasets is not yet sufficient for testing the desired linguistic capabilities of text generation at a document-scale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
