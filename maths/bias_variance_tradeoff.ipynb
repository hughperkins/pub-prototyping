{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bias-variance tradeoff\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n",
    "- https://stats.stackexchange.com/questions/129478/when-is-the-bootstrap-estimate-of-bias-valid\n",
    "- https://johanndejong.wordpress.com/2016/12/18/bias-variance-decomposition/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so:\n",
    "- we sample some datapoints from a population\n",
    "- we fit parameters to these sampled datapoints\n",
    "- then we create predictions, using these parameters, for each input $x_n$\n",
    "- we're going to fix the input data points $\\mathcal{X} = \\{x_1, x_2, \\dots, x_N \\}$, otherwise how can we average across datasets?\n",
    "\n",
    "So, we have:\n",
    "\n",
    "- $N$ samples in each dataset\n",
    "- $M$ datasets\n",
    "\n",
    "Input data $\\mathcal{X}$ is common across all datasets:\n",
    "\n",
    "- $\\mathcal{X} = x_{1}, x_{2}, \\dots, x_{N}$\n",
    "\n",
    "This means that ground-truth $\\mathcal{Y}^*$ is also common across all datasets:\n",
    "\n",
    "- $\\mathcal{Y}^* = \\{ y^*_{1}, y^*_{2}, \\dots, y^*_{N} \\}$\n",
    "\n",
    "Then, the following are per-dataset:\n",
    "\n",
    "- targets $\\mathcal{Y}_m = \\{ y_{m,1}, y_{m,2}, \\dots, y_{m,N} \\}$\n",
    "- parameters $\\theta_m$ (fitted to above $\\mathcal{X}$ and $\\mathcal{Y}_m$\n",
    "- predictions $\\hat{\\mathcal{Y}}_m = \\{ \\hat{y}_{m,1}, \\hat{y}_{m,2}, \\dots, \\hat{y}_{m,N} \\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then conceptually:\n",
    "\n",
    "- bias and variance are first calculated for each datapoint $x_n$, and then averaged over all datapoints\n",
    "- for each datapoint, the bias is the difference between the expected prediction, and the ground truth, squared, ie:\n",
    "\n",
    "$$\n",
    "\\text{bias}_n = \\left(\n",
    "    y^*_n - \\frac{1}{M} \\sum_{m=1}^M \\hat{y}_{m,n}\n",
    "\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the bias is:\n",
    "\n",
    "$$\n",
    "\\text{bias} = \\frac{1}{N} \\sum_{n=1}^N\n",
    "    \\left(\n",
    "        y_n^* - \\frac{1}{M} \\sum_{m=1}^M \\hat{y}_{m,n}\n",
    "    \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, variance is:\n",
    "\n",
    "$$\n",
    "\\text{variance} = \\frac{1}{N} \\sum_{n=1}^N \\frac{1}{M} \\sum_{m=1}^M\n",
    "    \\left(\n",
    "        \\hat{y}_{m,n} - \\frac{1}{M} \\sum_{m=1}^M \\hat{y}_{m,n}\n",
    "    \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, mse is averaged over each dataset. For each dataset, we calculate the mse over all datapoints, ie:\n",
    "\n",
    "$$\n",
    "\\text{mse} = \\frac{1}{M} \\sum_{m=1}^M\n",
    "   \\frac{1}{N} \\sum_{n=1}^N \\left(\n",
    "       y^*_n - \\hat{y}_{m,n}\n",
    "   \\right)^2 \\\\\n",
    "= \\frac{1}{N} \\sum_{n=1}^N \n",
    "\\frac{1}{M}\n",
    "\\sum_{m=1}^M\n",
    "\\left(\n",
    "       y^*_n - \\hat{y}_{m,n}\n",
    "   \\right)^2\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# N = 2000\n",
    "N = 10\n",
    "# K = 1\n",
    "num_runs = 1000\n",
    "true_a = 1.23\n",
    "true_b = 0.34\n",
    "# true_epsilon = 0.23\n",
    "true_epsilon = 0.51\n",
    "# true_epsilon = 0.0\n",
    "\n",
    "print('ground truth a=%s b=%s e=%s' % (true_a, true_b, true_epsilon))\n",
    "\n",
    "def generate_X():\n",
    "#     torch.manual_seed(seed)\n",
    "    torch.manual_seed(123)\n",
    "    X = torch.rand(N, 1)\n",
    "#     Y = X.view(-1) * true_a + true_b + torch.randn(N) * true_epsilon\n",
    "#     return X, Y\n",
    "    return X\n",
    "\n",
    "def generate_Y(X, seed):\n",
    "    torch.manual_seed(seed)\n",
    "#     X = torch.rand(N, 1)\n",
    "    Y = X.view(-1) * true_a + true_b + torch.randn(N) * true_epsilon\n",
    "    return Y\n",
    "\n",
    "def generate_features(X, order):\n",
    "    X2 = torch.zeros(N, order + 1)\n",
    "    for k in range(order + 1):\n",
    "        X2[:, k] = X[:, 0].pow(k)\n",
    "    return X2\n",
    "\n",
    "def fit(X, Y):\n",
    "    W = (X.transpose(0, 1) @ X)\n",
    "    W = W.inverse()\n",
    "    W = W @ X.transpose(0, 1)\n",
    "    W = W @ Y.view(-1, 1)\n",
    "    return W\n",
    "\n",
    "def calc_stats(Y_star, Y, preds):\n",
    "    bias_sum = 0\n",
    "    variance_sum = 0\n",
    "    mse_sum = 0\n",
    "    \n",
    "    N = Y.size()[0]\n",
    "    print('N', N)\n",
    "    num_samples = len(preds)\n",
    "    for n in range(N):\n",
    "#         yv = torch.zero(num_samples)\n",
    "        predv = torch.zeros(num_samples)\n",
    "        for j in range(num_samples):\n",
    "#             print('preds[j][n]', preds[j][n])\n",
    "            predv[j] = preds[j][n]\n",
    "            mse_sqrt = preds[j][n] - Y[n]\n",
    "            mse = mse_sqrt * mse_sqrt\n",
    "            mse_sum += mse\n",
    "        pred_avg = predv.mean()\n",
    "        bias_sqrt = pred_avg - Y_star[n]\n",
    "        bias = bias_sqrt * bias_sqrt\n",
    "        bias_sum += bias\n",
    "        variance = predv.var()\n",
    "#         help(torch.var)\n",
    "#         asdf\n",
    "        variance_sum += variance\n",
    "    return bias_sum / N, variance_sum / N, mse_sum / N / num_samples\n",
    "    \n",
    "#     err = pred - Y.view(-1, 1)\n",
    "#     mse = (err * err).sum() / N\n",
    "#     bias = (err * err).sum() / N\n",
    "#     print('mse', mse)\n",
    "#     print('bias', bias)\n",
    "#     variance = \n",
    "#     asdfasdf\n",
    "#     bias = err.sum() * err.sum() / N / N\n",
    "#     variance = (pred * pred).sum() / N - pred.sum() * pred.sum() / N / N\n",
    "#     noise = mse - bias - variance\n",
    "\n",
    "def run(order):\n",
    "    print('')\n",
    "    print('order %s' % order)\n",
    "    preds = []\n",
    "    X = generate_X()\n",
    "    Y_star = (X * true_a + true_b).view(-1)\n",
    "    X2 = generate_features(X=X, order=order)\n",
    "    Ws = torch.zeros(num_runs, order + 1)\n",
    "    for i in range(num_runs):\n",
    "        Y = generate_Y(X=X, seed=i)\n",
    "        W = fit(X2, Y)\n",
    "        Ws[i] = W\n",
    "        pred = X2 @ W\n",
    "        pred = pred.view(-1)\n",
    "        preds.append(pred)\n",
    "    bias, variance, mse = calc_stats(Y_star=Y_star, Y=Y, preds=preds)\n",
    "    noise = mse - bias - variance\n",
    "    W_avg = Ws.mean(0)\n",
    "    print('W_avg', W_avg.view(1, -1))\n",
    "    print('bias %.3f' % bias, 'variance %.4f' % variance,\n",
    "          'noise %.4f' % noise, 'mse %.4f' % mse)\n",
    "#         calc_stats(Y=Y, pred=pred)\n",
    "#         run_one(order=order, seed=i)\n",
    "\n",
    "run(order=0)\n",
    "run(order=1)\n",
    "run(order=2)\n",
    "run(order=3)\n",
    "# run(order=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation\n",
    "\n",
    "\"The derivation of the bias-variance decomposition for squared error proceeds as follows. For notational convenience, abbreviate $f = f(x)$ and $\\hat{f} = \\hat{f}(x)$. First recall that "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
