{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bias-variance tradeoff\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Bias%E2%80%93variance_tradeoff\n",
    "- https://stats.stackexchange.com/questions/129478/when-is-the-bootstrap-estimate-of-bias-valid\n",
    "- https://johanndejong.wordpress.com/2016/12/18/bias-variance-decomposition/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so:\n",
    "- we sample some datapoints from a population\n",
    "- we fit parameters to these sampled datapoints\n",
    "- then we create predictions, using these parameters, for each input $x_n$\n",
    "- we're going to fix the input data points $\\mathcal{X} = \\{x_1, x_2, \\dots, x_N \\}$, otherwise how can we average across datasets?\n",
    "\n",
    "So, we have:\n",
    "\n",
    "- $N$ samples in each dataset\n",
    "- $M$ datasets\n",
    "\n",
    "Input data $\\mathcal{X}$ is common across all datasets:\n",
    "\n",
    "- $\\mathcal{X} = x_{1}, x_{2}, \\dots, x_{N}$\n",
    "\n",
    "This means that ground-truth $\\mathcal{Y}^*$ is also common across all datasets:\n",
    "\n",
    "- $\\mathcal{Y}^* = \\{ y^*_{1}, y^*_{2}, \\dots, y^*_{N} \\}$\n",
    "\n",
    "Then, the following are per-dataset:\n",
    "\n",
    "- targets $\\mathcal{Y}_m = \\{ y_{m,1}, y_{m,2}, \\dots, y_{m,N} \\}$\n",
    "- parameters $\\theta_m$ (fitted to above $\\mathcal{X}$ and $\\mathcal{Y}_m$\n",
    "- predictions $\\hat{\\mathcal{Y}}_m = \\{ \\hat{y}_{m,1}, \\hat{y}_{m,2}, \\dots, \\hat{y}_{m,N} \\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then conceptually:\n",
    "\n",
    "- bias and variance are first calculated for each datapoint $x_n$, and then averaged over all datapoints\n",
    "- for each datapoint, the bias is the difference between the expected prediction, and the ground truth, squared, ie:\n",
    "\n",
    "$$\n",
    "\\text{bias}_n = \\left(\n",
    "    y^*_n - \\frac{1}{M} \\sum_{m=1}^M \\hat{y}_{m,n}\n",
    "\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the bias is:\n",
    "\n",
    "$$\n",
    "\\text{bias} = \\frac{1}{N} \\sum_{n=1}^N\n",
    "    \\left(\n",
    "        y_n^* - \\frac{1}{M} \\sum_{m=1}^M \\hat{y}_{m,n}\n",
    "    \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, variance is:\n",
    "\n",
    "$$\n",
    "\\text{variance} = \\frac{1}{N} \\sum_{n=1}^N \\frac{1}{M} \\sum_{m=1}^M\n",
    "    \\left(\n",
    "        \\hat{y}_{m,n} - \\frac{1}{M} \\sum_{m=1}^M \\hat{y}_{m,n}\n",
    "    \\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the noise, we have:\n",
    "\n",
    "$$\n",
    "\\text{noise}_n = \\frac{1}{M} \\sum_{m=1}^M \\left(\n",
    "    y_{m,n} - \\frac{1}{M} \\sum_{m=1}^M y_{m,n}\n",
    "\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And:\n",
    "\n",
    "$$\n",
    "\\text{noise} = \\frac{1}{N} \\sum_{n=1}^N \\frac{1}{M} \\sum_{m=1}^M \\left(\n",
    "   y_{m,n} - \\frac{1}{M} \\sum_{m=1}^M y_{m,n}\n",
    "\\right)^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, mse is averaged over each dataset. For each dataset, we calculate the mse over all datapoints, ie:\n",
    "\n",
    "$$\n",
    "\\text{mse} = \\frac{1}{M} \\sum_{m=1}^M\n",
    "   \\frac{1}{N} \\sum_{n=1}^N \\left(\n",
    "       y^*_n - \\hat{y}_{m,n}\n",
    "   \\right)^2 \\\\\n",
    "= \\frac{1}{N} \\sum_{n=1}^N \n",
    "\\frac{1}{M}\n",
    "\\sum_{m=1}^M\n",
    "\\left(\n",
    "       y^*_n - \\hat{y}_{m,n}\n",
    "   \\right)^2\n",
    "   $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ground truth a=1.23 b=0.34 e=0.51\n",
      "\n",
      "order 0\n",
      "N 10\n",
      "W_avg \n",
      " 1.0177\n",
      "[torch.FloatTensor of size 1x1]\n",
      "\n",
      "bias 0.048 variance 0.0273 noise 0.2596 noise2 0.4214 mse 0.4967\n",
      "\n",
      "order 1\n",
      "N 10\n",
      "W_avg \n",
      " 0.3303  1.2448\n",
      "[torch.FloatTensor of size 1x2]\n",
      "\n",
      "bias 0.000 variance 0.0538 noise 0.2596 noise2 0.3206 mse 0.3744\n",
      "\n",
      "order 2\n",
      "N 10\n",
      "W_avg \n",
      " 0.3944  0.9423  0.3054\n",
      "[torch.FloatTensor of size 1x3]\n",
      "\n",
      "bias 0.000 variance 0.0791 noise 0.2596 noise2 0.3205 mse 0.3996\n",
      "\n",
      "order 3\n",
      "N 10\n",
      "W_avg \n",
      " 0.6098 -0.6744  3.9526 -2.5258\n",
      "[torch.FloatTensor of size 1x4]\n",
      "\n",
      "bias 0.000 variance 0.1042 noise 0.2596 noise2 0.3170 mse 0.4213\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# N = 2000\n",
    "N = 10\n",
    "# K = 1\n",
    "num_runs = 1000\n",
    "true_a = 1.23\n",
    "true_b = 0.34\n",
    "# true_epsilon = 0.23\n",
    "true_epsilon = 0.51\n",
    "# true_epsilon = 0.0\n",
    "\n",
    "print('ground truth a=%s b=%s e=%s' % (true_a, true_b, true_epsilon))\n",
    "\n",
    "def generate_X():\n",
    "#     torch.manual_seed(seed)\n",
    "    torch.manual_seed(123)\n",
    "    X = torch.rand(N, 1)\n",
    "#     Y = X.view(-1) * true_a + true_b + torch.randn(N) * true_epsilon\n",
    "#     return X, Y\n",
    "    return X\n",
    "\n",
    "def generate_Y(X, seed):\n",
    "    torch.manual_seed(seed)\n",
    "#     X = torch.rand(N, 1)\n",
    "    Y = X.view(-1) * true_a + true_b + torch.randn(N) * true_epsilon\n",
    "    return Y\n",
    "\n",
    "def generate_features(X, order):\n",
    "    X2 = torch.zeros(N, order + 1)\n",
    "    for k in range(order + 1):\n",
    "        X2[:, k] = X[:, 0].pow(k)\n",
    "    return X2\n",
    "\n",
    "def fit(X, Y):\n",
    "    W = (X.transpose(0, 1) @ X)\n",
    "    W = W.inverse()\n",
    "    W = W @ X.transpose(0, 1)\n",
    "    W = W @ Y.view(-1, 1)\n",
    "    return W\n",
    "\n",
    "def calc_stats(Y_star, Y, preds, Ys):\n",
    "    bias_sum = 0\n",
    "    variance_sum = 0\n",
    "    noise_sum = 0\n",
    "    mse_sum = 0\n",
    "    \n",
    "    N = Y.size()[0]\n",
    "    print('N', N)\n",
    "    num_samples = len(preds)\n",
    "    for n in range(N):\n",
    "#         yv = torch.zero(num_samples)\n",
    "        predv = torch.zeros(num_samples)\n",
    "        yv = torch.zeros(num_samples)\n",
    "        for j in range(num_samples):\n",
    "#             print('preds[j][n]', preds[j][n])\n",
    "            predv[j] = preds[j][n]\n",
    "            yv[j] = Ys[j][n]\n",
    "            mse_sqrt = preds[j][n] - Y[n]\n",
    "            mse = mse_sqrt * mse_sqrt\n",
    "            mse_sum += mse\n",
    "        pred_avg = predv.mean()\n",
    "        bias_sqrt = pred_avg - Y_star[n]\n",
    "        bias = bias_sqrt * bias_sqrt\n",
    "        bias_sum += bias\n",
    "        variance = predv.var()\n",
    "#         help(torch.var)\n",
    "#         asdf\n",
    "        variance_sum += variance\n",
    "        noise = yv.var()\n",
    "        noise_sum += noise\n",
    "    return bias_sum / N, variance_sum / N, \\\n",
    "        noise_sum / N, \\\n",
    "        mse_sum / N / num_samples\n",
    "    \n",
    "#     err = pred - Y.view(-1, 1)\n",
    "#     mse = (err * err).sum() / N\n",
    "#     bias = (err * err).sum() / N\n",
    "#     print('mse', mse)\n",
    "#     print('bias', bias)\n",
    "#     variance = \n",
    "#     asdfasdf\n",
    "#     bias = err.sum() * err.sum() / N / N\n",
    "#     variance = (pred * pred).sum() / N - pred.sum() * pred.sum() / N / N\n",
    "#     noise = mse - bias - variance\n",
    "\n",
    "def run(order):\n",
    "    print('')\n",
    "    print('order %s' % order)\n",
    "    preds = []\n",
    "    Ys = []\n",
    "    X = generate_X()\n",
    "    Y_star = (X * true_a + true_b).view(-1)\n",
    "    X2 = generate_features(X=X, order=order)\n",
    "    Ws = torch.zeros(num_runs, order + 1)\n",
    "    for i in range(num_runs):\n",
    "        Y = generate_Y(X=X, seed=i)\n",
    "        Ys.append(Y)\n",
    "        W = fit(X2, Y)\n",
    "        Ws[i] = W\n",
    "        pred = X2 @ W\n",
    "        pred = pred.view(-1)\n",
    "        preds.append(pred)\n",
    "    bias, variance, noise, mse = calc_stats(\n",
    "        Y_star=Y_star, Y=Y, preds=preds, Ys=Ys)\n",
    "    noise2 = mse - bias - variance\n",
    "    W_avg = Ws.mean(0)\n",
    "    print('W_avg', W_avg.view(1, -1))\n",
    "    print('bias %.3f' % bias, 'variance %.4f' % variance,\n",
    "          'noise %.4f noise2 %.4f' % (noise, noise2),\n",
    "          'mse %.4f' % mse)\n",
    "#         calc_stats(Y=Y, pred=pred)\n",
    "#         run_one(order=order, seed=i)\n",
    "\n",
    "run(order=0)\n",
    "run(order=1)\n",
    "run(order=2)\n",
    "run(order=3)\n",
    "# run(order=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-variance decomposition of squared error\n",
    "\n",
    "\"Suppose that we have a training set consisting of a set of points $x_1, \\dots, x_n$ and real values $y_i$, associated with each point $x_i$. We assume that there is a function with noise $y = f(x) + \\epsilon$, where the noise, $\\epsilon$, has zero mean and variance $\\sigma^2$.\n",
    "\n",
    "\"We want to find a function $\\hat{f}(x)$, that approximates the true function $f(x)$ as well as possible, by means of some learning algorithm. We make 'as well as possible' precise by measuring the mean squared error between $y$ and $\\hat{f}(x)$: we want $(y - \\hat{f}(x))^2$ to be minimal, both for $x_1, \\dots, x_n$, and for points outside of our sample. Of course, we cannot hope to do so perfectly, since the $y_i$ contain noise $\\epsilon$; this means we must be prepared to accept an irreducible error in any function we come up with.\n",
    "\n",
    "\"Finding an $\\hat{f}$ that generalizes to points outside of the training set can be done with any of the countless algorithms used for supervised learning. It turns out that whichever function $\\hat{f}$ we select, we can decompose its expected error on an unseen sample $x$ as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\def\\Exp{\\mathbb{E}}\n",
    "\\def\\Bias{\\text{Bias}}\n",
    "\\def\\Var{\\text{Var}}\n",
    "\\def\\fhat{\\hat{f}}\n",
    "\\Exp[(y - \\fhat(x))^2] = \\Bias[\\fhat(x)]^2 + \\Var[\\fhat(x)] + \\sigma^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "$$\n",
    "\\Bias[\\fhat(x)] = \\Exp[\\fhat(x) - f(x)] = \\Exp[\\fhat(x)] - \\Exp[f(x)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and:\n",
    "\n",
    "$$\n",
    "\\Var[\\fhat(x)] = \\Exp[\\fhat(x)^2] - \\Exp[\\fhat(x)]^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"The expectation ranges over different choices of the training set $x_1, \\dots, x_n, y_1, \\dots, y_n$, all sampled from the same joint distribution $P(x,y)$. The three terms represent:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- the square of the bias of the learning method, which can be thought of as the error caused by the simplifying assumptions built into the method. Eg, when approximating a non-linear function $f(x)$ using a learning method for linear models, there will be error in the estimates $\\fhat(x)$ due to this assumption\n",
    "- the variance of the learning method, or, intuitively, how much the learning method $\\fhat(x)$ will move around its mean\n",
    "- the irreducible error $\\sigma^2$. Since all three terms are non-negative, this forms a lower coutn on the expected error on unseen samples.\n",
    "\n",
    "\"The more complex the model $\\fhat(x)$ is, the more data points it will capture, and the lower the bias will be. However, complexity will make the model 'move' more to capture the data points, and hence its variance will be larger.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivation\n",
    "\n",
    "\"The derivation of the bias-variance decomposition for squared error proceeds as follows. For notational convenience, abbreviate $f = f(x)$ and $\\fhat = \\fhat(x)$. First recall that, by definition, for any random variable $X$, we have\n",
    "\n",
    "$$\n",
    "\\Var[X] = \\Exp[X^2] - \\Exp[X]^2\n",
    "$$\n",
    "\n",
    "Rearranging, we get\n",
    "\n",
    "$$\n",
    "\\Exp[X^2] = \\Var[X] + \\Exp[X]^2\n",
    "$$\n",
    "\n",
    "Since $f$ is deterministic\n",
    "\n",
    "$$\n",
    "\\Exp[f] = f\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, given $y = f+ \\epsilon$ and $\\Exp[\\epsilon] = 0$, implies $\\Exp[y] = \\Exp[f + \\epsilon] = \\Exp[f] = f$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, since $\\Var[\\epsilon] = \\sigma^2$:\n",
    "\n",
    "$$\n",
    "\\Var[y] = \\Exp[(y - \\Exp[y])^2] = \\Exp[(y -f)^2] = \\Exp[(f + \\epsilon -f )^2] = \\Exp[\\epsilon^2] = \\Var[\\epsilon] + \\Exp[\\epsilon]^2 = \\sigma^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, since $\\epsilon$ and $\\fhat$ are independent, we can write\n",
    "\n",
    "$$\n",
    "\\Exp[(y-\\fhat)^2] = \\Exp[y^2 + \\fhat^2 - 2y\\fhat] \\\\\n",
    "= \\Exp[y^2] + \\Exp[\\fhat^2] - \\Exp[2y\\fhat] \\\\\n",
    "= \\Var[y] + \\Exp[y]^2 + \\Var[\\fhat] + \\Exp[\\fhat]^2 - 2f\\Exp[\\fhat] \\\\\n",
    "= \\Var[y] + \\Var[\\fhat] + (f^2 - 2f\\Exp[\\fhat] + \\Exp[\\fhat]^2) \\\\\n",
    "= \\Var[y] + \\Var[\\fhat] + ( f - \\Exp[\\fhat])^2 \\\\\n",
    " = \\sigma^2 + \\Var[\\fhat] + \\Bias[\\fhat]^2\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
