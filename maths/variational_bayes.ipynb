{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Variational Bayes\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was written using a combination of at least the following sources:\n",
    "\n",
    "- https://en.wikipedia.org/wiki/Variational_Bayesian_methods\n",
    "- https://benmoran.wordpress.com/2015/02/21/variational-bayes-and-the-evidence-lower-bound\n",
    "- http://www.umiacs.umd.edu/~xyang35/files/understanding-variational-lower.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to evaluate the posterior $p(z \\mid x)$ given a prior $p(z)$, and $p(x \\mid z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior is given, using Bayes Rule, as:\n",
    "\n",
    "$$\n",
    "p(z \\mid x) = \\frac{p(x \\mid z)\\,p(z)}\n",
    "  {p(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have $p(x \\mid z)$ and $p(z)$. How to get $p(x)$? We can get it by marginalizing the joint distribution over $z$:\n",
    "\n",
    "$$\n",
    "p(z \\mid x) = \\frac{p(x \\mid z)p(z)}\n",
    "  {\\int_z p(x,z) \\, dz}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The issue is that the marginalization over $z$ is often intractible. For example, the space of $z$ could be combinatorially large."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Variational approximation lets us provide a distribution that approximates $p(z \\mid x)$, and replace the marginalization by expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a distribution $q(z)$ to approximate $p(z \\mid x)$. This distribution will likely be a family of parametric distributions, ie $q_\\phi(z)$. We need to minimize the difference of $q_\\phi(z)$ from $p(z \\mid x)$. How to do this? We can measure this using the KL-divergence, and then minimize this KL-divergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL-divergence is:\n",
    "\n",
    "$$\n",
    "\\def\\Exp{\\mathbb{E}}\n",
    "D_{KL}(q_\\phi(z) \\| p(z | x))\n",
    "= \\Exp_{q_\\phi(z)}\\left[\n",
    "   \\log\n",
    "   \\frac{q_\\phi(z)}\n",
    "     {p(z \\mid x)}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "=\\Exp_{q_\\phi(z)} \\left[\n",
    "   \\log q_\\phi(z)\n",
    "   + \\log \\left(\n",
    "       \\frac{p(x)}\n",
    "           {p(z,x)}\n",
    "   \\right)\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= \\Exp_{q_\\phi(z)} \\left[\n",
    "   \\log q_\\phi(z)\n",
    "   + \\log p(x)\n",
    "   - \\log p(z,x)\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But $p(x)$ doesnt depend on $z$, so we can remove it from the expectation:\n",
    "\n",
    "$$\n",
    "D_{KL}(q_\\phi(z) \\| p(z \\mid x)) =\n",
    "\\Exp_{q_\\phi(z)} \\left[\n",
    "   \\log q_\\phi(z)\n",
    "   - \\log p(z,x)\n",
    "\\right]\n",
    "+ \\log p(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\log p(x) = D_{KL}(q_\\phi(z) \\| p(z \\mid x)) - \\Exp_{q_\\phi(z)} \\left[\n",
    "   \\log q_\\phi(z) - \\log p(z,x)\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "= D_{KL}(q_\\phi(z) \\| p(z \\mid x)) + L[q_\\phi]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $L[q_\\phi] = \\Exp_{q_\\phi(z)}[\\log p(z,x) - \\log q_\\phi(z)]$ is a functional of $q_\\phi$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since $D_{KL}(\\cdot \\| \\cdot)$ is more than 0, and $\\log p(x)$ is independent of $q_\\phi$, then maximizing $L$ will minimize the KL-divergence, and the approximate distribution $q_\\phi(z)$ becomes most similar to the posterior distribution $p(z \\mid x)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L[q_\\phi]$ is termed the __Evidence Lower Bound (ELBO)__, or the __Variational Lower Bound__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
